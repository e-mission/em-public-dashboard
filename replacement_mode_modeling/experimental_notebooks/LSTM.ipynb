{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2cdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ace37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global experiment flags and variables.\n",
    "SEED = 13210\n",
    "\n",
    "'''\n",
    "'No Travel', 'Free Shuttle', 'Other', 'Gas Car, drove alone',\n",
    "       'Regular Bike', 'Walk', 'Gas Car, with others', 'Bus', 'E-bike',\n",
    "       'Scooter share', 'Taxi/Uber/Lyft', 'Train', 'Bikeshare',\n",
    "       'Skate board', 'Not a Trip'\n",
    "'''\n",
    "\n",
    "TARGET_MAPPING = {\n",
    "    'No Travel': 'no_trip',\n",
    "    'Free Shuttle': 'transit',\n",
    "    'Other': 'unknown',\n",
    "    'Gas Car, drove alone': 'car',\n",
    "    'Regular Bike': 'p_micro',\n",
    "    'Walk': 'walk',\n",
    "    'Gas Car, with others': 's_micro',\n",
    "    'Bus': 'transit',\n",
    "    'E-bike': 'p_micro',\n",
    "    'Scooter share': 's_micro',\n",
    "    'Taxi/Uber/Lyft': 'ridehail',\n",
    "    'Train': 'transit',\n",
    "    'Bikeshare': 's_micro',\n",
    "    'Skate board': 'p_micro',\n",
    "    'Not a Trip': 'no_trip'\n",
    "}\n",
    "\n",
    "\n",
    "TARGETS = {\n",
    "    x: ix for (ix, x) in enumerate([\n",
    "        'p_micro', 'no_trip', 's_car', 'transit', 'car', 's_micro', 'ridehail', 'walk', 'unknown'\n",
    "    ])\n",
    "}\n",
    "\n",
    "av_modes = {\n",
    "    'Skateboard': 'p_micro', \n",
    "    'Walk/roll': 'walk', \n",
    "    'Shared bicycle or scooter': 's_micro', \n",
    "    'Taxi (regular taxi, Uber, Lyft, etc)': 'ridehail', \n",
    "    'Rental car (including Zipcar/ Car2Go)': 'car',\n",
    "    'Bicycle': 'p_micro', \n",
    "    'Public transportation (bus, subway, light rail, etc.)': 'transit',\n",
    "    'Get a ride from a friend or family member': 's_car',\n",
    "    'None': 'no_trip', \n",
    "    'Prefer not to say': 'unknown'\n",
    "}\n",
    "\n",
    "# Set the Numpy seed too.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481cc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_modeling_data_02142024.csv')\n",
    "weather_df = pd.read_csv('../data/denver_weather_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Replaced_mode = data.Replaced_mode.replace(TARGET_MAPPING)\n",
    "data.Replaced_mode = data.Replaced_mode.replace(TARGETS)\n",
    "data.rename(columns={'Replaced_mode': 'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[list(av_modes.values())] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_availability(x):\n",
    "    modes = [y.strip() for y in x.available_modes.split(';')]\n",
    "    mapped = set([av_modes[x] for x in modes])\n",
    "    \n",
    "    for mode in mapped:\n",
    "        x[mode] = 1\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "data = data.apply(lambda x: encode_availability(x), axis=1)\n",
    "data.drop(columns=['available_modes'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mark'] = 0\n",
    "\n",
    "data.section_distances = data.section_distances.apply(lambda x: ast.literal_eval(x))\n",
    "data.section_modes = data.section_modes.apply(lambda x: ast.literal_eval(x))\n",
    "data.section_durations = data.section_durations.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "data.mark = data.apply(\n",
    "    lambda x: 1 if (len(x.section_distances) == len(x.section_modes) == len(x.section_durations))\n",
    "    and len(x.section_distances) > 0 and len(x.section_modes) > 0 and len(x.section_durations) > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "data.section_distances = data.section_distances.apply(lambda x: np.array(x).astype(np.float64))\n",
    "data.section_modes = data.section_modes.apply(lambda x: np.array(x))\n",
    "data.section_durations = data.section_durations.apply(lambda x: np.array(x).astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.mark == 1, :].drop(columns=['mark'], inplace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionScaler:\n",
    "    def __init__(self):\n",
    "        self.dur = dict()\n",
    "        self.dist = dict()\n",
    "    \n",
    "    def compute_stats(self, df):\n",
    "        \n",
    "        for _, row in df[['section_modes', 'section_distances', 'section_durations']].iterrows():\n",
    "            for (mode, distance, duration) in zip(\n",
    "                row['section_modes'], row['section_distances'], row['section_durations']\n",
    "            ):\n",
    "                if mode not in self.dur.keys():\n",
    "                    self.dur[mode] = [duration]\n",
    "                else:\n",
    "                    self.dur[mode].append(duration)\n",
    "                \n",
    "                if mode not in self.dist.keys():\n",
    "                    self.dist[mode] = [distance]\n",
    "                else:\n",
    "                    self.dist[mode].append(distance)\n",
    "\n",
    "        for mode in self.dur.keys():\n",
    "            self.dur[mode] = [np.nanmean(self.dur[mode]), np.std(self.dur[mode])]\n",
    "    \n",
    "        for mode in self.dist.keys():\n",
    "            self.dist[mode] = [np.nanmean(self.dist[mode]), np.std(self.dist[mode])]\n",
    "    \n",
    "    def apply(self, df):\n",
    "\n",
    "        rows = list()\n",
    "        \n",
    "        for ix, x in df.iterrows():\n",
    "            row = x.to_dict()\n",
    "            modes = row['section_modes']\n",
    "            distances = row['section_distances']\n",
    "            durations = row['section_durations']\n",
    "            \n",
    "            norm_distances = [\n",
    "                (distances[i] - self.dist[mode][0])/self.dist[mode][1] for i, mode in enumerate(modes)\n",
    "            ]\n",
    "            \n",
    "            norm_durations = [\n",
    "                (durations[i] - self.dur[mode][0])/self.dur[mode][1] for i, mode in enumerate(modes)\n",
    "            ]\n",
    "\n",
    "            if ix == 0:\n",
    "                print(norm_distances, norm_durations)\n",
    "            \n",
    "            row['section_distances'] = norm_distances\n",
    "            row['section_durations'] = norm_durations\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "        return pd.DataFrame(data=rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLIT_TYPE(Enum):\n",
    "    INTRA_USER = 0\n",
    "    INTER_USER = 1\n",
    "    TARGET = 2\n",
    "    MODE = 3\n",
    "    INTER_USER_STATIC = 4\n",
    "    \n",
    "\n",
    "class SPLIT(Enum):\n",
    "    TRAIN = 0\n",
    "    TEST = 1\n",
    "\n",
    "def get_splits(count_df: pd.DataFrame, n:int, test_size=0.2):\n",
    "    maxsize = int(n * test_size)\n",
    "\n",
    "    max_threshold = int(maxsize * 1.05)\n",
    "    min_threshold = int(maxsize * 0.95)\n",
    "\n",
    "    print(f\"{min_threshold}, {max_threshold}\")\n",
    "    \n",
    "    # Allow a 10% tolerance\n",
    "    def _dp(ix, curr_size, ids, cache):\n",
    "        \n",
    "        if ix >= count_df.shape[0]:\n",
    "            return []\n",
    "\n",
    "        key = ix\n",
    "\n",
    "        if key in cache:\n",
    "            return cache[key]\n",
    "\n",
    "        if curr_size > max_threshold:\n",
    "            return []\n",
    "\n",
    "        if min_threshold <= curr_size <= max_threshold:\n",
    "            return ids\n",
    "\n",
    "        # two options - either pick the current id or skip it.\n",
    "        branch_a = _dp(ix, curr_size+count_df.loc[ix, 'count'], ids+[count_df.loc[ix, 'index']], cache)\n",
    "        branch_b = _dp(ix+1, curr_size, ids, cache)\n",
    "        \n",
    "        curr_max = []\n",
    "        if branch_a and len(branch_a) > 0:\n",
    "            curr_max = branch_a\n",
    "        \n",
    "        if branch_b and len(branch_b) > len(branch_a):\n",
    "            curr_max = branch_b\n",
    "            \n",
    "        cache[key] = curr_max\n",
    "        return cache[key]\n",
    "    \n",
    "    return _dp(0, 0, ids=list(), cache=dict())\n",
    "\n",
    "\n",
    "def get_train_test_splits(data: pd.DataFrame, how=SPLIT_TYPE, test_ratio=0.2, shuffle=True):\n",
    "\n",
    "    n_users = list(data.user_id.unique())\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    if shuffle:\n",
    "        data = data.sample(data.shape[0], random_state=SEED).reset_index(drop=True, inplace=False)\n",
    "\n",
    "    if how == SPLIT_TYPE.INTER_USER:\n",
    "        # Make the split, ensuring that a user in one fold is not leaked into the other fold.\n",
    "        # Basic idea: we want to start with the users with the highest instances and place \n",
    "        # alternating users in each set.\n",
    "        counts = data.user_id.value_counts().reset_index(drop=False, inplace=False, name='count')\n",
    "\n",
    "        # Now, start with the user_id at the top, and keep adding to either split.\n",
    "        # This can be achieved using a simple DP program.\n",
    "        test_ids = get_splits(counts, data.shape[0])\n",
    "        test_data = data.loc[data.user_id.isin(test_ids), :]\n",
    "        train_index = data.index.difference(test_data.index)\n",
    "        train_data = data.loc[data.user_id.isin(train_index), :]\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    elif how == SPLIT_TYPE.INTRA_USER:\n",
    "        \n",
    "        # There are certain users with only one observation. What do we do with those?\n",
    "        # As per the mobilitynet modeling pipeline, we randomly assign them to either the\n",
    "        # training or test set.\n",
    "        \n",
    "        value_counts = data.user_id.value_counts()\n",
    "        single_count_ids = value_counts[value_counts == 1].index\n",
    "        \n",
    "        data_filtered = data.loc[~data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "        data_single_counts = data.loc[data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "        \n",
    "        X_tr, X_te = train_test_split(\n",
    "            data_filtered, test_size=test_ratio, shuffle=shuffle, stratify=data_filtered.user_id,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        data_single_counts['assigned'] = np.random.choice(['train', 'test'], len(data_single_counts))\n",
    "        X_tr_merged = pd.concat(\n",
    "            [X_tr, data_single_counts.loc[data_single_counts.assigned == 'train', :].drop(\n",
    "                columns=['assigned'], inplace=False\n",
    "            )],\n",
    "            ignore_index=True, axis=0\n",
    "        )\n",
    "        \n",
    "        X_te_merged = pd.concat(\n",
    "            [X_te, data_single_counts.loc[data_single_counts.assigned == 'test', :].drop(\n",
    "                columns=['assigned'], inplace=False\n",
    "            )],\n",
    "            ignore_index=True, axis=0\n",
    "        )\n",
    "        \n",
    "        return X_tr_merged, X_te_merged\n",
    "    \n",
    "    elif how == SPLIT_TYPE.TARGET:\n",
    "        \n",
    "        X_tr, X_te = train_test_split(\n",
    "            data, test_size=test_ratio, shuffle=shuffle, stratify=data.target,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        return X_tr, X_te\n",
    "    \n",
    "    elif how == SPLIT_TYPE.MODE:\n",
    "        X_tr, X_te = train_test_split(\n",
    "            data, test_size=test_ratio, shuffle=shuffle, stratify=data.section_mode_argmax,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        return X_tr, X_te\n",
    "    \n",
    "    elif how == SPLIT_TYPE.INTER_USER_STATIC:\n",
    "        \n",
    "        train_ids = ['810be63d084746e3b7da9d943dd88e8c', 'bf774cbe6c3040b0a022278d36a23f19', '8a8332a53a1b4cdd9f3680434e91a6ef', \n",
    "                     '5ad862e79a6341f69f28c0096fe884da', '7f89656bd4a94d12ad8e5ad9f0afecaf', 'fbaa338d7cd7457c8cad4d0e60a44d18', \n",
    "                     '3b25446778824941a4c70ae5774f4c68', '28cb1dde85514bbabfd42145bdaf7e0a', '3aeb5494088542fdaf798532951aebb0', \n",
    "                     '531732fee3c24366a286d76eb534aebc', '950f4287bab5444aa0527cc23fb082b2', '737ef8494f26407b8b2a6b1b1dc631a4', \n",
    "                     'e06cf95717f448ecb81c440b1b2fe1ab', '7347df5e0ac94a109790b31ba2e8a02a', 'bd9cffc8dbf1402da479f9f148ec9e60', \n",
    "                     '2f3b66a5f98546d4b7691fba57fa640f', 'f289f7001bd94db0b33a7d2e1cd28b19', '19a043d1f2414dbcafcca44ea2bd1f19', \n",
    "                     '68788082836e4762b26ad0877643fdcf', '4e8b1b7f026c4384827f157225da13fa', '703a9cee8315441faff7eb63f2bfa93f', \n",
    "                     'add706b73839413da13344c355dde0bb', '47b5d57bd4354276bb6d2dcd1438901d', 'e4cfb2a8f600426897569985e234636e', \n",
    "                     '0154d71439284c34b865e5a417cd48af', '234f4f2366244fe682dccded2fa7cc4e', '0d0ae3a556414d138c52a6040a203d24', \n",
    "                     '44c10f66dec244d6b8644231d4a8fecb', '30e9b141d7894fbfaacecd2fa18929f9', '0eb313ab00e6469da78cc2d2e94660fb', \n",
    "                     'fc51d1258e4649ecbfb0e6ecdaeca454', 'a1954793b1454b2f8cf95917d7547169', '6656c04c6cba4c189fed805eaa529741', \n",
    "                     '6a0f3653b80a4c949e127d6504debb55', 'dfe5ca1bb0854b67a6ffccad9565d669', '8b1f3ba43de945bea79de6a81716ad04', \n",
    "                     'cde34edb8e3a4278a18e0adb062999e5', '6d96909e5ca442ccb5679d9cdf3c8f5b', 'a60a64d82d1c439a901b683b73a74d73', \n",
    "                     '60e6a6f6ed2e4e838f2bbed6a427028d', '88041eddad7542ea8c92b30e5c64e198', '1635c003b1f94a399ebebe21640ffced', \n",
    "                     '1581993b404a4b9c9ca6b0e0b8212316', 'b1aed24c863949bfbfa3a844ecf60593', '4b89612d7f1f4b368635c2bc48bd7993', \n",
    "                     'eb2e2a5211564a9290fcb06032f9b4af', '26767f9f3da54e93b692f8be6acdac43', '8a98e383a2d143e798fc23869694934a', \n",
    "                     'b346b83b9f7c4536b809d5f92074fdae', 'd929e7f8b7624d76bdb0ec9ada6cc650', '863e9c6c8ec048c4b7653f73d839c85b', \n",
    "                     'f50537eb104e4213908f1862c8160a3e', '4a9db5a9bac046a59403b44b883cc0ba', 'cded005d5fd14c64a5bba3f5c4fe8385', \n",
    "                     'c7ce889c796f4e2a8859fa2d7d5068fe', '405b221abe9e43bc86a57ca7fccf2227', '0b3e78fa91d84aa6a3203440143c8c16', \n",
    "                     'fbff5e08b7f24a94ab4b2d7371999ef7', 'e35e65107a34496db49fa5a0b41a1e9e', 'd5137ebd4f034dc193d216128bb7fc9a', \n",
    "                     '3f7f2e536ba9481e92f8379b796ad1d0', 'dc75e0b776214e1b9888f6abd042fd95', 'b41dd7d7c6d94fe6afe2fd26fa4ac0bd', \n",
    "                     'eec6936e1ac347ef9365881845ec74df', '8c7d261fe8284a42a777ffa6f380ba3b', '4baf8c8af7b7445e9067854065e3e612', \n",
    "                     'c6e4db31c18b4355b02a7dd97deca70b', 'f0db3b1999c2410ba5933103eca9212f', '487e20ab774742378198f94f5b5b0b43', \n",
    "                     'dc1ed4d71e3645d0993885398d5628ca', '8c3c63abb3ec4fc3a61e7bf316ee4efd', '15eb78dd6e104966ba6112589c29dc41', \n",
    "                     'c23768ccb817416eaf08be487b2e3643', 'ecd2ae17d5184807abd87a287115c299', '71f21d53b655463784f3a3c63c56707b', \n",
    "                     '2931e0a34319495bbb5898201a54feb5', '92bde0d0662f45ac864629f486cffe77', '42b3ee0bc02a481ab1a94644a8cd7a0d', \n",
    "                     '15aa4ba144a34b8b8079ed7e049d84df', '509b909390934e988eb120b58ed9bd8c', '14103cda12c94642974129989d39e50d', \n",
    "                     '8b0876430c2641bcaea954ea00520e64', 'baa4ff1573ae411183e10aeb17c71c53', '14fe8002bbdc4f97acbd1a00de241bf6', \n",
    "                     '1b7d6dfea8464bcab9321018b10ec9c9', '487ad897ba93404a8cbe5de7d1922691', '5182d93d69754d7ba06200cd1ac5980a', \n",
    "                     '91f3ca1c278247f79a806e49e9cc236f', 'e66e63b206784a559d977d4cb5f1ec34', '840297ae39484e26bfebe83ee30c5b3e', \n",
    "                     'c6807997194c4c528a8fa8c1f6ee1595', '802667b6371f45b29c7abb051244836a', 'b2bbe715b6a14fd19f751cae8adf6b4e', \n",
    "                     'feb1d940cd3647d1a101580c2a3b3f8c', '1b9883393ab344a69bc1a0fab192a94c', 'ac604b44fdca482fb753034cb55d1351', \n",
    "                     'f446bf3102ff4bd99ea1c98f7d2f7af0', 'c2c5d4b9a607487ea405a99c721079d4', '85ddd3c34c58407392953c47a32f5428', \n",
    "                     'd51de709f95045f8bacf473574b96ba5', '6373dfb8cb9b47e88e8f76adcfadde20', '313d003df34b4bd9823b3474fc93f9f9', \n",
    "                     '53e78583db87421f8decb529ba859ca4', '8fdc9b926a674a9ea07d91df2c5e06f2', '90480ac60a3d475a88fbdab0a003dd5d', \n",
    "                     '7559c3f880f341e898a402eba96a855d', '19a4c2cf718d40588eb96ac25a566353', 'f4427cccaa9442b48b42bedab5ab648e', \n",
    "                     'e192b8a00b6c422296851c93785deaf7', '355e25bdfc244c5e85d358e39432bd44', 'a0c3a7b410b24e18995f63369a31d123', \n",
    "                     '03a395b4d8614757bb8432b4984559b0', 'a2d48b05d5454d428c0841432c7467b6', '3d981e617b304afab0f21ce8aa6c9786', \n",
    "                     '2cd5668ac9054e2eb2c88bb4ed94bc6d', 'd7a732f4a8644bcbb8dedfc8be242fb2', '367eb90b929d4f6e9470d15c700d2e3f', \n",
    "                     'e049a7b2a6cb44259f907abbb44c5abc', 'a231added8674bef95092b32bc254ac8', 'e88a8f520dde445484c0a9395e1a0599',\n",
    "                     'cba570ae38f341faa6257342727377b7', '97953af1b97d4e268c52e1e54dcf421a', 'd200a61757d84b1dab8fbac35ff52c28', \n",
    "                     'fc68a5bb0a7b4b6386b3f08a69ead36f', '4a8210aec25e443391efb924cc0e5f23', '903742c353ce42c3ad9ab039fc418816', \n",
    "                     '2114e2a75304475fad06ad201948fbad', 'ac917eae407c4deb96625dd0dc2f2ba9', '3dddfb70e7cd40f18a63478654182e9a', \n",
    "                     'd3735ba212dd4c768e1675dca7bdcb6f', '7abe572148864412a33979592fa985fb', 'd3dff742d07942ca805c2f72e49e12c5' \n",
    "                     ]\n",
    "        \n",
    "        X_tr = data.loc[data.user_id.isin(train_ids), :]\n",
    "        X_te = data.loc[~data.user_id.isin(train_ids), :]\n",
    "        \n",
    "        return X_tr, X_te\n",
    "    \n",
    "    raise NotImplementedError(\"Unknown split type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df: pd.DataFrame):\n",
    "    to_drop = [\n",
    "       'raw_trip',\n",
    "        'start_ts',\n",
    "        'start_loc',\n",
    "        'start_place',\n",
    "        'end_place',\n",
    "        'cleaned_trip',\n",
    "        'inferred_labels',\n",
    "        'inferred_trip',\n",
    "        'expectation',\n",
    "        'confidence_threshold',\n",
    "        'expected_trip',\n",
    "        'user_input',\n",
    "        'start:year',\n",
    "        'start:month',\n",
    "        'start:day',\n",
    "        'start:hour',\n",
    "        'start_local_dt_minute',\n",
    "        'start_local_dt_second',\n",
    "        'start_local_dt_weekday',\n",
    "        'start_local_dt_timezone',\n",
    "        'end:year',\n",
    "        'end:month',\n",
    "        'end:day',\n",
    "        'end:hour',\n",
    "        'end_local_dt_minute',\n",
    "        'end_local_dt_second',\n",
    "        'end_local_dt_weekday',\n",
    "        'end_local_dt_timezone',\n",
    "        '_id',\n",
    "        'metadata_write_ts',\n",
    "        'additions',\n",
    "        'mode_confirm',\n",
    "        'purpose_confirm',\n",
    "        'distance_miles',\n",
    "        'Mode_confirm',\n",
    "        'Trip_purpose',\n",
    "        'original_user_id',\n",
    "        'program',\n",
    "        'opcode',\n",
    "        'Timestamp',\n",
    "        'birth_year',\n",
    "        'gender_Man',\n",
    "        'gender_Man;Nonbinary/genderqueer/genderfluid',\n",
    "        'gender_Nonbinary/genderqueer/genderfluid',\n",
    "        'gender_Prefer not to say',\n",
    "        'gender_Woman',\n",
    "        'gender_Woman;Nonbinary/genderqueer/genderfluid',\n",
    "        'has_multiple_jobs_No',\n",
    "        'has_multiple_jobs_Prefer not to say',\n",
    "        'has_multiple_jobs_Yes',\n",
    "        \"highest_education_Bachelor's degree\",\n",
    "        'highest_education_Graduate degree or professional degree',\n",
    "        'highest_education_High school graduate or GED',\n",
    "        'highest_education_Less than a high school graduate',\n",
    "        'highest_education_Prefer not to say',\n",
    "        'highest_education_Some college or associates degree',\n",
    "        'primary_job_type_Full-time',\n",
    "        'primary_job_type_Part-time',\n",
    "        'primary_job_type_Prefer not to say',\n",
    "        'is_overnight_trip',\n",
    "        'n_working_residents',\n",
    "        'start_lat',\n",
    "        'start_lng',\n",
    "        'end_lat',\n",
    "        'end_lng',\n",
    "        'source', 'end_ts', 'end_fmt_time', 'end_loc',\n",
    "       ]\n",
    "\n",
    "    # Drop section_mode_argmax and available_modes.\n",
    "    return df.drop(\n",
    "        columns=to_drop, \n",
    "        inplace=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fa4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = drop_columns(data)\n",
    "\n",
    "train_df, test_df = get_train_test_splits(data=processed, how=SPLIT_TYPE.INTER_USER_STATIC, shuffle=True)\n",
    "\n",
    "scaler = SectionScaler()\n",
    "scaler.compute_stats(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44354097",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaler.dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fee4f-0da8-4391-9528-ef5fd7837365",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = scaler.apply(train_df)\n",
    "test_df = scaler.apply(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eed47f-3a58-4072-8dbe-f5287084f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d39919",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.section_distances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1c96e-7f18-44bd-8df2-f92239114e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=32):\n",
    "        super(SectionEmbedding, self).__init__()\n",
    "        self.dpt = nn.Dropout(0.2)\n",
    "        self.encoder = nn.Linear(input_dim, emb_dim)\n",
    "        self.decoder = nn.Linear(emb_dim, input_dim)\n",
    "        self.act = nn.LeakyReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input will be a one-hot encoded matrix, where nrows=number of modes, ncols=input_dim\n",
    "        dim = (B, N, D)\n",
    "        \n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplacedModeDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.data = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data.ix.unique())\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        \n",
    "        # Could be between 1 - 15.\n",
    "        sequence = self.data.loc[self.data.ix == ix, :]\n",
    "        \n",
    "        # Static features that do not vary with time.\n",
    "        demographic_features = ['n_residence_members', \n",
    "        'primary_job_commute_time', 'income_category',\n",
    "        'n_residents_u18', 'n_residents_with_license', 'n_motor_vehicles', 'age', \n",
    "        'p_micro', 'walk', 's_micro', 'ridehail', 'car', 'transit', 's_car', 'no_trip', 'unknown',\n",
    "        'has_drivers_license_No', 'has_drivers_license_Prefer not to say', 'has_drivers_license_Yes', \n",
    "        'primary_job_description_Clerical or administrative support', 'primary_job_description_Custodial', \n",
    "        'primary_job_description_Education', 'primary_job_description_Food service', \n",
    "        'primary_job_description_Manufacturing, construction, maintenance, or farming', \n",
    "        'primary_job_description_Medical/healthcare', 'primary_job_description_Other', \n",
    "        'primary_job_description_Professional, managerial, or technical', \n",
    "        'primary_job_description_Sales or service', 'primary_job_commute_mode_Active transport', \n",
    "        'primary_job_commute_mode_Car transport', 'primary_job_commute_mode_Hybrid', \n",
    "        'primary_job_commute_mode_Public transport', 'primary_job_commute_mode_Unknown', \n",
    "        'primary_job_commute_mode_WFH', 'duration', 'distance']\n",
    "        \n",
    "        seq_features = ['section_distances', 'section_durations', 'section_modes', 'mph']\n",
    "        \n",
    "        weather_features = ['temperature_2m (°F)', \n",
    "        'relative_humidity_2m (%)', 'dew_point_2m (°F)', 'rain (inch)', 'snowfall (inch)', \n",
    "        'wind_speed_10m (mp/h)', 'wind_gusts_10m (mp/h)']\n",
    "        \n",
    "        return (\n",
    "            sequence[seq_features], sequence[demographic_features], \n",
    "            sequence[weather_features], sequence['target']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b36058",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = ReplacedModeDataset(train_df)\n",
    "\n",
    "print(dset.__getitem__(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b78758",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = CustomDataset(train_df)\n",
    "test_dset = CustomDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    X, y = zip(*batch)\n",
    "    \n",
    "    seq_modes = [x[0] for x in X]\n",
    "    seq_metrics = [x[1] for x in X]\n",
    "    features = [x[-1] for x in X]\n",
    "\n",
    "    padded_seq = pad_sequence([s for s in seq_modes], batch_first=True)\n",
    "    padded_metrics = pad_sequence([m for m in seq_metrics], batch_first=True)\n",
    "    lengths = [len(seq) for seq in seq_modes]\n",
    "    stacked_features = torch.stack(features)\n",
    "\n",
    "    return (padded_seq, padded_metrics, stacked_features), torch.stack(y), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca34681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dset, batch_size=16, collate_fn=collate, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dset, batch_size=16, collate_fn=collate, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(modes, metrics, features), sY1, lX = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.size(), modes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 0 for no dropout.\n",
    "DROPOUT = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48871ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GELU_new(nn.Module):\n",
    "    \"\"\"\n",
    "    Taken from OpenAI GPT-2 implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GELU_new, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "\n",
    "class DilatedBlock(nn.Module):\n",
    "    def __init__(self, n_c):\n",
    "        super(DilatedBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(n_c, 4*n_c, bias=False),\n",
    "            GELU_new(),\n",
    "            nn.Linear(4*n_c, n_c, bias=False),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_features, head_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # in: (B, F, 64)\n",
    "        self.k = nn.Linear(n_features, head_size, bias=False)\n",
    "        self.q = nn.Linear(n_features, head_size, bias=False)\n",
    "        self.v = nn.Linear(n_features, head_size, bias=False)\n",
    "        self.dpt = nn.Dropout(DROPOUT)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(head_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        # Q.K.t\n",
    "        dot = torch.bmm(q, k.permute(0, 2, 1))\n",
    "        \n",
    "        # normalize dot product.\n",
    "        dot /= self.sqrt_d\n",
    "        \n",
    "        # softmax over -1 dim.\n",
    "        softmax = self.dpt(torch.softmax(dot, dim=-1))\n",
    "        \n",
    "        # dot with values. (B, F, F) * (B, F, x) = (B, F, x)\n",
    "        return torch.bmm(softmax, v)\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # 64 dims, 4 heads => 16 dims per head.\n",
    "        head_size = n_dim//n_heads\n",
    "        self.heads = nn.ModuleList([SelfAttention(n_dim, head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_dim, n_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is (B, seq, n_dim)\n",
    "        cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.proj(cat)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_c):\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        self.sa = MultiHeadAttention(n_heads=4, n_dim=n_c)\n",
    "        self.dilated = DilatedBlock(n_c)\n",
    "        self.ln1 = nn.LayerNorm(n_c)\n",
    "        self.ln2 = nn.LayerNorm(n_c)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.dilated(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, \n",
    "        output_size: int, n_lstm_layers: int = 1\n",
    "    ):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        n_embed_mode = 16\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(7, n_embed_mode, padding_idx=0)\n",
    "        self.dpt = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size + n_embed_mode,\n",
    "            hidden_size=hidden_size,\n",
    "            bias=False,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            num_layers=n_lstm_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, modes, x, lengths):\n",
    "        mode_emb = self.embedding(modes)\n",
    "        x = torch.cat([x, mode_emb], dim=-1)\n",
    "        \n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        return self.dpt(unpacked)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, output_size: int, \n",
    "        n_features: int, n_lstm_layers: int = 1, **kwargs\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        block1_ip_dim = hidden_size*2\n",
    "        block2_ip_dim = (hidden_size*2) + n_features\n",
    "        \n",
    "        self.lstm = LSTMLayer(\n",
    "            input_size, hidden_size, \n",
    "            output_size, n_lstm_layers\n",
    "        )\n",
    "        \n",
    "        self.block_l1 = nn.ModuleList([Block(block1_ip_dim) for _ in range(kwargs['l1_blocks'])])\n",
    "        self.block_l2 = nn.ModuleList([Block(block2_ip_dim) for _ in range(kwargs['l2_blocks'])])\n",
    "        self.final_proj = nn.Linear(block2_ip_dim, output_size, bias=True)\n",
    "    \n",
    "    def forward(self, modes, x, features, lengths):\n",
    "        \n",
    "        b = x.size(0)\n",
    "        \n",
    "        # Out = (B, seq, hidden*2)\n",
    "        lstm_out = self.lstm(modes, x, lengths)\n",
    "        \n",
    "        # Pass the raw output through the blocks.\n",
    "        for module in self.block_l1:\n",
    "            lstm_out = module(lstm_out)\n",
    "        \n",
    "        features_rshp = features.unsqueeze(1).expand(b, lstm_out.size(1), -1)\n",
    "        \n",
    "        # Out = (B, seq, n+40)\n",
    "        cat = torch.cat([lstm_out, features_rshp], dim=-1)\n",
    "        \n",
    "        for module in self.block_l2:\n",
    "            cat = module(cat)\n",
    "        \n",
    "        # (8, 3, 104) -> (B, 104)\n",
    "        # flattened = cat.view(b, -1)\n",
    "        \n",
    "        # proj = self.runtime_ffw(flattened.size(-1), 64)(flattened)\n",
    "        proj = cat.mean(dim=1)\n",
    "        \n",
    "        return self.final_proj(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "        if module.padding_idx is not None:\n",
    "            module.weight.data[module.padding_idx].zero_()\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "    elif isinstance(module, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        init.xavier_normal_(module.weight.data)\n",
    "        if module.bias is not None:\n",
    "            init.normal_(module.bias.data)\n",
    "    elif isinstance(module, nn.LSTM):\n",
    "        for param in module.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ecd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    n_lstm_layers=3,\n",
    "    input_size=3,\n",
    "    hidden_size=32, \n",
    "    output_size=num_classes,\n",
    "    n_features=40,\n",
    "    l1_blocks=4,\n",
    "    l2_blocks=4\n",
    ")\n",
    "\n",
    "model = model.apply(init_weights)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fec22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = train_df.shape[0]/(np.bincount(train_df.chosen.values) * len(np.unique(train_df.chosen)))\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=INIT_LR)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, tr_loader, te_loader):\n",
    "        pass\n",
    "    \n",
    "    def set_optim_params(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def set_criterion(self, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loader, opt, criterion, val_ix):\n",
    "    \n",
    "    print(\"\\tBeginning training.\")\n",
    "    \n",
    "    n_batches = len(loader)\n",
    "    print_every = n_batches//5\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for ix, (X, y, lengths) in enumerate(loader):\n",
    "        \n",
    "        # Unpack X.\n",
    "        modes, metrics, features = X\n",
    "        # Cast y to appropriate type.\n",
    "        y = y.float()\n",
    "        \n",
    "        if ix in val_ix:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
    "                loss = criterion(y_pred.view(-1, num_classes), y.view(-1, num_classes))\n",
    "                val_losses.append(loss.item())\n",
    "        else:\n",
    "            model.train()\n",
    "            \n",
    "            opt.zero_grad()\n",
    "\n",
    "            y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
    "            loss = criterion(y_pred.view(-1, num_classes), y.view(-1, num_classes))\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        if ix and ix % print_every == 0:\n",
    "                print(\n",
    "                    f\"\\t-> Train loss: {np.nanmean(train_losses)}\\n\\t-> Val loss: {np.nanmean(val_losses)}\"\n",
    "                )\n",
    "                print('\\t'+20*'*')\n",
    "\n",
    "    print(50*'-')\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    \n",
    "    print(\"\\tBeginning evaluation.\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print_every = len(loader)//5\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for ix, (X, y, lengths) in enumerate(loader):\n",
    "        \n",
    "        modes, metrics, features = X\n",
    "\n",
    "        y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
    "        y = y.float()\n",
    "        \n",
    "        loss = criterion(y_pred.view(-1, num_classes), y.view(-1, num_classes))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if ix and ix % print_every == 0:\n",
    "            print(f\"\\t -> Average loss: {np.nanmean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def evaluate_f1(model, tr_loader, val_ix, te_loader=None):\n",
    "    \n",
    "    tr_preds, val_preds, te_preds = np.array([]), np.array([]), np.array([])\n",
    "    tr_gt, val_gt, te_gt = np.array([]), np.array([]), np.array([])\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\\tEvaluating F1...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ix, (X, y, lengths) in enumerate(tr_loader):\n",
    "        \n",
    "            modes, metrics, features = X\n",
    "\n",
    "            y_pred = model(modes, metrics.float(), features.float(), lengths).view(-1, num_classes)\n",
    "            y = y.float().view(-1, num_classes)\n",
    "\n",
    "            preds = torch.argmax(F.softmax(y_pred, dim=-1), dim=-1).numpy().ravel()\n",
    "            true = torch.argmax(y.long(), dim=-1).numpy().ravel()\n",
    "            \n",
    "            if ix in val_ix:\n",
    "                val_preds = np.append(val_preds, preds)\n",
    "                val_gt = np.append(val_gt, true)\n",
    "            else:\n",
    "                tr_preds = np.append(tr_preds, preds)\n",
    "                tr_gt = np.append(tr_gt, true)\n",
    "            \n",
    "        tr_f1 = f1_score(y_true=tr_gt, y_pred=tr_preds, average='weighted')\n",
    "        val_f1 = f1_score(y_true=val_gt, y_pred=val_preds, average='weighted')\n",
    "        print(f\"\\t -> Train F1: {tr_f1}, Val F1: {val_f1}\")\n",
    "        \n",
    "        if not te_loader:\n",
    "            return tr_f1, val_f1, None\n",
    "\n",
    "        for ix, (X, y, lengths) in enumerate(te_loader):\n",
    "        \n",
    "            modes, metrics, features = X\n",
    "\n",
    "            y_pred = model(modes, metrics.float(), features.float(), lengths).view(-1, num_classes)\n",
    "            y = y.float().view(-1, num_classes)\n",
    "            \n",
    "            preds = torch.argmax(F.softmax(y_pred, dim=-1), dim=-1).numpy().ravel()\n",
    "            true = torch.argmax(y.long(), dim=-1).numpy().ravel()\n",
    "\n",
    "            te_preds = np.append(te_preds, preds)\n",
    "            te_gt = np.append(te_gt, true)\n",
    "        \n",
    "        te_f1 = f1_score(y_true=te_gt, y_pred=te_preds, average='weighted')\n",
    "        print(f\"\\t -> Test F1: {te_f1}\")\n",
    "        \n",
    "        return tr_f1, val_f1, te_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other training hyperparameters.\n",
    "num_epochs = 18\n",
    "num_decays = 6\n",
    "decay_at = num_epochs // num_decays\n",
    "decay = 0.9\n",
    "eval_every = 3\n",
    "\n",
    "# Static hold-out val set.\n",
    "n_batches = len(train_loader)\n",
    "val_split = 0.2\n",
    "val_batches = np.random.choice(n_batches, size=(int(val_split * n_batches),), replace=False)\n",
    "\n",
    "# Just checking what LRs should be after decaying.\n",
    "for power in range(num_decays):\n",
    "    print(f\"{decay_at * power} - {decay_at * (power + 1)} :: {INIT_LR * decay**power:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b72de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We'd like to start at a loss of at most -ln(1/9) ~ 2.19\n",
    "\n",
    "# Wrapper to contain all losses.\n",
    "tr_losses, val_losses = list(), list()\n",
    "save_at_best_loss = True\n",
    "best_loss = np.inf\n",
    "model_name = \"../models/LSTM_{epoch}_{loss}.pt\"\n",
    "patience, delta = 2, 0\n",
    "\n",
    "for epoch_ix in range(1, num_epochs+1):\n",
    "    print(f\"Epoch {epoch_ix}:\")\n",
    "    tr_loss, val_loss = train(epoch_ix, model, train_loader, optimizer, criterion, val_batches)\n",
    "    \n",
    "    tr_losses.extend(tr_loss)\n",
    "    val_losses.extend(val_loss)\n",
    "    \n",
    "    mean_val_loss = np.nanmean(val_loss)\n",
    "    \n",
    "    if epoch_ix and epoch_ix % eval_every == 0:\n",
    "        # evaluate(epoch_ix, model, test_loader, criterion)\n",
    "        tr_f1, val_f1, _ = evaluate_f1(model, train_loader, val_batches)\n",
    "    \n",
    "    if mean_val_loss < best_loss and save_at_best_loss:\n",
    "        best_loss = mean_val_loss\n",
    "        \n",
    "        # Reset delta.\n",
    "        delta = 0\n",
    "        \n",
    "        loss_str = str(best_loss).replace(\".\", \"_\")\n",
    "        torch.save(model.state_dict(), model_name.format(epoch=str(epoch_ix), loss=loss_str))\n",
    "        print(\"\\tSaved model checkpoint.\")\n",
    "    else:\n",
    "        # Increase delta by 1.\n",
    "        delta += 1\n",
    "        print(f\"\\tLoss did not decrease. Status is now {delta}/{patience}\")\n",
    "    \n",
    "    # Tolerate for `patience` epochs.\n",
    "    if delta == patience + 1:\n",
    "        # Stop training.\n",
    "        break\n",
    "\n",
    "    if epoch_ix % decay_at == 0:\n",
    "        optimizer.param_groups[0]['lr'] *= decay\n",
    "        print(f\"\\tLearning rate is now: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "    \n",
    "    print(50*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate once on the test set.\n",
    "evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tr_f1, final_val_f1, te_f1 = evaluate_f1(model, train_loader, val_batches, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# ax.plot(tr_losses, 'r-')\n",
    "# ax.plot(val_losses, 'b-')\n",
    "# ax.set_title('Training and Validation losses')\n",
    "# plt.legend(['Training loss', 'Validation loss'], loc='best')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d53498",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "```\n",
    "LR scheme:\n",
    "0 - 5 :: 0.00070\n",
    "5 - 10 :: 0.00067\n",
    "10 - 15 :: 0.00063\n",
    "15 - 20 :: 0.00060\n",
    "20 - 25 :: 0.00057\n",
    "25 - 30 :: 0.00054\n",
    "```\n",
    "\n",
    "```language=python\n",
    "model = Model(\n",
    "    n_lstm_layers=1,\n",
    "    input_size=3,\n",
    "    hidden_size=16, \n",
    "    output_size=9,\n",
    "    n_features=40,\n",
    "    l1_blocks=6,\n",
    "    l2_blocks=6\n",
    ")\n",
    "```\n",
    "\n",
    "\\# params: ~450k\n",
    "\n",
    "mode_embedding = 4\n",
    "\n",
    "Best stats:\n",
    "\t -> Train F1: 0.7047532574096045\n",
    "\t -> Test F1: 0.6560129685481192\n",
    "\n",
    "<hr />\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "Same LR scheme as above.\n",
    "\n",
    "```language=python\n",
    "model = Model(\n",
    "    n_lstm_layers=3,\n",
    "    input_size=3,\n",
    "    hidden_size=32, \n",
    "    output_size=9,\n",
    "    n_features=40,\n",
    "    l1_blocks=4,\n",
    "    l2_blocks=4\n",
    ")\n",
    "```\n",
    "\n",
    "\\# params: 770k\n",
    "\n",
    "mode_embedding = 4\n",
    "\n",
    "Best stats:\n",
    "\t -> Train F1: 0.7365035440256072\n",
    "\t -> Test F1: 0.6610215030981759\n",
    "     \n",
    " <hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8dc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
