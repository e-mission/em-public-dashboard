{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install biogeme: `pip3 install biogeme==3.2.12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.database as db\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta, DefineVariable\n",
    "from biogeme.expressions import Variable\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import f1_score, r2_score, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global experiment flags and variables.\n",
    "SEED = 19348\n",
    "TARGETS = ['p_micro', 'no_trip', 's_car', 'transit', 'car', 's_micro', 'ridehail', 'walk', 'unknown']\n",
    "\n",
    "# Set the Numpy seed too.\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLIT_TYPE(Enum):\n",
    "    INTRA_USER = 0\n",
    "    TARGET = 1\n",
    "    MODE = 2\n",
    "    \n",
    "\n",
    "class SPLIT(Enum):\n",
    "    TRAIN = 0\n",
    "    TEST = 1\n",
    "\n",
    "\n",
    "def get_train_test_splits(data: pd.DataFrame, how=SPLIT_TYPE, test_ratio=0.2, shuffle=True):\n",
    "\n",
    "    if how == SPLIT_TYPE.INTRA_USER:\n",
    "        \n",
    "        # There are certain users with only one observation. What do we do with those?\n",
    "        # As per the mobilitynet modeling pipeline, we randomly assign them to either the\n",
    "        # training or test set.\n",
    "        \n",
    "        value_counts = data.user_id.value_counts()\n",
    "        single_count_ids = value_counts[value_counts == 1].index\n",
    "        \n",
    "        data_filtered = data.loc[~data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "        data_single_counts = data.loc[data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "        \n",
    "        X_tr, X_te = train_test_split(\n",
    "            data_filtered, test_size=test_ratio, shuffle=shuffle, stratify=data_filtered.user_id,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        data_single_counts['assigned'] = np.random.choice(['train', 'test'], len(data_single_counts))\n",
    "        X_tr_merged = pd.concat(\n",
    "            [X_tr, data_single_counts.loc[data_single_counts.assigned == 'train', :].drop(\n",
    "                columns=['assigned'], inplace=False\n",
    "            )],\n",
    "            ignore_index=True, axis=0\n",
    "        )\n",
    "        \n",
    "        X_te_merged = pd.concat(\n",
    "            [X_te, data_single_counts.loc[data_single_counts.assigned == 'test', :].drop(\n",
    "                columns=['assigned'], inplace=False\n",
    "            )],\n",
    "            ignore_index=True, axis=0\n",
    "        )\n",
    "        \n",
    "        return X_tr_merged, X_te_merged\n",
    "    \n",
    "    elif how == SPLIT_TYPE.TARGET:\n",
    "        \n",
    "        X_tr, X_te = train_test_split(\n",
    "            data, test_size=test_ratio, shuffle=shuffle, stratify=data.target,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        return X_tr, X_te\n",
    "    \n",
    "    elif how == SPLIT_TYPE.MODE:\n",
    "        \n",
    "        X_tr, X_te = train_test_split(\n",
    "            data, test_size=test_ratio, shuffle=shuffle, stratify=data.section_mode_argmax,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        return X_tr, X_te\n",
    "    \n",
    "    raise NotImplementedError(\"Unknown split type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are common features across all datasets:\n",
    "\n",
    "```\n",
    "{'age_21___25_years_old', 'cost_unknown', 'start_local_dt_hour', 'av_walk', 'distance', 'duration', 'av_unknown', 'ft_job', 'end_local_dt_hour', 'cost_no_trip', 'cost_s_micro', 'mph', 'n_residents_u18', 'is_paid', 'n_motor_vehicles', 'target', 'n_working_residents', 'section_distance_argmax', 'n_residence_members', 'has_medical_condition', 'primary_job_description_Other', 'cost_walk', 'cost_p_micro', 'av_transit', 'age_16___20_years_old', 'income_category', 'av_s_car', 'av_no_trip', 'cost_s_car', 'multiple_jobs', 'n_residents_with_license', 'section_duration_argmax', 'age_26___30_years_old', 'cost_car', 'av_p_micro', 'av_ridehail', 'av_car', 'cost_transit', 'available_modes', 'av_s_micro', 'has_drivers_license', 'cost_ridehail', 'user_id', 'section_mode_argmax', 'is_student'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data.\n",
    "\n",
    "DATA_SOURCES = [\n",
    "    ('../data/filtered_data/preprocessed_data_Stage_database.csv', 'allceo'),\n",
    "    ('../data/filtered_data/preprocessed_data_openpath_prod_uprm_nicr.csv', 'nicr'),\n",
    "    ('../data/filtered_data/preprocessed_data_openpath_prod_durham.csv', 'durham')\n",
    "]\n",
    "\n",
    "DB_IX = 2\n",
    "\n",
    "PATH = DATA_SOURCES[DB_IX][0]\n",
    "CURRENT_DB = DATA_SOURCES[DB_IX][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(df: pd.DataFrame, split: SPLIT, scaler=None):\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Ignore dummy features (1/0).\n",
    "    ignore_cols = [\n",
    "        c for c in columns if 'age_' in c or 'av_' in c or 'gender_' in c \n",
    "        or 'primary_job_description' in c or 'is_' in c or 'highest_education' in c\n",
    "        or '_job' in c or 'has_' in c\n",
    "    ] + ['user_id', 'target', 'section_mode_argmax']\n",
    "    \n",
    "    data = df.loc[:, [c for c in df.columns if c not in ignore_cols]]\n",
    "    ignored = df.loc[:, ignore_cols]\n",
    "    \n",
    "    if split == SPLIT.TRAIN:\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(data), \n",
    "            columns=data.columns, \n",
    "            index=data.index\n",
    "        )\n",
    "    \n",
    "    elif split == SPLIT.TEST:\n",
    "        scaled = pd.DataFrame(\n",
    "            scaler.transform(data), \n",
    "            columns=data.columns, \n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown split\")\n",
    "    \n",
    "    return pd.concat([scaled, ignored], axis=1), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df: pd.DataFrame):\n",
    "    \n",
    "    to_drop = [\n",
    "        'available_modes'\n",
    "    ]\n",
    "    \n",
    "    for col in to_drop:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_estimate(df: pd.DataFrame, dset: SPLIT, model_dict: dict):\n",
    "    \n",
    "    X_features = ['section_distance_argmax', 'mph']\n",
    "    \n",
    "    if dset == SPLIT.TRAIN and model_dict is None:\n",
    "        model_dict = dict()\n",
    "    \n",
    "    if dset == SPLIT.TEST and model_dict is None:\n",
    "        raise AttributeError(\"Expected model dict for testing.\")\n",
    "    \n",
    "    if dset == SPLIT.TRAIN:\n",
    "        for section_mode in df.section_mode_argmax.unique():\n",
    "            section_data = df.loc[df.section_mode_argmax == section_mode, :]\n",
    "            if section_mode not in model_dict:\n",
    "                model_dict[section_mode] = dict()\n",
    "\n",
    "                model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "                X = section_data[\n",
    "                    X_features\n",
    "                ]\n",
    "                Y = section_data[['section_duration_argmax']]\n",
    "\n",
    "                model.fit(X, Y.values.ravel())\n",
    "\n",
    "                r2 = r2_score(y_pred=model.predict(X), y_true=Y.values.ravel())\n",
    "                print(f\"Train R2 for {section_mode}: {r2}\")\n",
    "\n",
    "                model_dict[section_mode]['model'] = model\n",
    "                \n",
    "    elif dset == SPLIT.TEST:\n",
    "        for section_mode in df.section_mode_argmax.unique():\n",
    "            \n",
    "            section_data = df.loc[df.section_mode_argmax == section_mode, :]\n",
    "            \n",
    "            X = section_data[\n",
    "                X_features\n",
    "            ]\n",
    "            Y = section_data[['section_duration_argmax']]\n",
    "            \n",
    "            if section_mode not in model_dict:\n",
    "                y_pred = [np.nan for _ in range(len(X))]\n",
    "            else:\n",
    "                y_pred = model_dict[section_mode]['model'].predict(X)\n",
    "                \n",
    "            r2 = r2_score(y_pred=y_pred, y_true=Y.values.ravel())\n",
    "            print(f\"Test R2 for {section_mode}: {r2}\")\n",
    "    \n",
    "    # Create the new columns for the duration.\n",
    "    new_columns = ['p_micro','no_trip','s_car','transit','car','s_micro','ridehail','walk','unknown']\n",
    "    df[new_columns] = 0\n",
    "    df['temp'] = 0\n",
    "    \n",
    "    for section in df.section_mode_argmax.unique():\n",
    "        X_section = df.loc[df.section_mode_argmax == section, X_features]\n",
    "        \n",
    "        # broadcast to all columns.\n",
    "        df.loc[df.section_mode_argmax == section, 'temp'] = model_dict[section]['model'].predict(X_section)\n",
    "    \n",
    "    for c in new_columns:\n",
    "        df[c] = df['av_' + c] * df['temp']\n",
    "    \n",
    "    df.drop(columns=['temp'], inplace=True)\n",
    "    \n",
    "    df.rename(columns=dict([(x, 'tt_'+x) for x in new_columns]), inplace=True)\n",
    "    \n",
    "    # return model_dict, result_df\n",
    "    return model_dict, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we drop columns, split the data, and normalize\n",
    "\n",
    "data = drop_columns(data)\n",
    "\n",
    "train_data, test_data = get_train_test_splits(data=data, how=SPLIT_TYPE.INTRA_USER, shuffle=True)\n",
    "\n",
    "train_data, scaler = norm_data(train_data, split=SPLIT.TRAIN)\n",
    "test_data, _ = norm_data(test_data, SPLIT.TEST, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS = list(data.user_id.unique())\n",
    "\n",
    "USER_MAP = {\n",
    "    u: i+1 for (i, u) in enumerate(USERS)\n",
    "}\n",
    "\n",
    "train_data['user_id'] = train_data['user_id'].apply(lambda x: USER_MAP[x])\n",
    "test_data['user_id'] = test_data['user_id'].apply(lambda x: USER_MAP[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 7))\n",
    "train_data.target.hist(ax=ax[0])\n",
    "test_data.target.hist(ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, train_data = get_duration_estimate(train_data, SPLIT.TRAIN, None)\n",
    "print(10 * \"-\")\n",
    "_, test_data = get_duration_estimate(test_data, SPLIT.TEST, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop section_mode\n",
    "\n",
    "train_data.drop(columns=['section_mode_argmax'], inplace=True)\n",
    "# test_data.drop(columns=['section_mode_argmax'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions that will help ease redundancy in the code.\n",
    "\n",
    "def get_database(df: pd.DataFrame, split: SPLIT):\n",
    "    return db.Database(split.name + '_db', df)\n",
    "\n",
    "\n",
    "def get_variables(database: db.Database):\n",
    "    \n",
    "    columns = database.data\n",
    "    \n",
    "    # User-level features.\n",
    "    START_HOUR = Variable('start_local_dt_hour')\n",
    "    END_HOUR = Variable('end_local_dt_hour')\n",
    "    TRIP_DISTANCE = Variable('distance')\n",
    "    INCOME = Variable('income_category')\n",
    "    N_MEMBERS = Variable('n_residence_members')\n",
    "    N_U18 = Variable('n_residents_u18')\n",
    "    N_LICENSE = Variable('n_residents_with_license')\n",
    "    N_VEHICLES = Variable('n_motor_vehicles')\n",
    "    LICENSE = Variable('has_drivers_license')\n",
    "    CONDITION = Variable('has_medical_condition')\n",
    "    FT_JOB = Variable('ft_job')\n",
    "    MULTIPLE_JOBS = Variable('multiple_jobs')\n",
    "    \n",
    "    # Sections\n",
    "    DISTANCE_ARGMAX = Variable('section_distance_argmax')\n",
    "    TT_ARGMAX = Variable('section_duration_argmax')\n",
    "    MPH = Variable('mph')\n",
    "    \n",
    "    # Costs\n",
    "    COST_P_MICRO = Variable('cost_p_micro')\n",
    "    COST_NO_TRIP = Variable('cost_no_trip')\n",
    "    COST_S_CAR = Variable('cost_s_car')\n",
    "    COST_CAR = Variable('cost_car')\n",
    "    COST_S_MICRO = Variable('cost_s_micro')\n",
    "    COST_RIDEHAIL = Variable('cost_ridehail')\n",
    "    COST_WALK = Variable('cost_walk')\n",
    "    COST_UNKNOWN = Variable('cost_unknown')\n",
    "    COST_TRANSIT = Variable('cost_transit')\n",
    "\n",
    "    # Availability.\n",
    "    AV_P_MICRO = Variable('av_p_micro')\n",
    "    AV_NO_TRIP = Variable('av_no_trip')\n",
    "    AV_S_CAR = Variable('av_s_car')\n",
    "    AV_TRANSIT = Variable('av_transit')\n",
    "    AV_CAR = Variable('av_car')\n",
    "    AV_S_MICRO = Variable('av_s_micro')\n",
    "    AV_RIDEHAIL = Variable('av_ridehail')\n",
    "    AV_WALK = Variable('av_walk')\n",
    "    AV_UNKNOWN = Variable('av_unknown')\n",
    "    \n",
    "    # OHE\n",
    "    G = [Variable(x) for x in columns if 'gender_' in x]\n",
    "    E = [Variable(x) for x in columns if 'highest_education' in x]\n",
    "    PJ = [Variable(x) for x in columns if 'primary_job_description' in x]\n",
    "    \n",
    "    # Times.\n",
    "    TT_P_MICRO = Variable('tt_p_micro')\n",
    "    TT_NO_TRIP = Variable('tt_no_trip')\n",
    "    TT_S_CAR = Variable('tt_s_car')\n",
    "    TT_TRANSIT = Variable('tt_transit')\n",
    "    TT_CAR = Variable('tt_car')\n",
    "    TT_S_MICRO = Variable('tt_s_micro')\n",
    "    TT_RIDEHAIL = Variable('tt_ridehail')\n",
    "    TT_WALK = Variable('tt_walk')\n",
    "    TT_UNKNOWN = Variable('tt_unknown')\n",
    "    \n",
    "    # Choice.\n",
    "    CHOICE = Variable('target')\n",
    "    \n",
    "    return_dict = locals().copy()\n",
    "    \n",
    "    # Remove the gender list and place them in the locals dict.\n",
    "    for i, val in enumerate(G):\n",
    "        return_dict.update({'G_' + str(i): val})\n",
    "    \n",
    "    del return_dict['G']\n",
    "    \n",
    "    \n",
    "    ## Education\n",
    "    for i, val in enumerate(E):\n",
    "        return_dict.update({'E_' + str(i): val})\n",
    "    \n",
    "    del return_dict['E']\n",
    "    \n",
    "    ## Job\n",
    "    for i, val in enumerate(PJ):\n",
    "        return_dict.update({'PJ_' + str(i): val})\n",
    "    \n",
    "    del return_dict['PJ']\n",
    "    \n",
    "    # return the filtered locals() dictionary.\n",
    "    return {k:v for k,v in return_dict.items() if not k.startswith('_') and k not in ['database', 'columns']}\n",
    "\n",
    "\n",
    "# def exclude_from_db(v_dict: dict, db: db.Database):\n",
    "#     EXCLUDE = (v_dict['CHOICE'] == 2) + (v_dict['CHOICE'] == 9) > 0\n",
    "#     db.remove(EXCLUDE)\n",
    "\n",
    "def get_params(variables):\n",
    "    \n",
    "    param_dict = {'B_' + k: Beta('B_' + k, 0, None, None, 0) for k in variables.keys()}\n",
    "    \n",
    "    param_dict['ASC_P_MICRO'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_NO_TRIP'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_S_CAR'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_TRANSIT'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_CAR'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_S_MICRO'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_RIDEHAIL'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_WALK'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    param_dict['ASC_UNKNOWN'] = Beta('ASC_P_MICRO', 0, None, None, 0)\n",
    "    \n",
    "    # Return filtered locals dict.\n",
    "    return param_dict\n",
    "\n",
    "\n",
    "def get_utility_functions(v: dict):\n",
    "    \n",
    "    ## User-level utility.\n",
    "    user = 1.\n",
    "    for var in [\n",
    "        'INCOME', 'N_MEMBERS', \n",
    "        'N_U18', 'N_LICENSE', 'N_VEHICLES', 'LICENSE', 'CONDITION', 'FT_JOB', 'MULTIPLE_JOBS'\n",
    "    ]:\n",
    "        user += v[var] * v['B_'+var]\n",
    "    \n",
    "    # OHE (One-hot encoded utility.)\n",
    "    ohe = 1.\n",
    "    ohe_vars = [var for var in v if ('G_' in var or 'E_' in var or 'PJ_' in var) and 'B_' not in var]\n",
    "    for var in ohe_vars:\n",
    "        ohe += v[var] * v['B_'+var]\n",
    "    \n",
    "    ## Trip utility.\n",
    "    trip = 1.\n",
    "    for var in ['MPH', 'DISTANCE_ARGMAX', 'TT_ARGMAX', 'START_HOUR', 'END_HOUR', 'TRIP_DISTANCE']:\n",
    "        trip += v[var] * v['B_' + var]\n",
    "    \n",
    "    \n",
    "    V_P_MICRO = v['ASC_P_MICRO'] + \\\n",
    "        ohe + user + trip + \\\n",
    "        v['TT_P_MICRO'] * v['B_TT_P_MICRO'] + \\\n",
    "        v['COST_P_MICRO'] * v['B_COST_P_MICRO']\n",
    "    \n",
    "    V_S_MICRO = v['ASC_S_MICRO'] + \\\n",
    "        ohe + user + trip + \\\n",
    "        v['TT_S_MICRO'] * v['B_TT_S_MICRO'] + \\\n",
    "        v['COST_S_MICRO'] * v['B_COST_S_MICRO']\n",
    "    \n",
    "    V_S_CAR = v['ASC_S_CAR'] + \\\n",
    "        ohe + user + trip + \\\n",
    "        v['TT_S_CAR'] * v['B_TT_S_CAR'] + \\\n",
    "        v['COST_S_CAR'] * v['B_COST_S_CAR']\n",
    "    \n",
    "    V_CAR = v['ASC_CAR'] + \\\n",
    "        ohe + user + trip + \\\n",
    "        v['TT_CAR'] * v['B_TT_CAR'] + \\\n",
    "        v['COST_CAR'] * v['B_COST_CAR']\n",
    "    \n",
    "    V_TRANSIT = v['ASC_TRANSIT'] + \\\n",
    "        ohe + user + trip + \\\n",
    "        v['TT_TRANSIT'] * v['B_TT_TRANSIT'] + \\\n",
    "        v['COST_TRANSIT'] * v['B_COST_TRANSIT']\n",
    "    \n",
    "    V_WALK = v['ASC_WALK'] + \\\n",
    "        ohe + user + trip + \\\n",
    "        v['TT_WALK'] * v['B_TT_WALK'] + \\\n",
    "        v['COST_WALK'] * v['B_COST_WALK']\n",
    "    \n",
    "    V_RIDEHAIL = v['ASC_RIDEHAIL'] + \\\n",
    "        ohe + user + trip + \\\n",
    "        v['TT_RIDEHAIL'] * v['B_TT_RIDEHAIL'] + \\\n",
    "        v['COST_RIDEHAIL'] * v['B_COST_RIDEHAIL']\n",
    "    \n",
    "    V_NO_TRIP = -100\n",
    "    V_UNKNOWN = -100\n",
    "    \n",
    "    # Remember to exclude the input argument.\n",
    "    return {k:v for k,v in locals().items() if not k.startswith('_') and k != 'v'}\n",
    "\n",
    "\n",
    "def get_utility_mapping(var: dict):\n",
    "    # Map alterative to utility functions.\n",
    "    return {\n",
    "        1: var['V_P_MICRO'], \n",
    "        2: var['V_NO_TRIP'],\n",
    "        3: var['V_S_CAR'], \n",
    "        4: var['V_TRANSIT'],\n",
    "        5: var['V_CAR'], \n",
    "        6: var['V_S_MICRO'],\n",
    "        7: var['V_RIDEHAIL'], \n",
    "        8: var['V_WALK'], \n",
    "        9: var['V_UNKNOWN']\n",
    "    }\n",
    "\n",
    "\n",
    "def get_availability_mapping(var: dict):\n",
    "    return {\n",
    "        1: var['AV_P_MICRO'],\n",
    "        2: var['AV_NO_TRIP'],\n",
    "        3: var['AV_S_CAR'],\n",
    "        4: var['AV_TRANSIT'],\n",
    "        5: var['AV_CAR'],\n",
    "        6: var['AV_S_MICRO'],\n",
    "        7: var['AV_RIDEHAIL'],\n",
    "        8: var['AV_WALK'],\n",
    "        9: var['AV_UNKNOWN']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, drop columns.\n",
    "\n",
    "# train_data = drop_columns(train_data)\n",
    "\n",
    "# train_data, scaler = norm_data(train_data, split=SPLIT.TRAIN)\n",
    "\n",
    "# get dbs.\n",
    "train_db = get_database(train_data, SPLIT.TRAIN)\n",
    "\n",
    "# get vars.\n",
    "train_vars = get_variables(train_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = get_params(train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars.update(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_V = get_utility_functions(train_vars)\n",
    "train_vars.update(train_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = get_utility_mapping(train_vars)\n",
    "av = get_availability_mapping(train_vars)\n",
    "logprob = models.loglogit(V, av, train_vars['CHOICE'])\n",
    "\n",
    "# logit1 = models.logit(V, av, 1)\n",
    "# logit2 = models.logit(V, av, 2)\n",
    "# logit3 = models.logit(V, av, 3)\n",
    "# logit4 = models.logit(V, av, 4)\n",
    "# logit5 = models.logit(V, av, 5)\n",
    "# logit6 = models.logit(V, av, 6)\n",
    "# logit7 = models.logit(V, av, 7)\n",
    "# logit8 = models.logit(V, av, 8)\n",
    "# logit9 = models.logit(V, av, 9)\n",
    "\n",
    "# models = {f'logit_{ix}': logit for ix, logit in enumerate(\n",
    "#     [logit1, logit2, logit3, logit4, logit5, logit6, logit7, logit8, logit9]\n",
    "# )}\n",
    "\n",
    "model = bio.BIOGEME(train_db, logprob)\n",
    "model.modelName = 'customUtility-new'\n",
    "model.generate_html = False\n",
    "model.generate_pickle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = model.estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_results.short_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_results.getEstimatedParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biogeme.expressions import Derive\n",
    "\n",
    "\n",
    "def simulate_results(V, av, db, beta_dict):\n",
    "    \n",
    "    wtp = {\n",
    "        'WTP s_car': Derive(V[3], 'tt_s_car')/Derive(V[3], 'scaled_cost_s_car'),\n",
    "        'WTP transit': Derive(V[4], 'tt_transit')/Derive(V[4], 'scaled_cost_transit'),\n",
    "        'WTP car': Derive(V[5], 'tt_car')/Derive(V[5], 'scaled_cost_car'),\n",
    "        'WTP s_micro': Derive(V[6], 'tt_s_micro')/Derive(V[6], 'scaled_cost_s_micro'),\n",
    "        'WTP ridehail': Derive(V[7], 'tt_ridehail')/Derive(V[7], 'scaled_cost_ridehail')\n",
    "    }\n",
    "    \n",
    "    prob_labels = ['Prob. ' + x for x in TARGETS]\n",
    "    probs = [models.logit(V, av, i+1) for i in range(len(prob_labels))]\n",
    "    \n",
    "    simulate = dict(zip(prob_labels, probs))\n",
    "    \n",
    "    # simulate.update(wtp)\n",
    "    \n",
    "    biosim = bio.BIOGEME(db, simulate)\n",
    "    biosim.modelName = 'test-3'\n",
    "    \n",
    "    return biosim.simulate(theBetaValues=beta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = drop_columns(test_data)\n",
    "\n",
    "# Scale cost.\n",
    "test_data, _ = norm_data(test_data, SPLIT.TEST, scaler)\n",
    "\n",
    "test_data.drop(columns=['section_mode_argmax'], inplace=True)\n",
    "\n",
    "# get dbs.\n",
    "test_db = get_database(test_data, SPLIT.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = simulate_results(V, av, test_db, train_results.getBetaValues())\n",
    "# test_utilities = get_utility_df(train_results, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_probs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax starts from 0. Offset all predicted indices by 1.\n",
    "choices = np.argmax(test_probs.values, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_data.chosen\n",
    "score = f1_score(y_true, choices, average='weighted')\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "counts = pd.Series(choices).value_counts()\n",
    "ix = counts.index.tolist()\n",
    "_x = [i+1 for i in range(len(TARGETS))]\n",
    "height = [0 if i not in ix else counts[i] for i in _x]\n",
    "ax.bar(x=_x, height=height)\n",
    "ax.set_xticks(range(1, 10, 1))\n",
    "ax.set_xticklabels(TARGETS, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cm = ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=choices, ax=ax)\n",
    "\n",
    "y_unique = np.unique(y_true)\n",
    "labelset = [t for i, t in enumerate(TARGETS) if (i+1) in y_unique]\n",
    "\n",
    "ax.set_xticklabels(labelset, rotation=45)\n",
    "ax.set_yticklabels(labelset)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.diag(cm.confusion_matrix)/np.sum(cm.confusion_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_np = test_utilities.values\n",
    "# choice_df = np.exp(u_np)/np.sum(np.exp(u_np), axis=1, keepdims=True)\n",
    "\n",
    "# choice_df = pd.DataFrame(choice_df, columns=test_utilities.columns)\n",
    "# display(choice_df.head())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab0c6e94c9422d07d42069ec9e3bb23090f5e156fc0e23cc25ca45a62375bf53"
  },
  "kernelspec": {
   "display_name": "emission",
   "language": "python",
   "name": "emission"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
