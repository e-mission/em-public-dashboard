{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789df947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from typing import List, Dict, Union\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 13210\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = [\n",
    "    ('./data/filtered_data/preprocessed_data_Stage_database.csv', 'allceo'),\n",
    "    ('./data/filtered_data/preprocessed_data_openpath_prod_durham.csv', 'durham'),\n",
    "    ('./data/filtered_data/preprocessed_data_openpath_prod_mm_masscec.csv', 'masscec'),\n",
    "    ('./data/filtered_data/preprocessed_data_openpath_prod_ride2own.csv', 'ride2own'),\n",
    "    ('./data/filtered_data/preprocessed_data_openpath_prod_uprm_nicr.csv', 'nicr')\n",
    "]\n",
    "\n",
    "# Switch between 0-4\n",
    "DB_NUMBER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this name to something unique\n",
    "PATH = DATA_SOURCE[DB_NUMBER][0]\n",
    "CURRENT_DB = DATA_SOURCE[DB_NUMBER][1]\n",
    "\n",
    "OUTPUT_DIR = Path('./outputs')\n",
    "\n",
    "if not OUTPUT_DIR.exists():\n",
    "    OUTPUT_DIR.mkdir()\n",
    "\n",
    "df = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f69976",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_needed = ['deprecatedID', 'data.key']\n",
    "\n",
    "for col in not_needed:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2281bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(\n",
    "    columns={'end_local_dt_hour': 'end:hour', 'start_local_dt_hour': 'start:hour', 'replaced_mode': 'target'}, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = ['p_micro', 'no_trip', 's_car', 'transit', 'car', 's_micro', 'ridehail', 'walk', 'unknown']\n",
    "MAP = {ix+1: t for (ix, t) in enumerate(TARGETS)}\n",
    "TARGET_MAP = {v:k for k, v in MAP.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'target': TARGET_MAP}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of trips per mode.\n",
    "trip_percents = df.groupby(['user_id'])['section_mode_argmax'].apply(lambda x: x.value_counts(normalize=True)).unstack(level=-1)\n",
    "trip_percents.fillna(0., inplace=True)\n",
    "\n",
    "trip_percents.columns = ['coverage_'+x for x in trip_percents.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trips = pd.DataFrame(df.groupby('user_id').size(), columns=['n_trips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff378a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_start = df.groupby('user_id')['start:hour'].apply(lambda x: x.value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbd401",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_end = df.groupby('user_id')['end:hour'].apply(lambda x: x.value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of distance in each primary sensed mode.\n",
    "total_distance = df.groupby(['user_id', 'section_mode_argmax'])['section_distance_argmax'].sum().unstack(level=-1)\n",
    "total_distance = total_distance.div(total_distance.sum(axis=1), axis=0)\n",
    "total_distance.fillna(0., inplace=True)\n",
    "total_distance.columns = ['pct_distance_' + x for x in total_distance.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc0a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure1_df = trip_percents.merge(right=total_distance, left_index=True, right_index=True).merge(\n",
    "   right=n_trips, left_index=True, right_index=True\n",
    ").merge(\n",
    "   right=most_common_start, left_index=True, right_index=True\n",
    ").merge(right=most_common_end, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750fbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the last three columns.\n",
    "\n",
    "def min_max_normalize(col: pd.Series):\n",
    "    _max, _min = col.max(), col.min()\n",
    "    return pd.Series((col - _min)/(_max - _min))\n",
    "\n",
    "figure1_df['n_trips'] = min_max_normalize(figure1_df['n_trips'])\n",
    "figure1_df['start:hour'] = np.sin(figure1_df['start:hour'].values)\n",
    "figure1_df['end:hour'] = np.sin(figure1_df['end:hour'].values)\n",
    "\n",
    "figure1_df.fillna(0., inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f5a04",
   "metadata": {},
   "source": [
    "### Uncomment the following if you want to find the best eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilons = np.linspace(1e-3, 1., 1000)\n",
    "\n",
    "# best_eps = -np.inf\n",
    "# best_score = -np.inf\n",
    "\n",
    "# for eps in epsilons:\n",
    "#     model = DBSCAN(eps=eps).fit(figure1_df)\n",
    "    \n",
    "#     if len(np.unique(model.labels_)) < 2:\n",
    "#         continue\n",
    "    \n",
    "#     score = silhouette_score(figure1_df, model.labels_)\n",
    "#     if score > best_score:\n",
    "#         best_eps = eps\n",
    "#         best_score = score\n",
    "\n",
    "# print(best_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AlLCEO: eps=0.542\n",
    "durham: eps=0.661\n",
    "masscec: eps=0.64\n",
    "'''\n",
    "\n",
    "clustering = DBSCAN(eps=0.8).fit(figure1_df)\n",
    "\n",
    "print(Counter(clustering.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9a7c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After clustering, we would like to see what the replaced mode argmax distribution in each cluster is.\n",
    "\n",
    "labels = clustering.labels_\n",
    "\n",
    "for cix in np.unique(labels):\n",
    "    cluster_users = figure1_df.iloc[labels == cix,:].index\n",
    "    \n",
    "    print(f\"{len(cluster_users)} users in cluster {cix}\")\n",
    "    \n",
    "    # Now, for each user, look at the actual data and determine the replaced mode argmax distribution.\n",
    "    sub_df = df.loc[df.user_id.isin(cluster_users), :].reset_index(drop=True)\n",
    "    \n",
    "    sub_df['target'] =  sub_df['target'].apply(lambda x: MAP[x])\n",
    "    \n",
    "    rm_argmax = sub_df.groupby('user_id')['target'].apply(lambda x: x.value_counts().idxmax())\n",
    "    fig, ax = plt.subplots()\n",
    "    rm_argmax.hist(ax=ax)\n",
    "    ax.set_title(f\"Replaced mode argmax distribution for users in cluster {cix}\")\n",
    "    ax.set_xlabel(\"Target\")\n",
    "    \n",
    "    plt.savefig(OUTPUT_DIR / f'{CURRENT_DB}__FIG1_cluster_{cix}_target_dist.png', dpi=300)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_target_pct = pd.DataFrame()\n",
    "\n",
    "# For every user, compute the replaced mode distribution.\n",
    "for user_id, user_data in df.groupby('user_id'):\n",
    "    \n",
    "    target_distribution = user_data['target'].value_counts(normalize=True)\n",
    "    target_distribution.rename(index=MAP, inplace=True)\n",
    "    user_target_pct = pd.concat([user_target_pct, target_distribution.to_frame(user_id).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99369dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_target_pct.columns = ['pct_trips_' + str(x) for x in user_target_pct.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distance = pd.DataFrame()\n",
    "\n",
    "# For every user, compute the replaced mode distribution.\n",
    "for user_id, user_data in df.groupby('user_id'):\n",
    "    \n",
    "    # total_distance = user_data['distance'].sum()\n",
    "    distance_per_target = user_data.groupby('target')['section_distance_argmax'].sum()\n",
    "    distance_per_target.rename(index=MAP, inplace=True)\n",
    "    row = distance_per_target.to_frame(user_id).T\n",
    "    target_distance = pd.concat([target_distance, row])\n",
    "    \n",
    "target_distance.columns = ['distance_' + str(x) for x in target_distance.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18093734",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_duration = df.groupby(['user_id', 'target'])['section_duration_argmax'].sum().unstack()\n",
    "target_duration.rename(columns=MAP, inplace=True)\n",
    "target_duration.fillna(0., inplace=True)\n",
    "target_duration.columns = ['duration_' + str(x) for x in target_duration.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = user_target_pct.merge(right=target_distance, left_index=True, right_index=True).merge(\n",
    "    right=target_duration, left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "target_df.fillna(0., inplace=True)\n",
    "\n",
    "target_df = pd.DataFrame(\n",
    "    MinMaxScaler().fit_transform(target_df),\n",
    "    columns=target_df.columns,\n",
    "    index=target_df.index\n",
    ")\n",
    "\n",
    "display(target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4f246",
   "metadata": {},
   "source": [
    "### Uncomment if you want to find the best eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fecc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilons = np.linspace(5e-3, 1., 1500)\n",
    "# best_score = -np.inf\n",
    "# best_eps = None\n",
    "# best_n = None\n",
    "# # alpha = 0.7\n",
    "# beta = 0.05\n",
    "\n",
    "# for eps in epsilons:\n",
    "#     for n in range(2, 30):\n",
    "#         labels = DBSCAN(eps=eps, min_samples=n).fit(target_df).labels_\n",
    "    \n",
    "#         n_unique = np.unique(labels)\n",
    "#         n_outliers = len(labels[labels == -1])\n",
    "        \n",
    "#         if n_outliers == len(labels) or len(n_unique) < 2:\n",
    "#             continue\n",
    "    \n",
    "#         # Encourage more clustering and discourage more outliers.\n",
    "#         score = silhouette_score(target_df, labels) + (len(labels) - n_outliers)/n_outliers\n",
    "        \n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_eps = eps\n",
    "#             best_n = n\n",
    "\n",
    "# print(f\"{best_score=}, {best_n=}, {best_eps=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.35 is a good value\n",
    "\n",
    "'''\n",
    "allCEO = DBSCAN(eps=0.52, min_samples=2)\n",
    "durham: DBSCAN(eps=best_eps, min_samples=2)\n",
    "masscec: min_samples=2, eps=0.986724482988659\n",
    "'''\n",
    "\n",
    "cl2 = DBSCAN(eps=0.6, min_samples=2).fit(target_df)\n",
    "# cl2 = KMeans(n_clusters=5).fit(target_df)\n",
    "\n",
    "Counter(cl2.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tsfm = PCA(n_components=2).fit_transform(target_df)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x=tsfm[:,0], y=tsfm[:,1], c=cl2.labels_)\n",
    "ax.set(xlabel='Latent Dim 0', ylabel='Latent Dim 1')\n",
    "plt.savefig(OUTPUT_DIR / f'{CURRENT_DB}__Fig2__PCA_w_colors.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e444316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-cluster users.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "demographic_cols = {\n",
    "    'allceo': [ \n",
    "        'has_drivers_license', 'is_student', 'is_paid', 'income_category', 'n_residence_members', \n",
    "        'n_residents_u18', 'n_residents_with_license', 'n_motor_vehicles',\n",
    "        'has_medical_condition', 'ft_job', 'multiple_jobs', 'n_working_residents', \n",
    "        \"highest_education_Bachelor's degree\", 'highest_education_Graduate degree or professional degree', \n",
    "        'highest_education_High school graduate or GED', 'highest_education_Less than a high school graduate', \n",
    "        'highest_education_Prefer not to say', 'highest_education_Some college or associates degree', \n",
    "        'primary_job_description_Clerical or administrative support', 'primary_job_description_Custodial', \n",
    "        'primary_job_description_Education', \n",
    "        'primary_job_description_Manufacturing, construction, maintenance, or farming', \n",
    "        'primary_job_description_Medical/healthcare', 'primary_job_description_Other', 'gender_Man', \n",
    "        'gender_Man;Nonbinary/genderqueer/genderfluid', 'gender_Nonbinary/genderqueer/genderfluid', \n",
    "        'gender_Prefer not to say', 'gender_Test', 'gender_Woman', 'gender_Woman;Nonbinary/genderqueer/genderfluid', \n",
    "        'age_16___20_years_old', 'age_1___5_years_old', 'age_21___25_years_old', 'age_26___30_years_old', \n",
    "        'age_31___35_years_old', 'age_36___40_years_old', 'age_41___45_years_old', 'age_46___50_years_old', \n",
    "        'age_51___55_years_old', 'age_56___60_years_old', 'age_61___65_years_old', 'age___65_years_old', \n",
    "        'av_s_car', 'av_walk', 'av_ridehail', 'av_s_micro', 'av_transit', 'av_no_trip', 'av_car', 'av_unknown', \n",
    "        'av_p_micro'\n",
    "    ],\n",
    "    'durham': [\n",
    "         'is_student', 'is_paid', 'has_drivers_license', 'n_residents_u18', 'n_residence_members', 'income_category',\n",
    "         'n_residents_with_license', 'n_working_residents', 'n_motor_vehicles', 'has_medical_condition', 'ft_job',\n",
    "         'multiple_jobs', 'highest_education_bachelor_s_degree', 'highest_education_graduate_degree_or_professional_degree',\n",
    "         'highest_education_high_school_graduate_or_ged', 'highest_education_less_than_a_high_school_graduate',\n",
    "         'highest_education_some_college_or_associates_degree', 'primary_job_description_Clerical or administrative support',\n",
    "         'primary_job_description_Manufacturing, construction, maintenance, or farming',\n",
    "         'primary_job_description_Other', 'primary_job_description_Prefer not to say', 'primary_job_description_Professional, Manegerial, or Technical',\n",
    "         'primary_job_description_Sales or service', 'gender_man', 'gender_non_binary_genderqueer_gender_non_confor',\n",
    "         'gender_prefer_not_to_say', 'gender_woman', 'age_16___20_years_old', 'age_21___25_years_old', 'age_26___30_years_old',\n",
    "         'age_31___35_years_old', 'age_36___40_years_old', 'age_41___45_years_old', 'age_46___50_years_old',\n",
    "         'age_51___55_years_old', 'age_56___60_years_old', 'av_unknown', 'av_no_trip', 'av_s_micro', 'av_s_car', 'av_car',\n",
    "         'av_p_micro', 'av_walk', 'av_transit', 'av_ridehail'\n",
    "    ],\n",
    "    'nicr': [\n",
    "\n",
    "        'is_student', 'is_paid', 'has_drivers_license', 'n_residents_u18', 'n_residence_members', \n",
    "        'income_category', 'n_residents_with_license', 'n_working_residents', 'n_motor_vehicles', \n",
    "        'has_medical_condition', 'ft_job', 'multiple_jobs', 'highest_education_bachelor_s_degree', \n",
    "        'highest_education_high_school_graduate_or_ged', 'highest_education_prefer_not_to_say', \n",
    "        'highest_education_some_college_or_associates_degree', \n",
    "        'primary_job_description_Clerical or administrative support', 'primary_job_description_Other', \n",
    "        'gender_man', 'gender_woman', 'age_16___20_years_old', 'age_21___25_years_old', 'age_26___30_years_old', \n",
    "        'av_s_car', 'av_no_trip', 'av_s_micro', 'av_walk', 'av_unknown', 'av_p_micro', 'av_transit', 'av_car', \n",
    "        'av_ridehail'\n",
    "    ],\n",
    "    'masscec': [\n",
    "        'is_student', 'is_paid', 'has_drivers_license', 'n_residents_u18', 'n_residence_members', \n",
    "        'income_category', 'n_residents_with_license', 'n_working_residents', \n",
    "        'n_motor_vehicles', 'has_medical_condition', 'ft_job', 'multiple_jobs', \n",
    "        'highest_education_bachelor_s_degree', 'highest_education_graduate_degree_or_professional_degree',\n",
    "        'highest_education_high_school_graduate_or_ged', \n",
    "        'highest_education_less_than_a_high_school_graduate', 'highest_education_prefer_not_to_say', \n",
    "        'highest_education_some_college_or_associates_degree', \n",
    "        'primary_job_description_Clerical or administrative support', \n",
    "        'primary_job_description_Manufacturing, construction, maintenance, or farming', \n",
    "        'primary_job_description_Other', 'primary_job_description_Prefer not to say', \n",
    "        'primary_job_description_Professional, Manegerial, or Technical', \n",
    "        'primary_job_description_Sales or service', 'gender_man', \n",
    "        'gender_non_binary_genderqueer_gender_non_confor', 'gender_prefer_not_to_say', 'gender_woman', \n",
    "        'age_16___20_years_old', 'age_21___25_years_old', 'age_26___30_years_old', \n",
    "        'age_31___35_years_old', 'age_36___40_years_old', 'age_41___45_years_old', \n",
    "        'age_46___50_years_old', 'age_51___55_years_old', 'age_56___60_years_old', \n",
    "        'age_61___65_years_old', 'age___65_years_old', 'av_no_trip', 'av_transit', \n",
    "        'av_ridehail', 'av_walk', 'av_car', 'av_p_micro', 'av_unknown', 'av_s_micro', 'av_s_car'\n",
    "    ],\n",
    "    'ride2own': [\n",
    "        'has_drivers_license', 'is_student', 'is_paid', 'income_category', 'n_residence_members', \n",
    "        'n_working_residents', 'n_residents_u18', 'n_residents_with_license', 'n_motor_vehicles', \n",
    "        'has_medical_condition', 'ft_job', 'multiple_jobs', 'highest_education_bachelor_s_degree', \n",
    "        'highest_education_graduate_degree_or_professional_degree', \n",
    "        'highest_education_high_school_graduate_or_ged', \n",
    "        'highest_education_less_than_a_high_school_graduate', \n",
    "        'highest_education_some_college_or_associates_degree', 'primary_job_description_Other', \n",
    "        'primary_job_description_Professional, Manegerial, or Technical', \n",
    "        'primary_job_description_Sales or service', 'gender_man', \n",
    "        'gender_non_binary_genderqueer_gender_non_confor', 'gender_woman', 'age_16___20_years_old', \n",
    "        'age_21___25_years_old', 'age_26___30_years_old', 'age_31___35_years_old', \n",
    "        'age_36___40_years_old', 'age_41___45_years_old', 'age_51___55_years_old', \n",
    "        'age_56___60_years_old', 'age___65_years_old', 'av_p_micro', 'av_s_car', 'av_car', \n",
    "        'av_ridehail', 'av_walk', 'av_transit', 'av_no_trip', 'av_s_micro', 'av_unknown'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "cluster_labels = cl2.labels_\n",
    "demographics = df.groupby('user_id').first()[demographic_cols[CURRENT_DB]]\n",
    "demographics = demographics.loc[target_df.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c6355",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### DEMOGRAPHICS\n",
    "\n",
    "def entropy(x):\n",
    "    # Compute bincount, normalize over the entire size. Gives us probabilities.\n",
    "    p = np.unique(x, return_counts=True)[1]/len(x)\n",
    "    # Compute the enropy usnig the probabilities.\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "def preprocess_demo_data(df: pd.DataFrame):\n",
    "    return df\n",
    "\n",
    "\n",
    "within_cluster_homogeneity = dict()\n",
    "other_cluster_homogeneity = dict()\n",
    "labels = cl2.labels_\n",
    "\n",
    "for cix in np.unique(labels):\n",
    "    within_cluster_homogeneity[cix] = dict()\n",
    "    users = target_df[labels == cix].index\n",
    "    data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "    processed = preprocess_demo_data(data)\n",
    "    \n",
    "    for col in processed.columns:\n",
    "        # Numeric/ordinal values. Use std. to measure homogeneity.\n",
    "        if col in [\n",
    "            'n_residence_members', 'n_residents_u18', 'n_working_residents', 'n_motor_vehicles',\n",
    "            'n_residents_with_license', 'income_category'\n",
    "        ]:\n",
    "            within_cluster_homogeneity[cix][col] = processed[col].std()\n",
    "        else:\n",
    "            within_cluster_homogeneity[cix][col] = entropy(processed[col])\n",
    "\n",
    "# Compute average homogeneity across other clusters.\n",
    "for cix in within_cluster_homogeneity.keys():\n",
    "    other_cluster_homogeneity[cix] = dict()\n",
    "    other_clusters = set(within_cluster_homogeneity.keys()) - set([cix])\n",
    "    for feature in within_cluster_homogeneity[cix].keys():\n",
    "        homogeneity_in_others = [within_cluster_homogeneity[x][feature] for x in other_clusters]\n",
    "        other_cluster_homogeneity[cix][feature] = np.mean(homogeneity_in_others)\n",
    "\n",
    "        \n",
    "# Compute contrastive homogeneity\n",
    "# CH = homogeneity within cluster / average homogeneity across other clusters\n",
    "for cix in within_cluster_homogeneity.keys():\n",
    "    ch_scores = list()\n",
    "    print(f\"For cluster {cix}:\")\n",
    "    for feature in within_cluster_homogeneity[cix].keys():\n",
    "        feature_ch = within_cluster_homogeneity[cix][feature]/(other_cluster_homogeneity[cix][feature] + 1e-6)\n",
    "        ch_scores.append((feature, feature_ch))\n",
    "    \n",
    "    ch_df = pd.DataFrame(ch_scores, columns=['feature', 'ch']).sort_values(by=['ch']).head(4)\n",
    "    \n",
    "    # Display actual values.\n",
    "    users = target_df[labels == cix].index\n",
    "    data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "    processed = preprocess_demo_data(data)\n",
    "    \n",
    "    display(ch_df)\n",
    "    print()\n",
    "    filtered = processed.loc[:, processed.columns.isin(ch_df.feature)][ch_df.feature]\n",
    "    filtered_features = ch_df.feature.tolist()\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "    for i, a in enumerate(ax.flatten()):\n",
    "        sns.histplot(filtered[filtered_features[i]], ax=a, stat=\"percent\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CURRENT_DB}_{cix}_Demographic_consistency.png\", dpi=300)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def get_trip_summary_df(users, df):\n",
    "    '''\n",
    "    Group the trips by user ID and argmax_mode and compute trip summaries. Additional\n",
    "    statistics that could be incorporated: IQR.\n",
    "    \n",
    "    mode_coverage computes trips summaries for the sections with the most-traveled distance.\n",
    "    '''\n",
    "    \n",
    "    costs = [c for c in df.columns if 'av_' in c]\n",
    "    \n",
    "    mode_coverage = df.groupby(['user_id', 'section_mode_argmax'])[\n",
    "        ['section_duration_argmax', 'section_distance_argmax', 'mph'] + costs\n",
    "    ].agg(['mean', 'median']).unstack()\n",
    "    \n",
    "    global_stats = df.groupby('user_id')[['duration', 'distance']].agg(\n",
    "        ['mean', 'median']\n",
    "    )\n",
    "\n",
    "    mode_coverage.columns = mode_coverage.columns.map('_'.join)\n",
    "    global_stats.columns = global_stats.columns.map('_'.join)\n",
    "    \n",
    "    # return mode_coverage\n",
    "    return mode_coverage.merge(right=global_stats, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad2485",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TRIP SUMMARIES\n",
    "\n",
    "# Per-cluster users.\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "labels = cl2.labels_\n",
    "\n",
    "def get_data(cix):\n",
    "    users = target_df.iloc[labels == cix, :].index\n",
    "    \n",
    "    # Compute trip summaries.\n",
    "    X = df.loc[df.user_id.isin(users), [\n",
    "        'section_distance_argmax', 'duration', 'distance', 'section_mode_argmax',\n",
    "        'section_duration_argmax', 'mph', 'target', 'user_id'\n",
    "    ] + [c for c in df.columns if 'cost_' in c]].reset_index(drop=True)\n",
    "    \n",
    "    # Compute the target distribution and select the argmax.\n",
    "    target_distribution = X.target.value_counts(ascending=False, normalize=True)\n",
    "    target_distribution.rename(index=MAP, inplace=True)\n",
    "    \n",
    "    # Caution - this summary df has NaNs. Use nanstd() to compute nan-aware std.\n",
    "    subset = get_trip_summary_df(users, X)\n",
    "    \n",
    "    norm_subset = pd.DataFrame(\n",
    "        MinMaxScaler().fit_transform(subset),\n",
    "        columns=subset.columns, index=subset.index\n",
    "    )\n",
    "    \n",
    "    return norm_subset, target_distribution\n",
    "\n",
    "\n",
    "in_cluster_homogeneity = dict()\n",
    "out_cluster_homogeneity = dict()\n",
    "\n",
    "for cluster_ix in np.unique(labels):\n",
    "    in_cluster_homogeneity[cluster_ix] = dict()\n",
    "    norm_subset, _ = get_data(cluster_ix)\n",
    "    for feature in norm_subset.columns:\n",
    "        in_cluster_homogeneity[cluster_ix][feature] = np.nanstd(norm_subset[feature])\n",
    "\n",
    "for cix in in_cluster_homogeneity.keys():\n",
    "    out_cluster_homogeneity[cix] = dict()\n",
    "    oix = set(labels) - set([cix])\n",
    "    for feature in norm_subset.columns:\n",
    "        out_cluster_homogeneity[cix][feature] = np.nanmean([in_cluster_homogeneity[x].get(feature, np.nan) for x in oix])\n",
    "\n",
    "# Now, compute the per-cluster homogeneity.\n",
    "for cix in in_cluster_homogeneity.keys():\n",
    "    ch = list()\n",
    "    for feature in in_cluster_homogeneity[cix].keys():\n",
    "        if feature in in_cluster_homogeneity[cix] and feature in out_cluster_homogeneity[cix]:\n",
    "            ratio = in_cluster_homogeneity[cix][feature] / (out_cluster_homogeneity[cix][feature] + 1e-6)\n",
    "        ch.append([feature, ratio])\n",
    "    \n",
    "    ch_df = pd.DataFrame(ch, columns=['feature', 'ch']).sort_values(by=['ch']).head(4)\n",
    "    data, target_dist = get_data(cix)\n",
    "    \n",
    "    features = ch_df.feature.tolist()\n",
    "    \n",
    "    print(f\"For cluster {cix}:\")\n",
    "    display(target_dist)\n",
    "    display(ch_df)\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "    for i, a in enumerate(ax.flatten()):\n",
    "        sns.histplot(data[features[i]], ax=a, stat=\"percent\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CURRENT_DB}_{cix}_Trip_consistency.png\", dpi=300)\n",
    "    plt.show()\n",
    "    print()\n",
    "    \n",
    "    print(50*'=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992ff45",
   "metadata": {},
   "source": [
    "## Now check the combined homogeneity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8723e3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ic, oc = dict(), dict()\n",
    "\n",
    "labels = cl2.labels_\n",
    "TOP_K = 3\n",
    "\n",
    "\n",
    "for cix in np.unique(labels):\n",
    "    ic[cix] = dict()\n",
    "    \n",
    "    # Trip characteristics.\n",
    "    norm_subset, _ = get_data(cix)\n",
    "    for feature in norm_subset.columns:\n",
    "        ic[cix][feature] = np.nanstd(norm_subset[feature])\n",
    "    \n",
    "    # Demographics.\n",
    "    users = target_df[labels == cix].index\n",
    "    data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "    processed = preprocess_demo_data(data)\n",
    "    \n",
    "    for col in processed.columns:\n",
    "        # Numeric/ordinal values. Use std. to measure homogeneity.\n",
    "        if col in [\n",
    "            'n_residence_members', 'n_residents_u18', 'n_working_residents', 'n_motor_vehicles',\n",
    "            'n_residents_with_license', 'income_category'\n",
    "        ]:\n",
    "            ic[cix][col] = np.nanstd(processed[col])\n",
    "        else:\n",
    "            ic[cix][col] = entropy(processed[col])\n",
    "\n",
    "for cix in ic.keys():\n",
    "    oc[cix] = dict()\n",
    "    oix = set(labels) - set([cix])\n",
    "    for feature in ic[cix].keys():\n",
    "        oc[cix][feature] = np.nanmean([ic[x].get(feature, np.nan) for x in oix])\n",
    "\n",
    "per_cluster_most_homogeneous = dict()\n",
    "\n",
    "# Now, compute the per-cluster homogeneity.\n",
    "ax_ix = 0\n",
    "for cix in ic.keys():\n",
    "\n",
    "    print(f\"For cluster {cix}:\")\n",
    "\n",
    "    # For each, cluster, we will have (TOP_K x n_clusters) figures.\n",
    "    fig, ax = plt.subplots(nrows=TOP_K, ncols=len(ic.keys()), figsize=(12, 8))\n",
    "\n",
    "    other_ix = set(ic.keys()) - set([cix])\n",
    "    \n",
    "    ch = list()\n",
    "    for feature in ic[cix].keys():\n",
    "        if feature in oc[cix]:\n",
    "            ratio = ic[cix][feature] / (oc[cix][feature] + 1e-6)\n",
    "        ch.append([feature, ratio])\n",
    "    \n",
    "    # Just the top k.\n",
    "    ch_df = pd.DataFrame(ch, columns=['feature', 'ch']).sort_values(by=['ch']).reset_index(drop=True).head(TOP_K)\n",
    "\n",
    "    figure_data = dict()\n",
    "    \n",
    "    # Get the actual trip summary data.\n",
    "    trip_summary_data, target_dist = get_data(cix)\n",
    "    \n",
    "    # Get the actual demographic data.\n",
    "    users = target_df[labels == cix].index\n",
    "    data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "    processed = preprocess_demo_data(data)\n",
    "\n",
    "    # Left-most subplot will be that of the current cluster's feature.\n",
    "    for row_ix, row in ch_df.iterrows():\n",
    "        if row.feature in trip_summary_data.columns:\n",
    "            sns.histplot(trip_summary_data[row.feature], ax=ax[row_ix][0], stat='percent').set_title(\"Current cluster\")\n",
    "        else:\n",
    "            sns.histplot(processed[row.feature], ax=ax[row_ix][0], stat='percent').set_title(\"Current cluster\")\n",
    "        ax[row_ix][0].set_xlabel(ax[row_ix][0].get_xlabel(), fontsize=8)\n",
    "        ax[row_ix][0].set_ylim(0., 100.)\n",
    "\n",
    "    offset_col_ix = 1\n",
    "    ## Now, others.\n",
    "    for oix in other_ix:\n",
    "        # Get the actual trip summary data.\n",
    "        other_summary_data, _ = get_data(oix)\n",
    "        \n",
    "        # Get the actual demographic data.\n",
    "        users = target_df[labels == oix].index\n",
    "        data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "        other_demo = preprocess_demo_data(data)\n",
    "\n",
    "        for row_ix, row in ch_df.iterrows():\n",
    "            if row.feature in other_summary_data.columns:\n",
    "                sns.histplot(other_summary_data[row.feature], ax=ax[row_ix][offset_col_ix], stat='percent').set_title(f\"Cluster {oix}\")\n",
    "            else:\n",
    "                sns.histplot(other_demo[row.feature], ax=ax[row_ix][offset_col_ix], stat='percent').set_title(f\"Cluster {oix}\")\n",
    "            ax[row_ix][offset_col_ix].set_xlabel(ax[row_ix][offset_col_ix].get_xlabel(), fontsize=8)\n",
    "            ax[row_ix][offset_col_ix].set_ylim(0., 100.)\n",
    "            \n",
    "        offset_col_ix += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / f\"{CURRENT_DB}_cluster{cix}_combined_features.png\", dpi=300)\n",
    "    plt.show()\n",
    "    print(50 * '=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a80f68",
   "metadata": {},
   "source": [
    "## Try a different clustering technique? (Unexplored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0288db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# best_score = -np.inf\n",
    "# best_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls = AffinityPropagation(random_state=13210).fit(target_df)\n",
    "# labels = cls.labels_\n",
    "\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562bbb6-66eb-4283-8c08-6e20a0b2ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# center_embeddings = cls.cluster_centers_\n",
    "# centers_proj = PCA(n_components=2).fit_transform(center_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aad38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# sns.scatterplot(x=tsfm[:,0], y=tsfm[:,1], c=cls.labels_, ax=ax)\n",
    "# ax.scatter(x=centers_proj[:,0], y=centers_proj[:,1], marker='X', c='red', alpha=0.5)\n",
    "# ax.set(xlabel='Latent Dim 0', ylabel='Latent Dim 1')\n",
    "# # plt.legend([str(x) for x in ap_labels], loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce0238-b3f2-4f46-a52f-13e3160cc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data2(cix, labels):\n",
    "#     users = target_df.iloc[labels == cix, :].index\n",
    "    \n",
    "#     # Compute trip summaries.\n",
    "#     X = df.loc[df.user_id.isin(users), [\n",
    "#         'section_distance_argmax', 'section_duration_argmax',\n",
    "#         'section_mode_argmax', 'distance',\n",
    "#         'duration', 'mph', 'user_id', 'target'\n",
    "#     ]]\n",
    "    \n",
    "#     # Compute the target distribution and select the argmax.\n",
    "#     target_distribution = X.target.value_counts(ascending=False, normalize=True)\n",
    "#     target_distribution.rename(index=MAP, inplace=True)\n",
    "    \n",
    "#     # Caution - this summary df has NaNs. Use nanstd() to compute nan-aware std.\n",
    "#     subset = get_trip_summary_df(users, X)\n",
    "    \n",
    "#     norm_subset = pd.DataFrame(\n",
    "#         MinMaxScaler().fit_transform(subset),\n",
    "#         columns=subset.columns, index=subset.index\n",
    "#     )\n",
    "    \n",
    "#     return norm_subset, target_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27cf29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## Analaysis for this data.\n",
    "\n",
    "# ic, oc = dict(), dict()\n",
    "# labels = cls.labels_\n",
    "\n",
    "# for cix in np.unique(labels):\n",
    "#     users = target_df[labels == cix].index\n",
    "    \n",
    "#     ic[cix] = dict()\n",
    "    \n",
    "#      # Trip characteristics.\n",
    "#     norm_subset, _ = get_data2(cix, labels)\n",
    "#     for feature in norm_subset.columns:\n",
    "#         ic[cix][feature] = np.nanstd(norm_subset[feature])\n",
    "    \n",
    "#     # Demographics.\n",
    "#     data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "#     processed = preprocess_demo_data(data)\n",
    "    \n",
    "#     for col in processed.columns:\n",
    "#         # Numeric/ordinal values. Use std. to measure homogeneity.\n",
    "#         if col == 'age' or col == 'income_category' or col == 'n_working_residents':\n",
    "#             ic[cix][col] = np.nanstd(processed[col])\n",
    "#         else:\n",
    "#             ic[cix][col] = entropy(processed[col])\n",
    "\n",
    "# for cix in ic.keys():\n",
    "#     oc[cix] = dict()\n",
    "#     oix = set(labels) - set([cix])\n",
    "#     for feature in ic[cix].keys():\n",
    "#         oc[cix][feature] = np.nanmean([ic[x].get(feature, np.nan) for x in oix])\n",
    "\n",
    "# # # Now, compute the per-cluster homogeneity.\n",
    "# # for cix in ic.keys():\n",
    "    \n",
    "# #     users = users = target_df[labels == cix].index\n",
    "# #     norm_subset, target_dist = get_data(cix, labels)\n",
    "# #     data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "# #     processed = preprocess_demo_data(data)\n",
    "    \n",
    "# #     concat = processed.merge(norm_subset, left_index=True, right_index=True)\n",
    "    \n",
    "# #     ch = list()\n",
    "# #     for feature in ic[cix].keys():\n",
    "# #         ratio = ic[cix][feature] / (oc[cix][feature] + 1e-6)\n",
    "# #         ch.append([feature, ratio])\n",
    "    \n",
    "# #     ch_df = pd.DataFrame(ch, columns=['feature', 'ch']).sort_values(by=['ch']).head(TOP_K).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # Now, compute the per-cluster homogeneity.\n",
    "# ax_ix = 0\n",
    "# for cix in ic.keys():\n",
    "\n",
    "#     print(f\"For cluster {cix}:\")\n",
    "\n",
    "#     # For each, cluster, we will have (TOP_K x n_clusters) figures.\n",
    "#     fig, ax = plt.subplots(nrows=5, ncols=len(ic.keys()), figsize=(12, 8))\n",
    "\n",
    "#     other_ix = set(ic.keys()) - set([cix])\n",
    "    \n",
    "#     ch = list()\n",
    "#     for feature in ic[cix].keys():\n",
    "#         ratio = ic[cix][feature] / (oc[cix][feature] + 1e-6)\n",
    "#         ch.append([feature, ratio])\n",
    "    \n",
    "#     # Just the top k.\n",
    "#     ch_df = pd.DataFrame(ch, columns=['feature', 'ch']).sort_values(by=['ch']).reset_index(drop=True).head(5)\n",
    "#     figure_data = dict()\n",
    "    \n",
    "#     # Get the actual trip summary data.\n",
    "#     trip_summary_data, target_dist = get_data(cix)\n",
    "\n",
    "#     display(target_dist)\n",
    "    \n",
    "#     # Get the actual demographic data.\n",
    "#     users = target_df[labels == cix].index\n",
    "#     data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "#     processed = preprocess_demo_data(data)\n",
    "\n",
    "#     # Left-most subplot will be that of the current cluster's feature.\n",
    "#     for row_ix, row in ch_df.iterrows():\n",
    "#         if row.feature in trip_summary_data.columns:\n",
    "#             sns.histplot(trip_summary_data[row.feature], ax=ax[row_ix][0], stat='percent').set_title(\"Current cluster\")\n",
    "#         else:\n",
    "#             sns.histplot(processed[row.feature], ax=ax[row_ix][0], stat='percent').set_title(\"Current cluster\")\n",
    "#         ax[row_ix][0].set_xlabel(ax[row_ix][0].get_xlabel(), fontsize=6)\n",
    "#         ax[row_ix][0].set_ylim(0., 100.)\n",
    "\n",
    "#     offset_col_ix = 1\n",
    "#     ## Now, others.\n",
    "#     for oix in other_ix:\n",
    "#         # Get the actual trip summary data.\n",
    "#         other_summary_data, _ = get_data(oix)\n",
    "        \n",
    "#         # Get the actual demographic data.\n",
    "#         users = target_df[labels == oix].index\n",
    "#         data = demographics.loc[demographics.index.isin(users), :].reset_index(drop=True)\n",
    "#         other_demo = preprocess_demo_data(data)\n",
    "\n",
    "#         for row_ix, row in ch_df.iterrows():\n",
    "#             if row.feature in other_summary_data.columns:\n",
    "#                 sns.histplot(other_summary_data[row.feature], ax=ax[row_ix][offset_col_ix], stat='percent').set_title(f\"Cluster {oix}\")\n",
    "#             else:\n",
    "#                 sns.histplot(other_demo[row.feature], ax=ax[row_ix][offset_col_ix], stat='percent').set_title(f\"Cluster {oix}\")\n",
    "#             ax[row_ix][offset_col_ix].set_xlabel(ax[row_ix][offset_col_ix].get_xlabel(), fontsize=6)\n",
    "#             ax[row_ix][offset_col_ix].set_ylim(0., 100.)\n",
    "            \n",
    "#         offset_col_ix += 1\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     print(50 * '=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b642db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emission",
   "language": "python",
   "name": "emission"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
