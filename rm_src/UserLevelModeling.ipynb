{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ccf092",
   "metadata": {},
   "source": [
    "### Some important points to remember:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2985b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# warnings.simplefilter('ignore', Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21ef0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, f1_score, log_loss\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from enum import Enum\n",
    "from scipy.stats import uniform\n",
    "from typing import List\n",
    "\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fef98692",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 13210\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "SimilarityMetric = Enum('SimilarityMetric', ['COSINE', 'EUCLIDEAN', 'KNN', 'KMEANS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f8c51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_47441/224655024.py:1: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/ReplacedMode_Fix_02142024.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/ReplacedMode_Fix_02142024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72793473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'end_ts', 'end_fmt_time', 'end_loc', 'raw_trip', 'start_ts', 'start_fmt_time', 'start_loc', 'duration', 'distance', 'start_place', 'end_place', 'cleaned_trip', 'inferred_labels', 'inferred_trip', 'expectation', 'confidence_threshold', 'expected_trip', 'user_input', 'start:year', 'start:month', 'start:day', 'start:hour', 'start_local_dt_minute', 'start_local_dt_second', 'start_local_dt_weekday', 'start_local_dt_timezone', 'end:year', 'end:month', 'end:day', 'end:hour', 'end_local_dt_minute', 'end_local_dt_second', 'end_local_dt_weekday', 'end_local_dt_timezone', '_id', 'user_id', 'metadata_write_ts', 'additions', 'mode_confirm', 'purpose_confirm', 'distance_miles', 'Mode_confirm', 'Trip_purpose', 'original_user_id', 'program', 'opcode', 'Timestamp', 'birth_year', 'primary_job_commute_time', 'income_category', 'n_residence_members', 'n_residents_u18', 'n_residents_with_license', 'n_motor_vehicles', 'available_modes', 'age', 'gender_Man', 'gender_Man;Nonbinary/genderqueer/genderfluid', 'gender_Nonbinary/genderqueer/genderfluid', 'gender_Prefer not to say', 'gender_Woman', 'gender_Woman;Nonbinary/genderqueer/genderfluid', 'has_drivers_license_No', 'has_drivers_license_Prefer not to say', 'has_drivers_license_Yes', 'has_multiple_jobs_No', 'has_multiple_jobs_Prefer not to say', 'has_multiple_jobs_Yes', \"highest_education_Bachelor's degree\", 'highest_education_Graduate degree or professional degree', 'highest_education_High school graduate or GED', 'highest_education_Less than a high school graduate', 'highest_education_Prefer not to say', 'highest_education_Some college or associates degree', 'primary_job_type_Full-time', 'primary_job_type_Part-time', 'primary_job_type_Prefer not to say', 'primary_job_description_Clerical or administrative support', 'primary_job_description_Custodial', 'primary_job_description_Education', 'primary_job_description_Food service', 'primary_job_description_Manufacturing, construction, maintenance, or farming', 'primary_job_description_Medical/healthcare', 'primary_job_description_Other', 'primary_job_description_Professional, managerial, or technical', 'primary_job_description_Sales or service', 'primary_job_commute_mode_Active transport', 'primary_job_commute_mode_Car transport', 'primary_job_commute_mode_Hybrid', 'primary_job_commute_mode_Public transport', 'primary_job_commute_mode_Unknown', 'primary_job_commute_mode_WFH', 'is_overnight_trip', 'n_working_residents', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'temperature_2m (°F)', 'relative_humidity_2m (%)', 'dew_point_2m (°F)', 'rain (inch)', 'snowfall (inch)', 'wind_speed_10m (mp/h)', 'wind_gusts_10m (mp/h)', 'section_distance_argmax', 'section_duration_argmax', 'section_mode_argmax', 'section_coordinates_argmax', 'mph', 'target', 'av_car', 'av_transit', 'av_ridehail', 'av_p_micro', 'av_s_micro', 'av_walk', 'av_no_trip', 'av_s_car', 'av_unknown', 'cost_p_micro', 'cost_no_trip', 'cost_s_car', 'cost_transit', 'cost_car', 'cost_s_micro', 'cost_ridehail', 'cost_walk', 'cost_unknown']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85866e8a",
   "metadata": {},
   "source": [
    "### We want to experiment with two types of models:\n",
    "\n",
    "\n",
    "1. have one row per user, so that when predicting modes for a new user, we pick the \"similar user\" or users and determine the replaced mode\n",
    "    - In this, the traditional approach would only use demographics for the user features, we may experiment with some summaries of the trip data that will function as some level of \"fingerprint\" for the user. Ideally we would be able to show that this performs better than demographics alone\n",
    "    - Note also that the original method that you had outlined where the training set is a list of trips (O()) is a third approach which we will be comparing these two against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9efd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_availability(df: pd.DataFrame):\n",
    "    \n",
    "    # Borrowed directly from the cost_time_avl_preprocessing notebook.\n",
    "    available = {\n",
    "        'Bicycle': 'p_micro',\n",
    "        'Do not have vehicle': 'unknown',\n",
    "        'Get a ride from a friend or family member': 's_car',\n",
    "        'None': 'no_trip',\n",
    "        'Public transportation (bus, subway, light rail, etc.)': 'transit',\n",
    "        'Rental car (including Zipcar/ Car2Go)': 'car',\n",
    "        'Shared bicycle or scooter': 's_micro',\n",
    "        'Skateboard': 'p_micro',\n",
    "        'Taxi (regular taxi, Uber, Lyft, etc)': 'ridehail',\n",
    "        'Walk/roll': 'walk',\n",
    "        'Prefer not to say': 'unknown'\n",
    "    }\n",
    "    \n",
    "    newcols = list(available.values())\n",
    "    \n",
    "    # Start by creating dummy indicators.\n",
    "    df[newcols] = 0\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        modes = [e.strip() for e in row.available_modes.split(';')]\n",
    "        mapped_modes = [available[mode] for mode in modes]\n",
    "        df.loc[i, mapped_modes] = 1\n",
    "    \n",
    "    df.drop(columns=['available_modes'], inplace=True)\n",
    "    df.columns = ['av_' + str(c) if c in newcols else str(c) for c in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe76e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode_coverage(df: pd.DataFrame):\n",
    "    \n",
    "    coverage_df = df.groupby(['user_id', 'section_mode_argmax']).size().unstack(fill_value=0)\n",
    "    coverage_df.columns = ['coverage_' + str(c) for c in coverage_df.columns]\n",
    "    \n",
    "    # As a preventative measure.\n",
    "    coverage_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Normalize over rows.\n",
    "    coverage_df.iloc[:, 1:] = coverage_df.iloc[:, 1:].div(coverage_df.iloc[:, 1:].sum(axis=1), axis=0)\n",
    "    \n",
    "    return coverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75313008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trip_summaries(df: pd.DataFrame, group_key: str, feature_list: List[str]):\n",
    "    \n",
    "    def get_feature_summaries(trip_feature: str, is_ordinal: bool = False):\n",
    "        \n",
    "        if not is_ordinal:\n",
    "            # A mean of 0 is an actual value.\n",
    "            mean = df.groupby(['user_id', group_key])[trip_feature].mean().unstack(level=-1, fill_value=-1.)\n",
    "            mean.columns = [f'{trip_feature}_mean_' + str(c) for c in mean.columns]\n",
    "            \n",
    "            # Same with percentiles - 0 is an actual value.\n",
    "            median = df.groupby(['user_id', group_key])[trip_feature].median().unstack(level=-1, fill_value=-1.)\n",
    "            median.columns = [f'{trip_feature}_median_' + str(c) for c in median.columns]\n",
    "            \n",
    "            iqr_temp = df.groupby(['user_id', group_key])[trip_feature].quantile([0.25, 0.75]).unstack(level=-1)\n",
    "            iqr = (iqr_temp[0.75] - iqr_temp[0.25]).unstack(level=-1)\n",
    "            iqr.fillna(-1., inplace=True)\n",
    "            iqr.columns = [f'{trip_feature}_iqr_' + str(c) for c in iqr.columns]\n",
    "\n",
    "            # Now merge.\n",
    "            merged = mean.copy()\n",
    "            merged = merged.merge(right=median, left_index=True, right_index=True)\n",
    "            merged = merged.merge(right=iqr, left_index=True, right_index=True)\n",
    "\n",
    "            return merged\n",
    "        \n",
    "        # 0 is OK to indicate NaN values.\n",
    "        f_mode = df.groupby(['user_id', group_key])[trip_feature].apply(\n",
    "            lambda x: x.value_counts().idxmax()\n",
    "        ).unstack(fill_value=0.)\n",
    "        \n",
    "        f_mode.columns = [f'{trip_feature}_mode_' + str(c) for c in f_mode.columns]\n",
    "        \n",
    "        return f_mode\n",
    "    \n",
    "    # This will be the dataframe that all subsequent features will join to.\n",
    "    feature_df = None\n",
    "    \n",
    "    for ix, feature in enumerate(feature_list):\n",
    "        is_ordinal = feature == 'start:hour' or feature == 'end:hour'\n",
    "        if ix == 0:\n",
    "            feature_df = get_feature_summaries(feature, is_ordinal)\n",
    "        else:\n",
    "            next_feature_df = get_feature_summaries(feature, is_ordinal)\n",
    "            feature_df = feature_df.merge(right=next_feature_df, left_index=True, right_index=True)\n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63617ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demographic_data(df: pd.DataFrame, **trip_kwargs):\n",
    "    \n",
    "    '''\n",
    "    A method that returns a U x (D + t) matrix, where U = number of users,\n",
    "    D = number of demographic features, t (optional) = number of trip summary features.\n",
    "    \n",
    "    When use_trip_summaries=True, the 'available_modes' column is dropped in favor of\n",
    "    the already-preprocessed av_ columns. This is because we want to incorporate trip-level\n",
    "    information into the data. When the argument is False, we want to SOLELY use demographics.\n",
    "    '''\n",
    "    \n",
    "    trip_features_to_use = trip_kwargs.pop('trip_features', None)\n",
    "    trip_group_key = trip_kwargs.pop('trip_group_key', 'section_mode_argmax')\n",
    "    \n",
    "    demographics = [\n",
    "        'user_id', 'primary_job_commute_time', 'income_category', 'n_residence_members', 'n_residents_u18', \n",
    "        'n_residents_with_license', 'n_motor_vehicles', 'available_modes', 'age', 'gender_Man', \n",
    "        'gender_Man;Nonbinary/genderqueer/genderfluid', 'gender_Nonbinary/genderqueer/genderfluid', \n",
    "        'gender_Prefer not to say', 'gender_Woman', 'gender_Woman;Nonbinary/genderqueer/genderfluid', \n",
    "        'has_drivers_license_No', 'has_drivers_license_Prefer not to say', 'has_drivers_license_Yes', \n",
    "        'has_multiple_jobs_No', 'has_multiple_jobs_Prefer not to say', 'has_multiple_jobs_Yes', \n",
    "        \"highest_education_Bachelor's degree\", 'highest_education_Graduate degree or professional degree', \n",
    "        'highest_education_High school graduate or GED', 'highest_education_Less than a high school graduate', \n",
    "        'highest_education_Prefer not to say', 'highest_education_Some college or associates degree', \n",
    "        'primary_job_type_Full-time', 'primary_job_type_Part-time', 'primary_job_type_Prefer not to say', \n",
    "        'primary_job_description_Clerical or administrative support', 'primary_job_description_Custodial', \n",
    "        'primary_job_description_Education', 'primary_job_description_Food service', \n",
    "        'primary_job_description_Manufacturing, construction, maintenance, or farming', \n",
    "        'primary_job_description_Medical/healthcare', 'primary_job_description_Other', \n",
    "        'primary_job_description_Professional, managerial, or technical', \n",
    "        'primary_job_description_Sales or service', 'primary_job_commute_mode_Active transport', \n",
    "        'primary_job_commute_mode_Car transport', 'primary_job_commute_mode_Hybrid', \n",
    "        'primary_job_commute_mode_Public transport', 'primary_job_commute_mode_Unknown', \n",
    "        'primary_job_commute_mode_WFH', 'is_overnight_trip', 'n_working_residents'\n",
    "    ]\n",
    "    \n",
    "    # Retain only the first instance of each user and subset the columns.\n",
    "    filtered = df.groupby('user_id').first().reset_index(drop=False)[demographics]\n",
    "    \n",
    "    # Get the targets.\n",
    "    targets = df.groupby('user_id')['target'].apply(lambda x: x.value_counts().idxmax()).reset_index(drop=False)\n",
    "    \n",
    "    filtered = filtered.merge(right=targets, left_on='user_id', right_on='user_id')\n",
    "    \n",
    "    if not trip_features_to_use or len(trip_features_to_use) == 0:\n",
    "        # Use the available modes as indicators.\n",
    "        return encode_availability(filtered)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # Reaching here means that we need to include trip summaries\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    # If trip summaries are to be used, then re-use the preprocessed availability features.\n",
    "    availability = df[['user_id'] + [c for c in df.columns if 'av_' in c]]\n",
    "    availability = availability.groupby('user_id').first()\n",
    "    \n",
    "    # For every user, generate the global trip-level summaries.\n",
    "    global_aggs = df.groupby('user_id').agg({'duration': 'mean', 'distance': 'mean'})\n",
    "    \n",
    "    # coverage.\n",
    "    coverage = get_mode_coverage(df)\n",
    "    \n",
    "    # Trip-level features.\n",
    "    trip_features = get_trip_summaries(\n",
    "        df=df, \n",
    "        group_key=trip_group_key, \n",
    "        feature_list=trip_features_to_use\n",
    "    )\n",
    "    \n",
    "    targets = df.groupby('user_id')['target'].apply(lambda x: x.value_counts().idxmax())\n",
    "    \n",
    "    trip_features = trip_features.merge(right=coverage, left_index=True, right_index=True)\n",
    "    trip_features = trip_features.merge(right=global_aggs, left_index=True, right_index=True)\n",
    "    \n",
    "    # Finally, join with availability indicators and targets.\n",
    "    trip_features = trip_features.merge(right=availability, left_index=True, right_on='user_id')\n",
    "    trip_features = trip_features.merge(right=targets, left_index=True, right_index=True)\n",
    "    \n",
    "    return trip_features.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb51e8",
   "metadata": {},
   "source": [
    "## Experiment 1: Only demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66421120",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = get_demographic_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c023cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No stratification, pure random.\n",
    "train, test = train_test_split(demo_df, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "376a4391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 33\n"
     ]
    }
   ],
   "source": [
    "print(train.shape[0], test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef77c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_using_similarity(test_df, train_df, metric=SimilarityMetric.COSINE, **metric_kwargs):\n",
    "    \n",
    "    '''\n",
    "    This method treats each user row as a 'fingerprint' (embedding vector). We assume that we\n",
    "    have no idea about the test set labels. To find which replaced mode is most likely for the test\n",
    "    users, we compute the cosine similarity of each test user against the users in the training set.\n",
    "    For the most similar user, we use their target as a proxy for the test user's replaced mode.\n",
    "    This operates on the following intuition: If User A and User B are similar, then their replaced\n",
    "    modes are also similar.\n",
    "    '''\n",
    "    \n",
    "    tr_targets = train_df.target.values\n",
    "    tr = train_df.drop(columns=['target', 'user_id'], inplace=False).reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    te_targets = test_df.target.values\n",
    "    te = test_df.drop(columns=['target', 'user_id'], inplace=False).reset_index(drop=True, inplace=False)\n",
    "    \n",
    "    if metric == SimilarityMetric.COSINE:\n",
    "        # Use cosine similarity to determine which element in the train set this user is closest to.\n",
    "        # Offset the columns from the second entry to exclude the user_id column.\n",
    "        # Returns a (n_te, n_tr) matrix.\n",
    "        sim = cosine_similarity(te.values, tr.values)\n",
    "        \n",
    "        # Compute the argmax across the train set.\n",
    "        argmax = np.argmax(sim, axis=1)\n",
    "\n",
    "        # Index into the training targets to retrieve predicted label.\n",
    "        y_test_pred = tr_targets[argmax]\n",
    "        \n",
    "    elif metric == SimilarityMetric.EUCLIDEAN:\n",
    "        \n",
    "        # Here, we choose the embedding with the smallest L2 distance.\n",
    "        distances = euclidean_distances(te.values, tr.values)\n",
    "        \n",
    "        # We choose argmin\n",
    "        argmin = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Index into the targets.\n",
    "        y_test_pred = tr_targets[argmin]\n",
    "    \n",
    "    elif metric == SimilarityMetric.KNN:\n",
    "        \n",
    "        # Build the KNN classifier. By default, let it be 3.\n",
    "        knn = KNeighborsClassifier(\n",
    "            n_neighbors=metric_kwargs.pop('n_neighbors', 3),\n",
    "            weights='distance',\n",
    "            metric=metric_kwargs.pop('knn_metric', 'cosine'),\n",
    "            n_jobs=os.cpu_count()\n",
    "        )\n",
    "        \n",
    "        # Fit the data to the KNN model\n",
    "        knn.fit(tr, tr_targets)\n",
    "        \n",
    "        y_test_pred = knn.predict(te)\n",
    "    \n",
    "    elif metric == SimilarityMetric.KMEANS:\n",
    "        \n",
    "        # Build the model.\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=metric_kwargs.pop('n_clusters', 8),\n",
    "            max_iter=metric_kwargs.pop('max_iter', 300),\n",
    "            n_init='auto'\n",
    "        )\n",
    "        \n",
    "        # Fit the clustering model\n",
    "        kmeans.fit(tr)\n",
    "        \n",
    "        # Construct the auxiliary df and merge with the training set.\n",
    "        label_df = pd.DataFrame({'label': kmeans.labels_, 'target': tr_targets}, index=tr.index)\n",
    "        \n",
    "        # Now, perform an inference on the test set.\n",
    "        predicted_labels = kmeans.predict(te)\n",
    "        \n",
    "        y_test_pred = []\n",
    "        for prediction in predicted_labels:\n",
    "            most_likely = label_df.loc[label_df.label == prediction, 'target'].value_counts().idxmax()\n",
    "            y_test_pred.append(most_likely)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown similarity metric\")\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(y_true=te_targets, y_pred=y_test_pred, average='weighted')\n",
    "    print(f\"Test F1 score using {metric.name} = {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a95ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using COSINE = 0.32806324110671936\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44cfbe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using EUCLIDEAN = 0.2742577288031834\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.EUCLIDEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe343c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using KNN = 0.35950413223140487\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9681639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using KMEANS = 0.33347902097902093\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.KMEANS, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e435a6",
   "metadata": {},
   "source": [
    "Not bad - using just a simple random split gives us the following results:\n",
    "\n",
    "$F1_{cosine} = 0.32$, $F1_{euclidean} = 0.27$, $F1_{KNN} = 0.36$, $F1_{kmeans} = 0.33$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81f0e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nll_scorer(clf, X, y):\n",
    "    y_pred = clf.predict_proba(X)\n",
    "    return -log_loss(y_true=y, y_pred=y_pred, labels=sorted(np.unique(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3a6af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_using_model(train, test):\n",
    "    \n",
    "    # Define the train-val splitter.\n",
    "    splitter = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': np.arange(100, 1001, 50),\n",
    "        'max_depth': [i for i in range(5, 101, 5)],\n",
    "        'ccp_alpha': np.linspace(0, 1, 10),\n",
    "        'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "        'min_samples_split': np.arange(2, 25, 2),\n",
    "        'min_samples_leaf': np.arange(1, 25)\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=SEED)\n",
    "    \n",
    "    # Search over hparams to minimize negative log likelihood.  \n",
    "    clf = RandomizedSearchCV(\n",
    "        rf, params, n_iter=500, scoring=custom_nll_scorer, \n",
    "        n_jobs=os.cpu_count(), cv=splitter, random_state=SEED,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    X_tr = train.drop(columns=['user_id', 'target'])\n",
    "    y_tr = train.target.values.ravel()\n",
    "    \n",
    "    scorer = clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    best_model = scorer.best_estimator_\n",
    "    \n",
    "    print(f\"Best val score = {scorer.best_score_}\")\n",
    "    \n",
    "    X_te = test.drop(columns=['user_id', 'target'])\n",
    "    \n",
    "    # Use the best model to compute F1 on the test set.\n",
    "    test_f1 = f1_score(y_true=test.target.values, y_pred=best_model.predict(X_te), average='weighted')\n",
    "    \n",
    "    print(f\"Test F1 = {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fab93ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val score = -1.7985827701200345\n",
      "Test F1 = 0.32794612794612793\n"
     ]
    }
   ],
   "source": [
    "estimate_using_model(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988c1b2",
   "metadata": {},
   "source": [
    "Interesting! The model is slightly on par with the cosine similarity, but actually worse than the KNN model!\n",
    "\n",
    "To recap, $F1_{cosine} = 0.32806$, $F1_{euclidean} = 0.27$, $F1_{KNN} = 0.36$, $F1_{kmeans} = 0.33$, $F1_{RF} = 0.328$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b77353",
   "metadata": {},
   "source": [
    "## Experiment 2: Demographics with trip summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7753d4",
   "metadata": {},
   "source": [
    "Now that we've performed experiments with solely demographic data, let's expand the feature set by including \n",
    "trip summary statistics. We would like this approach to do better than the aforementioned baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d46ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_plus_trips = get_demographic_data(\n",
    "    df, \n",
    "    trip_features=['mph', 'section_duration_argmax', 'section_distance_argmax', 'start:hour', 'end:hour']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6159c90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 33\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(demo_plus_trips, test_size=0.2, random_state=SEED)\n",
    "\n",
    "print(train.shape[0], test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06e85bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using COSINE = 0.3686868686868687\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bc67e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using EUCLIDEAN = 0.3338758428272495\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.EUCLIDEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ea68a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using KNN = 0.30201171377641967\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52eb931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score using KMEANS = 0.36007130124777187\n"
     ]
    }
   ],
   "source": [
    "evaluate_using_similarity(test, train, SimilarityMetric.KMEANS, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba795489",
   "metadata": {},
   "source": [
    "Great! Marked improvement from last time. Simply incorporating trip-level features seems to increase the\n",
    "accuracy of finding similar users. Now, $F1_{cosine} = 0.37$, $F1_{euclidean} = 0.33$, $F1_{knn} = 0.3$, $F1_{kmeans} = 0.36$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9acd4b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val score = -1.8201049789158268\n",
      "Test F1 = 0.42158426368952684\n"
     ]
    }
   ],
   "source": [
    "# Now, we try with the model\n",
    "estimate_using_model(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94c548",
   "metadata": {},
   "source": [
    "Great! Compared to the previous model, we see definite improvements! I'm sure we can squeeze some more juice out of the models using fancy optimization, but as a baseline, these are good enough.\n",
    "\n",
    "\n",
    "So, to recap:\n",
    "$F1_{cosine} = 0.37$, $F1_{euclidean} = 0.33$, $F1_{knn} = 0.3$, $F1_{kmeans} = 0.36$, $F1_{RF} = 0.4215$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54d1b2",
   "metadata": {},
   "source": [
    "### Next objectives:\n",
    "\n",
    "1. Try grouping by other features, such as duration or distance\n",
    "2. For similarity search, use other techniques such as clustering or KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339fcc6",
   "metadata": {},
   "source": [
    "# Multi-level modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213676ec",
   "metadata": {},
   "source": [
    "In this approach, we want to piece together the similarity search and modeling processes. Here's a rough sketch of how it should be implemented:\n",
    "\n",
    "1. For every user in the training set, build a model using their entire trip history.\n",
    "2. Consolidate these user-level models in data structure, preferably a dictionary.\n",
    "3. Now, when we want to perform inference on a new user with no prior trips, we use the similarity search to get the user ID in the training set who is the most similar to the user in question.\n",
    "4. We retrieve the model for this corresponding user and perform an inference. The hypothesis is that since the two users are similar, their trip substitution patterns are also similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def construct_model_dictionary(train: pd.DataFrame):\n",
    "    \n",
    "#     def train_on_user(user_id: str):\n",
    "#         '''\n",
    "#         Given the training set and the user ID to query, filter the dataset and\n",
    "#         retain only the relevant trips. Then, create folds and optimize a model for this user.\n",
    "#         Return the trained model instance.\n",
    "#         '''\n",
    "        \n",
    "#         user_data = train.loc[train.user_id == user_id, :].reset_index(drop=True)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emission",
   "language": "python",
   "name": "emission"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
