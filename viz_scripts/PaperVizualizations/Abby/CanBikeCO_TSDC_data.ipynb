{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013ae92f",
   "metadata": {},
   "source": [
    "# TSDC Data Cleaning\n",
    "\n",
    "This notebook is set up to intake the files from the TSDC records and process them according to the data cleaning outlined in our paper\n",
    "\n",
    "Current count is precise num users and up by 3 trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path configuration\n",
    "to_data_parent = \"../Data/abby_ceo\" #path to the parent folder, should contain program subfolders\n",
    "to_mini_data = \"../Data/mini_pilot/data\" #path to the mini data folder, contains an analysis trips file\n",
    "to_data_folder = \"../Data\" #data folder, where composite data files will be written/read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from data_utilities import *\n",
    "\n",
    "# Loading mapping dictionaries from mapping_dictionaries notebook\n",
    "%store -r df_ei\n",
    "%store -r dic_re\n",
    "%store -r dic_pur\n",
    "%store -r dic_fuel\n",
    "\n",
    "# convert a dictionary to a defaultdict\n",
    "dic_re = defaultdict(lambda: 'Other',dic_re)\n",
    "dic_pur = defaultdict(lambda: 'Other',dic_pur)\n",
    "dic_fuel = defaultdict(lambda: 'Other',dic_fuel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9386ed",
   "metadata": {},
   "source": [
    "## Mini Pilot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_confirmed_trips = pd.read_csv(to_mini_data + '/analysis_confirmed_trip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mini_confirmed_trips), \"total minipilot trips\")\n",
    "print(mini_confirmed_trips.perno.nunique(), \"total minipilot users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove trips with no label and count again\n",
    "labeled_mini = mini_confirmed_trips[mini_confirmed_trips.data_user_input_mode_confirm.notna()]\n",
    "labeled_mini = mini_confirmed_trips[mini_confirmed_trips.data_user_input_purpose_confirm.notna()]\n",
    "\n",
    "print(len(labeled_mini), \"labeled minipilot trips\") #only 25 over data used in paper\n",
    "print(labeled_mini.perno.nunique(), \"minipilot users who labeled\")#same as data used in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data = labeled_mini.copy()\n",
    "\n",
    "#first, add the cleaned mode\n",
    "mini_data['Mode_confirm']= mini_data['data_user_input_mode_confirm'].map(dic_re)\n",
    "\n",
    "#second, add the cleaned replaced mode ASSUMES PROGRAM\n",
    "mini_data['Replaced_mode']= mini_data['data_user_input_replaced_mode'].map(dic_re)\n",
    "\n",
    "#third, add the cleaned purpose\n",
    "mini_data['Trip_purpose']= mini_data['data_user_input_purpose_confirm'].map(dic_pur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb50807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine variable categories\n",
    "mini_data = mini_data.replace('Gas Car, drove alone', 'Car')\n",
    "mini_data = mini_data.replace('Gas Car, with others', 'Shared Car')\n",
    "mini_data = mini_data.replace('Bikeshare', 'Shared Micromobility')\n",
    "mini_data = mini_data.replace('Scooter share', 'Shared Micromobility')\n",
    "mini_data = mini_data.replace('Regular Bike', 'Personal Micromobility')\n",
    "mini_data = mini_data.replace('Skate board', 'Personal Micromobility')\n",
    "mini_data = mini_data.replace('Train', 'Transit')\n",
    "mini_data = mini_data.replace('Free Shuttle', 'Transit')\n",
    "mini_data = mini_data.replace('Bus', 'Transit')\n",
    "mini_data = mini_data.replace('Walk', 'Walk')\n",
    "mini_data = mini_data.replace('Taxi/Uber/Lyft', 'Ridehail')\n",
    "mini_data = mini_data.replace('Pilot ebike', 'E-Bike')\n",
    "\n",
    "#filter out 'not a trip' trips\n",
    "mini_data = mini_data[~mini_data['Mode_confirm'].isin(['Not a Trip'])]\n",
    "mini_data = mini_data[~mini_data['Replaced_mode'].isin(['Not a Trip'])]\n",
    "mini_data = mini_data[~mini_data['Trip_purpose'].isin(['not_a_trip'])]\n",
    "\n",
    "print(len(mini_data), \"trips once not a trip is removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.loc[mini_data['Mode_confirm']=='Personal Micromobility', 'Mode_confirm'] = 'Other'\n",
    "mini_data.loc[mini_data['Mode_confirm']=='Shared Micromobility', 'Mode_confirm'] = 'Other'\n",
    "\n",
    "all_trips = mini_data.groupby(['Mode_confirm'], as_index=False).count()[['Mode_confirm','data_distance']]\n",
    "all_trips['proportion'] = all_trips['data_distance'] / np.sum(all_trips.data_distance)\n",
    "all_trips['trip_type'] = 'All Trips'\n",
    "\n",
    "work_trips = mini_data[mini_data['Trip_purpose']=='Work'].copy()\n",
    "work_trips = work_trips.groupby(['Mode_confirm'], as_index=False).count()[['Mode_confirm','data_distance']]\n",
    "work_trips['proportion'] = work_trips['data_distance'] / np.sum(work_trips.data_distance)\n",
    "work_trips['trip_type'] = 'Work Trips'\n",
    "work_trips.loc[1.5] = 'Other', 0, 0, 'Work Trips'\n",
    "work_trips = work_trips.sort_index().reset_index(drop=True)\n",
    "\n",
    "mini_data = pd.concat([all_trips,work_trips])\n",
    "mini_data['Dataset'] = 'Minipilot'\n",
    "mini_data.columns = ['Mode','Count','Proportion','Trip Type', \"Dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fccc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data #trip breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb06aa",
   "metadata": {},
   "source": [
    "### matching minis to survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_trips = pd.read_csv(to_mini_data + '/analysis_confirmed_trip.csv')\n",
    "mini_surveys = pd.read_csv(to_mini_data + '/survey_household.csv')\n",
    "\n",
    "print(len(mini_trips), \"minpilot trips\")\n",
    "print(len(mini_surveys), \"minpilot surveys\") #15 surveys\n",
    "print(mini_trips.perno.nunique(), \"minpilot users\") #13 unique users\n",
    "\n",
    "socio_data = mini_surveys[~mini_surveys.perno.isnull()]\n",
    "print(len(socio_data), \"surveys after dropping null\")\n",
    "\n",
    "# Deal with people who have multiple responses by using most recent\n",
    "socio_data = socio_data.sort_values(by=['perno', 'timestamp'])\n",
    "socio_data.drop_duplicates(subset=['perno'], keep='last', inplace=True)\n",
    "socio_data['user_id_socio'] = socio_data.perno\n",
    "socio_data.user_id_socio = [i.replace('-','') for i in socio_data.user_id_socio] # remove all dashes from strings\n",
    "socio_data = socio_data.drop(labels='perno', axis=1)\n",
    "\n",
    "# Lose some trips due to people with no survey responses\n",
    "mini_trips['user_id_socio'] = mini_trips.perno.astype(str)\n",
    "mini_trips.user_id_socio = [i.replace('-','') for i in mini_trips.user_id_socio] # remove all dashes from strings\n",
    "mini_trips = mini_trips.merge(socio_data, on='user_id_socio')\n",
    "\n",
    "print(mini_trips.user_id_socio.nunique(), \"minipilot users with surveys\")\n",
    "print(len(mini_trips), \"trips after pairing with surveys\")\n",
    "\n",
    "mini_trips.to_csv(to_data_folder + \"/minipilot_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd27a69",
   "metadata": {},
   "source": [
    "## Full Pilot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over\n",
    "folders = ['4c', 'cc', 'fc', 'pc', 'sc', 'vail_22']\n",
    "datasets = []\n",
    "\n",
    "for program in folders:\n",
    "    print('\\nstarting with ', program)\n",
    "    \n",
    "    #create dataset with surveys and trips\n",
    "    trips = pd.read_csv(to_data_parent + '/' + program + '/analysis_confirmed_trip.csv')\n",
    "    print(len(trips), 'trips')\n",
    "    print(trips.perno.nunique(), 'people')\n",
    "\n",
    "    surveys = pd.read_csv(to_data_parent + '/' + program + '/' + program + '_survey_household.csv')\n",
    "    print(len(surveys), 'surveys')\n",
    "\n",
    "    #drop any null ids\n",
    "    socio_data = surveys[~surveys['unique_user_id_autofilled_do_not_edit'].isnull()]\n",
    "    print(len(socio_data), 'surveys after dropping null ids')\n",
    "\n",
    "    #drop duplicates\n",
    "    socio_data = socio_data.sort_values(by=['unique_user_id_autofilled_do_not_edit', 'timestamp'])\n",
    "    socio_data.drop_duplicates(subset=['unique_user_id_autofilled_do_not_edit'], keep='last', inplace=True)\n",
    "    print(len(socio_data),'surveys', socio_data['unique_user_id_autofilled_do_not_edit'].nunique(), 'users after dropping duplicates')\n",
    "\n",
    "    #prepare survey ids for merging\n",
    "    socio_data['user_id_socio'] = socio_data['unique_user_id_autofilled_do_not_edit'].astype(str)\n",
    "    socio_data['user_id_socio'] = socio_data['user_id_socio'].str.strip() #remove leading or trailing whitespace!!\n",
    "    socio_data['user_id_socio'] = socio_data['user_id_socio']\n",
    "    socio_data = socio_data.drop(labels='unique_user_id_autofilled_do_not_edit', axis=1)\n",
    "    \n",
    "    \n",
    "    #prepare trip ids for merging\n",
    "    trips['user_id_socio'] = trips.perno.astype(str)\n",
    "    trips['user_id_socio'] = trips['user_id_socio'].str.strip() #remove leading or trailing whitespace!!\n",
    "    trips.user_id_socio = [i.replace('-','') for i in trips.user_id_socio] # remove all dashes from strings\n",
    "    \n",
    "    #merge the data\n",
    "    data = trips.merge(socio_data, on='user_id_socio')\n",
    "    print(len(data), 'trips after merging')\n",
    "    print(data.user_id_socio.nunique(), 'people after merging')\n",
    "    \n",
    "    data['program'] = program.split('_')[0]\n",
    "    \n",
    "    #add to list of datasets\n",
    "    datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1424cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge them all together\n",
    "full_data = pd.concat(datasets)\n",
    "print(len(full_data), 'trips')\n",
    "print(full_data.perno.nunique(), 'users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out unlabeled trips -- accept partial labels -- needed for proper user count\n",
    "labeled_data = full_data[full_data.data_user_input_mode_confirm.notna() | \n",
    "                         full_data.data_user_input_purpose_confirm.notna() |\n",
    "                         full_data.data_user_input_replaced_mode.notna()]\n",
    "\n",
    "print(len(labeled_data), 'labeled trips')\n",
    "print(labeled_data.user_id_socio.nunique(), 'users who labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2824b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data.rename(columns={'user_id_socio':'user_id',\n",
    "                          'please_identify_which_category_represents_your_total_household_':'HHINC',\n",
    "                          'how_many_motor_vehicles_are_owned_leased_or_available_for_regul':'VEH',\n",
    "                            ' how_many_motor_vehicles_are_owned_leased_or_available_for_regul':'VEH',\n",
    "                             'how_many_motor_vehicles_are_owned_leased_or_available_for_regul ':'VEH',\n",
    "                           'in_which_year_were_you_born?':'AGE',\n",
    "                          'including_yourself_how_many_people_live_in_your_home?':'HHSIZE',\n",
    "                          'how_many_children_under_age_18_live_in_your_home?':'CHILDREN',\n",
    "                          'what_is_your_gender?':'GENDER',\n",
    "                          'if_you_were_unable_to_use_your_household_vehicles_which_of_the_':'available_modes',\n",
    "                          'are_you_a_student?':'STUDENT',\n",
    "                         'data_duration':'duration', \n",
    "                         'data_distance':'distance'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d08302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labeled_data.copy()\n",
    "programs = [\"4c\", \"cc\", \"fc\", \"pc\", \"sc\", \"vail\"]\n",
    "returned_dfs = separate_and_count_programs(data, programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4826b",
   "metadata": {},
   "source": [
    "so far so good, we're looking for at least 122 users and at least 61,496 trips after ALL cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ab2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labeled_data.copy()\n",
    "\n",
    "#first, add the cleaned mode\n",
    "data['Mode_confirm']= data['data_user_input_mode_confirm'].map(dic_re)\n",
    "\n",
    "#second, add the cleaned replaced mode ASSUMES PROGRAM\n",
    "data['Replaced_mode']= data['data_user_input_replaced_mode'].map(dic_re)\n",
    "\n",
    "#third, add the cleaned purpose\n",
    "data['Trip_purpose']= data['data_user_input_purpose_confirm'].map(dic_pur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dc0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data for later\n",
    "data.to_csv(to_data_folder + \"/expanded_ct.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp from known year/month/day aggregated to days\n",
    "data.rename(columns={'data_start_local_dt_year':'year','data_start_local_dt_month':'month','data_start_local_dt_day':'day'}, inplace=True)\n",
    "data['date_time'] = pd.to_datetime(data[['year','month','day']])\n",
    "\n",
    "# Fix age (birth year to age)\n",
    "data['AGE'] = 2022 - data['AGE']\n",
    "\n",
    "# Number of workers (size of HH - kids)\n",
    "data['WORKERS'] = data['HHSIZE'] - data['CHILDREN']\n",
    "\n",
    "# Duration in minutes (hours to minutes)\n",
    "data['duration'] = data['duration'] / 60\n",
    "\n",
    "# duration in miles (meters to miles)\n",
    "data['distance_miles'] = data['distance'] * 0.0006213712\n",
    "\n",
    "# E-bike/not E-Bike variable\n",
    "data['is_ebike'] = \"E-Bike Trips\"\n",
    "data.loc[data['Mode_confirm']!=\"E-bike\", 'is_ebike'] = \"Non E-Bike Trips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data), \"trips\")\n",
    "print(data.user_id.nunique(), \"users\")\n",
    "\n",
    "#loose some users that did not give this information (and their trips)\n",
    "#records that had ’prefer not to say’ as a response for household income, household vehicles, and other available modes\n",
    "data = data[~data['HHINC'].isin(['Prefer not to say'])]\n",
    "data = data[~data['VEH'].isin(['Prefer not to say / Prefiero no decir.'])]\n",
    "data = data[~data['available_modes'].isin(['None', 'Prefer not to say'])]\n",
    "\n",
    "print(len(data), \"trips after dropping non responses\")\n",
    "print(data.user_id.nunique(), \"users after dropping non responses\")\n",
    "\n",
    "data['HHINC_NUM'] = data.HHINC.replace(['Less than $24,999',\n",
    "                                       '$25,000-$49,999',\n",
    "                                       '$50,000-$99,999',\n",
    "                                       '$100,000 -$149,999',\n",
    "                                       '$150,000-$199,999',\n",
    "                                       '$200,000 or more'], [12500,37500,75000,125000,175000,250000])\n",
    "\n",
    "# Calculate average income per adult in the household\n",
    "data['PINC'] = data['HHINC_NUM'] / data['WORKERS']\n",
    "\n",
    "# Combine variable categories\n",
    "data = data.replace('Gas Car, drove alone', 'Car')\n",
    "data = data.replace('Gas Car, with others', 'Shared Car')\n",
    "data = data.replace('Bikeshare', 'Shared Micromobility')\n",
    "data = data.replace('Scooter share', 'Shared Micromobility')\n",
    "data = data.replace('Regular Bike', 'Personal Micromobility')\n",
    "data = data.replace('Skate board', 'Personal Micromobility')\n",
    "data = data.replace('Train', 'Transit')\n",
    "data = data.replace('Free Shuttle', 'Transit')\n",
    "data = data.replace('Bus', 'Transit')\n",
    "data = data.replace('Walk', 'Walk')\n",
    "data = data.replace('Taxi/Uber/Lyft', 'Ridehail')\n",
    "data = data.replace('Pilot ebike', 'E-Bike')\n",
    "\n",
    "# Categorical type will include all days/modes in groupby even if there is no data for a particular tabulation\n",
    "data.user_id = pd.Categorical(data.user_id)\n",
    "data.date_time = pd.Categorical(data.date_time)\n",
    "data.mode_confirm = pd.Categorical(data.data_user_input_mode_confirm, ordered=True, categories=np.unique(list(dic_re.keys())))\n",
    "\n",
    "# Add order to categorical variables\n",
    "data.HHINC = pd.Categorical(data.HHINC, ordered=True)\n",
    "data['Mode'] = pd.Categorical(data.Mode_confirm, ordered=True, categories=[\n",
    "    'E-bike',\n",
    "    'Car',\n",
    "    'Shared Car',\n",
    "    'Walk',\n",
    "    'Transit',\n",
    "    'Personal Micromobility',\n",
    "    'Shared Micromobility',\n",
    "    'Ridehail',\n",
    "    'Other'])\n",
    "data.VEH = data.VEH.astype(str)\n",
    "data.VEH = pd.Categorical(data.VEH, ordered=True, categories=['0','1','2','3','4+'])\n",
    "data['PINC_NUM'] = data['PINC']\n",
    "data.PINC = pd.cut(data.PINC, bins=[0,10000,20000,30000,40000,50000,60000,70000,999999],\n",
    "                  labels=[\"$0-9\",\n",
    "                         \"$10-19\",\n",
    "                         \"$20-29\",\n",
    "                         \"$30-39\",\n",
    "                         \"$40-49\",\n",
    "                         \"$50-59\",\n",
    "                         \"$60-69\",\n",
    "                         \"$70+\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered out ages that were greater than 100\n",
    "data = data[data['AGE'] < 100]\n",
    "\n",
    "#filter out durations longer than 8 hours\n",
    "data = data[data['duration']<480]\n",
    "\n",
    "#distances more than 50 miles \n",
    "data = data[data['distance_miles']<50]\n",
    "\n",
    "#filter household sizes smaller than the number of kids\n",
    "data = data[data['HHSIZE']>data['CHILDREN']]\n",
    "\n",
    "#filter out households greater than 10\n",
    "data = data[data['HHSIZE']<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicles per driver\n",
    "data = data[data['VEH'].notna()] #vails VEH nums were not strings?\n",
    "data['VEH_num'] = data['VEH'].replace(['1','2','3','4+'],[1,2,3,4]).astype(int)\n",
    "data['DRIVERS'] = data[\"including_yourself_how_many_people_have_a_driver's_license_in_y\"]\n",
    "data['DRIVERS_num'] = data['DRIVERS'].replace\n",
    "data['veh_per_driver'] = (data['VEH_num'] / data['DRIVERS']).fillna(0)\n",
    "data.loc[data['veh_per_driver']==np.inf, 'veh_per_driver'] = 0\n",
    "\n",
    "#filter out 'not a trip' trips\n",
    "data = data[~data['Mode_confirm'].isin(['Not a Trip'])]\n",
    "data = data[~data['Replaced_mode'].isin(['Not a Trip'])]\n",
    "data = data[~data['Trip_purpose'].isin(['not_a_trip'])]\n",
    "\n",
    "print(len(data), 'trips after filtering') #around 63,000\n",
    "print(data.user_id.nunique(), 'users after filtering') #132"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3da64",
   "metadata": {},
   "source": [
    "# filtering out trips before first e-bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2762330",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'data_start_ts':'start_ts'}, inplace=True)\n",
    "\n",
    "programs = [\"4c\", \"cc\", \"fc\", \"pc\", \"sc\", \"vail\"]\n",
    "program_dfs = separate_and_count_programs(data, programs)\n",
    "\n",
    "#filtering each of them\n",
    "from datetime import datetime\n",
    "\n",
    "after_ebike_dfs = []\n",
    "for program_df in program_dfs:\n",
    "    print(\"processing\", program_df.program.unique())\n",
    "    after_first_ebike_trips = filter_before_ebike(program_df)\n",
    "    after_ebike_dfs.append(after_first_ebike_trips)\n",
    "    \n",
    "#combining the filtered datasets\n",
    "filtered_merged = pd.concat(after_ebike_dfs, axis=0)\n",
    "\n",
    "#check number of trips and users\n",
    "separate_and_count_programs(filtered_merged, programs)\n",
    "\n",
    "print(len(filtered_merged), \"trips in combined df\") #\n",
    "print(filtered_merged['user_id'].nunique(), \"users in combined df\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics table\n",
    "stat_data = filtered_merged[['distance','duration']]\n",
    "stat_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as a csv, to be used as input to analysis!\n",
    "filtered_merged.to_csv(to_data_folder + \"/tsdc_filtered_merged_trips.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
