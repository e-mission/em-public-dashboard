{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2cdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ebc3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ace37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd481a0e970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global experiment flags and variables.\n",
    "SEED = 19348\n",
    "TARGETS = ['p_micro', 'no_trip', 's_car', 'transit', 'car', 's_micro', 'ridehail', 'walk', 'unknown']\n",
    "\n",
    "# Set the Numpy seed too.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9addd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm = nn.LSTM(12, 64, batch_first=True, bidirectional=True, bias=False, num_layers=3)\n",
    "# inp = torch.FloatTensor(8, 3, 12).fill_(0.)\n",
    "# out, _ = lstm(inp)\n",
    "\n",
    "# print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889bd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLIT_TYPE(Enum):\n",
    "    INTRA_USER = 0\n",
    "    INTER_USER = 1\n",
    "    TARGET = 2\n",
    "    MIXED = 3\n",
    "    \n",
    "\n",
    "class SPLIT(Enum):\n",
    "    TRAIN = 0\n",
    "    TEST = 1\n",
    "\n",
    "\n",
    "def get_train_test_splits(data: pd.DataFrame, how=SPLIT_TYPE, test_ratio=0.2, shuffle=True):\n",
    "\n",
    "    n_users = list(data.user_id.unique())\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    if shuffle:\n",
    "        data = data.sample(data.shape[0], random_state=SEED).reset_index(drop=True, inplace=False)\n",
    "        \n",
    "    # There are certain users with only one observation. What do we do with those?\n",
    "    # As per the mobilitynet modeling pipeline, we randomly assign them to either the\n",
    "    # training or test set.\n",
    "\n",
    "    value_counts = data.user_id.value_counts()\n",
    "    single_count_ids = value_counts[value_counts == 1].index\n",
    "\n",
    "    data_filtered = data.loc[~data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "    data_single_counts = data.loc[data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "\n",
    "    X_tr, X_te = train_test_split(\n",
    "        data_filtered, test_size=test_ratio, shuffle=shuffle, stratify=data_filtered.user_id,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    data_single_counts['assigned'] = np.random.choice(['train', 'test'], len(data_single_counts))\n",
    "    X_tr_merged = pd.concat(\n",
    "        [X_tr, data_single_counts.loc[data_single_counts.assigned == 'train', :].drop(\n",
    "            columns=['assigned'], inplace=False\n",
    "        )],\n",
    "        ignore_index=True, axis=0\n",
    "    )\n",
    "\n",
    "    X_te_merged = pd.concat(\n",
    "        [X_te, data_single_counts.loc[data_single_counts.assigned == 'test', :].drop(\n",
    "            columns=['assigned'], inplace=False\n",
    "        )],\n",
    "        ignore_index=True, axis=0\n",
    "    )\n",
    "\n",
    "    return X_tr_merged, X_te_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfa847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM_weather contains more samples because data is not processed.\n",
    "# _Fix contains lesser samples, but is cleaned using heuristics.\n",
    "\n",
    "data = pd.read_csv('../data/FULL_preprocessed_data_RM_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa73ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline(ABC):\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "    \n",
    "    # Establish hooks.\n",
    "    @abstractmethod\n",
    "    def remove_negative_sections(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_mode_outliers(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def drop_outliers(self, outliers: List):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_mph_feature(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        self.remove_negative_sections()\n",
    "        outlier_ix = self.get_mode_outliers()\n",
    "        self.drop_outliers(list(outlier_ix))\n",
    "        return self.compute_mph_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e129c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionDataPipeline(DataPipeline):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        super().__init__(df)\n",
    "    \n",
    "    def remove_negative_sections(self):\n",
    "        f_rows = list()\n",
    "        for _, row in self.df[['section_durations', 'section_distances', 'section_modes']].iterrows():\n",
    "\n",
    "            dist = np.array(ast.literal_eval(row['section_distances']))\n",
    "            dur = np.array(ast.literal_eval(row['section_durations']))\n",
    "            modes = np.array(ast.literal_eval(row['section_modes']))\n",
    "\n",
    "            assert len(dist) == len(dur) == len(modes)\n",
    "\n",
    "            mask = np.logical_and(dist > 0, dur > 0)\n",
    "\n",
    "            f_dist, f_dur, f_modes = dist[mask], dur[mask], modes[mask]\n",
    "\n",
    "            assert len(f_dist) == len(f_dur) == len(f_modes)\n",
    "\n",
    "            f_rows.append({\n",
    "                # scale to miles.\n",
    "                'distances': f_dist * 0.00062,\n",
    "                # Scale to hours.\n",
    "                'durations': f_dur / 3600.,\n",
    "                'modes': f_modes\n",
    "            })\n",
    "\n",
    "        parsed = pd.DataFrame(f_rows, index=data.index)\n",
    "\n",
    "        self.df.drop(columns=['section_durations', 'section_distances', 'section_modes'], inplace=True)\n",
    "        self.df = pd.concat([self.df, parsed], axis=1)\n",
    "    \n",
    "    def get_mode_outliers(self):\n",
    "        \n",
    "        def compute_outliers(mode: str):\n",
    "            x = self.df[self.df.modes.str.contains(mode, regex=False)]\n",
    "            outlier_ix = []\n",
    "            dist, dur = np.array([]), np.array([])\n",
    "\n",
    "            # First, iterate to compute the 99th percentile for the mode.\n",
    "            for row_ix, row in x.iterrows():\n",
    "                ix = np.where(row['modes'] == mode)[0]\n",
    "                dist = np.append(dist, row['distances'][ix])\n",
    "                dur = np.append(dur, row['durations'][ix])\n",
    "\n",
    "            dist_99p = np.percentile(dist, 99)\n",
    "            dur_99p = np.percentile(dur, 99)\n",
    "\n",
    "            print(f\"99th Percentile for {mode}:\\n\\tdistance = {dist_99p}\\n\\tduration = {dur_99p}\")\n",
    "\n",
    "            # Iterate again.\n",
    "            for row_ix, row in x.iterrows():\n",
    "                ix = np.where(row['modes'] == mode)[0]\n",
    "                row_distances = row['distances'][ix]\n",
    "                row_durations = row['durations'][ix]\n",
    "\n",
    "                if np.any(np.logical_or(row_distances > dist_99p, row_durations > dur_99p)):\n",
    "                    outlier_ix.append(row_ix)\n",
    "\n",
    "            print(\"Number of candidate rows for deletion: \", len(outlier_ix))\n",
    "\n",
    "            return outlier_ix\n",
    "        \n",
    "        all_bad_ix = set()\n",
    "        for mode in ['walking', 'bicycling', 'train', 'bus', 'car']:\n",
    "            bad_mode_ix = compute_outliers(mode)\n",
    "            all_bad_ix = all_bad_ix.union(set(bad_mode_ix))\n",
    "            print(50*'=')\n",
    "        \n",
    "        print(f\"After analysis, {len(all_bad_ix)} total unique rows will be dropped.\")\n",
    "        \n",
    "        return all_bad_ix\n",
    "    \n",
    "    def drop_outliers(self, outliers: List):\n",
    "        self.df.drop(index=outliers, inplace=True)\n",
    "    \n",
    "    def compute_mph_feature(self):\n",
    "        self.df['mph'] = self.df['distances']/self.df['durations']\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38af3aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99th Percentile for walking:\n",
      "\tdistance = 4.305297486707881\n",
      "\tduration = 2.9548623706234824\n",
      "Number of candidate rows for deletion:  765\n",
      "==================================================\n",
      "99th Percentile for bicycling:\n",
      "\tdistance = 8.82048257219703\n",
      "\tduration = 0.9153121545202207\n",
      "Number of candidate rows for deletion:  228\n",
      "==================================================\n",
      "99th Percentile for train:\n",
      "\tdistance = 16.629460194644828\n",
      "\tduration = 0.7250753228180938\n",
      "Number of candidate rows for deletion:  2\n",
      "==================================================\n",
      "99th Percentile for bus:\n",
      "\tdistance = 7.551944729816364\n",
      "\tduration = 0.7316025587360053\n",
      "Number of candidate rows for deletion:  12\n",
      "==================================================\n",
      "99th Percentile for car:\n",
      "\tdistance = 40.548478579524726\n",
      "\tduration = 1.2145738347887987\n",
      "Number of candidate rows for deletion:  721\n",
      "==================================================\n",
      "After analysis, 1718 total unique rows will be dropped.\n"
     ]
    }
   ],
   "source": [
    "pipeline = SectionDataPipeline(data)\n",
    "cleaned = pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81151c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame):\n",
    "    \n",
    "    # Offset by -1 for torch.\n",
    "    df.chosen -= 1\n",
    "    \n",
    "#     df['start_fmt_time'] = pd.to_datetime(df['start_fmt_time'], utc=True)\n",
    "#     df['start_fmt_time'] = df['start_fmt_time'].dt.tz_convert('America/Denver')\n",
    "    \n",
    "    df.rename(\n",
    "        columns={'start_local_dt_weekday': 'start:DOW', 'end_local_dt_weekday': 'end:DOW'},\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    df.n_working_residents = df.n_working_residents.apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "    # Fix some age preprocessing issues.\n",
    "    df.age = df.age.apply(lambda x: x if x < 100 else 2024-x)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d4d6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_estimate(df: pd.DataFrame, dset: SPLIT, model_dict: dict):\n",
    "    \n",
    "    X_features = ['section_distance_argmax', 'age']\n",
    "    \n",
    "    if 'mph' in df.columns:\n",
    "        X_features += ['mph']\n",
    "    \n",
    "    if dset == SPLIT.TRAIN and model_dict is None:\n",
    "        model_dict = dict()\n",
    "    \n",
    "    if dset == SPLIT.TEST and model_dict is None:\n",
    "        raise AttributeError(\"Expected model dict for testing.\")\n",
    "    \n",
    "    if dset == SPLIT.TRAIN:\n",
    "        for section_mode in df.section_mode_argmax.unique():\n",
    "            section_data = df.loc[df.section_mode_argmax == section_mode, :]\n",
    "            if section_mode not in model_dict:\n",
    "                model_dict[section_mode] = dict()\n",
    "\n",
    "                model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "                X = section_data[\n",
    "                    X_features\n",
    "                ]\n",
    "                Y = section_data[['section_duration_argmax']]\n",
    "\n",
    "                model.fit(X, Y.values.ravel())\n",
    "\n",
    "                r2 = r2_score(y_pred=model.predict(X), y_true=Y.values.ravel())\n",
    "                print(f\"Train R2 for {section_mode}: {r2}\")\n",
    "\n",
    "                model_dict[section_mode]['model'] = model\n",
    "                \n",
    "    elif dset == SPLIT.TEST:\n",
    "        for section_mode in df.section_mode_argmax.unique():\n",
    "            section_data = df.loc[df.section_mode_argmax == section_mode, :]\n",
    "            X = section_data[\n",
    "                X_features\n",
    "            ]\n",
    "            \n",
    "            Y = section_data[['section_duration_argmax']]\n",
    "            \n",
    "            y_pred = model_dict[section_mode]['model'].predict(X)\n",
    "            r2 = r2_score(y_pred=y_pred, y_true=Y.values.ravel())\n",
    "            print(f\"Test R2 for {section_mode}: {r2}\")\n",
    "    \n",
    "    # Create the new columns for the duration.\n",
    "    df[TARGETS] = 0\n",
    "    df['temp'] = 0\n",
    "    \n",
    "    for section in df.section_mode_argmax.unique():\n",
    "        X_section = df.loc[df.section_mode_argmax == section, X_features]\n",
    "        \n",
    "        # broadcast to all columns.\n",
    "        df.loc[df.section_mode_argmax == section, 'temp'] = model_dict[section]['model'].predict(X_section)\n",
    "    \n",
    "    for c in TARGETS:\n",
    "        df[c] = df['av_' + c] * df['temp']\n",
    "    \n",
    "    df.drop(columns=['temp'], inplace=True)\n",
    "    \n",
    "    df.rename(columns=dict([(x, 'tt_'+x) for x in TARGETS]), inplace=True)\n",
    "    \n",
    "    # return model_dict, result_df\n",
    "    return model_dict, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b34ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df: pd.DataFrame):\n",
    "        u_time_features = [\n",
    "            'start:year', 'start:month', 'start:day',\n",
    "            'start:hour', 'end_fmt_time', 'end:year',\n",
    "            'end:month', 'end:day', 'end:hour', 'end:n_days_in_month', \n",
    "            'start:sin_DOM', 'start:sin_MOY', 'start:cos_MOY', 'start:cos_DOM',\n",
    "            'end:sin_DOM', 'end:sin_MOY', 'end:cos_DOM', 'end:cos_MOY', 'start:n_days_in_month',\n",
    "            'start_local_dt_weekday', 'end_local_dt_weekday', 'start_fmt_time'\n",
    "        ]\n",
    "\n",
    "        u_user_features = [\n",
    "            '_id', 'original_user_id', 'gender', 'birth_year', 'user_id'\n",
    "        ]\n",
    "\n",
    "        u_trip_features = [\n",
    "            'cleaned_trip', 'Mode_confirm', 'available_modes', 'duration', 'start_loc',\n",
    "            'end_loc', 'section_locations_argmax', 'section_coordinates_argmax',\n",
    "            'start_lat', 'start_lng', 'end_lat', 'end_lng', 'section_duration_argmax',\n",
    "            'section_distance_argmax', 'section_mode_argmax'\n",
    "        ]\n",
    "\n",
    "        # Drop section_mode_argmax and available_modes.\n",
    "        return df.drop(\n",
    "            columns=u_time_features + u_user_features + u_trip_features, \n",
    "            inplace=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904fa4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_train_test_splits(data=cleaned, how=SPLIT_TYPE.INTRA_USER, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da30bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = preprocess(drop_columns(train_df)), preprocess(drop_columns(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78683828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df.columns == test_df.columns).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb81799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_data(\n",
    "    df: pd.DataFrame, split: SPLIT, section_scalers: dict = None, scaler: StandardScaler = None\n",
    "):\n",
    "    # We start by normalizing the temporal features first. This has to be done carefully.\n",
    "    \n",
    "    if split == SPLIT.TRAIN:\n",
    "    \n",
    "        mode_dict = dict()\n",
    "\n",
    "        for _, row in df[['modes', 'durations', 'distances', 'mph']].iterrows():\n",
    "\n",
    "            for (mode, duration, distance, mph) in zip(\n",
    "                row['modes'], row['durations'], row['distances'], row['mph']\n",
    "            ):\n",
    "                if mode not in mode_dict:\n",
    "                    mode_dict[mode] = {\n",
    "                        'duration': np.array([duration]), \n",
    "                        'distance': np.array([distance]),\n",
    "                        'mph': np.array([mph])\n",
    "                    }\n",
    "                else:\n",
    "                    mode_dict[mode] = {\n",
    "                        'duration': np.append(mode_dict[mode]['duration'], duration),\n",
    "                        'distance': np.append(mode_dict[mode]['distance'], distance),\n",
    "                        'mph': np.append(mode_dict[mode]['mph'], mph)\n",
    "                    }\n",
    "\n",
    "        section_scalers = dict()\n",
    "\n",
    "        for mode in mode_dict.keys():\n",
    "            # Fit, but don't transform. Instead, get the mean and scale.\n",
    "            section_scalers[mode] = {'duration': dict(), 'distance': dict(), 'mph': dict()}\n",
    "            \n",
    "            section_scalers[mode]['duration']['mean'] = mode_dict[mode]['duration'].mean()\n",
    "            section_scalers[mode]['duration']['std'] = mode_dict[mode]['duration'].std()\n",
    "            \n",
    "            section_scalers[mode]['distance']['mean'] = mode_dict[mode]['distance'].mean()\n",
    "            section_scalers[mode]['distance']['std'] = mode_dict[mode]['distance'].std()\n",
    "            \n",
    "            section_scalers[mode]['mph']['mean'] = mode_dict[mode]['mph'].mean()\n",
    "            section_scalers[mode]['mph']['std'] = mode_dict[mode]['mph'].std()\n",
    "    \n",
    "    # Normalize the temporal features.\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "        for ix, mode in enumerate(row_dict['modes']):\n",
    "            row_dict['durations'][ix] = (\n",
    "                row_dict['durations'][ix] - section_scalers[mode]['duration']['mean']\n",
    "            )/section_scalers[mode]['duration']['std']\n",
    "            \n",
    "            row_dict['distances'][ix] = (\n",
    "                row_dict['distances'][ix] - section_scalers[mode]['distance']['mean']\n",
    "            )/section_scalers[mode]['distance']['std']\n",
    "            \n",
    "            row_dict['mph'][ix] = (\n",
    "                row_dict['mph'][ix] - section_scalers[mode]['mph']['mean']\n",
    "            )/section_scalers[mode]['mph']['std']\n",
    "        \n",
    "        rows.append(row_dict)\n",
    "        \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # -----------------------------------------\n",
    "    # Now, we start with the regular features.\n",
    "    # -----------------------------------------\n",
    "    \n",
    "    required_features = [\n",
    "        'income_category', 'n_motor_vehicles', 'n_residence_members', 'n_residents_u18',\n",
    "        'n_residents_with_license', 'distance_miles', 'age', 'n_working_residents', \n",
    "        'start:sin_HOD', 'start:cos_HOD', 'end:sin_HOD', 'end:cos_HOD', \n",
    "        'temperature_2m (°F)', 'relative_humidity_2m (%)', 'dew_point_2m (°F)', \n",
    "        'rain (inch)', 'snowfall (inch)', 'cloud_cover (%)', 'wind_speed_10m (mp/h)',\n",
    "        'wind_gusts_10m (mp/h)', 'cost_p_micro', 'cost_no_trip', 'cost_s_car', \n",
    "        'cost_transit', 'cost_car', 'cost_s_micro', 'cost_ridehail', 'cost_walk', 'cost_unknown',\n",
    "       ]\n",
    "    \n",
    "    if split == SPLIT.TRAIN:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "        t_df = pd.DataFrame(\n",
    "            scaler.fit_transform(df[required_features]),\n",
    "            columns=required_features,\n",
    "            index=df.index\n",
    "        )\n",
    "    else:\n",
    "        t_df = pd.DataFrame(\n",
    "            scaler.transform(df[required_features]),\n",
    "            columns=required_features,\n",
    "            index=df.index\n",
    "        )\n",
    "    \n",
    "    df.drop(columns=required_features, inplace=True)\n",
    "    \n",
    "    return pd.concat([df, t_df], axis=1), section_scalers, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2ab162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, section_scaler, scaler = normalize_data(train_df, SPLIT.TRAIN)\n",
    "test_df, _, _ = normalize_data(test_df, SPLIT.TEST, section_scaler, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df774dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ce6bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features for training:\n",
      "['is_student', 'is_overnight_trip', 'is_male', 'mph', 'chosen', 'av_no_trip', 'av_s_car', 'av_p_micro', 'av_walk', 'av_s_micro', 'av_transit', 'av_ridehail', 'av_car', 'av_unknown', 'distances', 'durations', 'modes', 'income_category', 'n_motor_vehicles', 'n_residence_members', 'n_residents_u18', 'n_residents_with_license', 'distance_miles', 'age', 'n_working_residents', 'start:sin_HOD', 'start:cos_HOD', 'end:sin_HOD', 'end:cos_HOD', 'temperature_2m (°F)', 'relative_humidity_2m (%)', 'dew_point_2m (°F)', 'rain (inch)', 'snowfall (inch)', 'cloud_cover (%)', 'wind_speed_10m (mp/h)', 'wind_gusts_10m (mp/h)', 'cost_p_micro', 'cost_no_trip', 'cost_s_car', 'cost_transit', 'cost_car', 'cost_s_micro', 'cost_ridehail', 'cost_walk', 'cost_unknown']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_student</th>\n",
       "      <th>is_overnight_trip</th>\n",
       "      <th>is_male</th>\n",
       "      <th>mph</th>\n",
       "      <th>chosen</th>\n",
       "      <th>av_no_trip</th>\n",
       "      <th>av_s_car</th>\n",
       "      <th>av_p_micro</th>\n",
       "      <th>av_walk</th>\n",
       "      <th>av_s_micro</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_gusts_10m (mp/h)</th>\n",
       "      <th>cost_p_micro</th>\n",
       "      <th>cost_no_trip</th>\n",
       "      <th>cost_s_car</th>\n",
       "      <th>cost_transit</th>\n",
       "      <th>cost_car</th>\n",
       "      <th>cost_s_micro</th>\n",
       "      <th>cost_ridehail</th>\n",
       "      <th>cost_walk</th>\n",
       "      <th>cost_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.6226942218711282]</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.118155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.614415</td>\n",
       "      <td>-0.286713</td>\n",
       "      <td>-0.550431</td>\n",
       "      <td>-0.431234</td>\n",
       "      <td>-0.592465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.32572174462565845]</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.598757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>-0.529027</td>\n",
       "      <td>0.167956</td>\n",
       "      <td>-0.431234</td>\n",
       "      <td>0.128516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.2722796444295434]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.409306</td>\n",
       "      <td>-0.304397</td>\n",
       "      <td>-0.550431</td>\n",
       "      <td>-0.431234</td>\n",
       "      <td>-0.386296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1.785796573896156]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.305702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.614415</td>\n",
       "      <td>-0.529027</td>\n",
       "      <td>-0.550431</td>\n",
       "      <td>-0.431234</td>\n",
       "      <td>-0.592465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.12709085963426053, -0.8967034453081592]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.177887</td>\n",
       "      <td>-0.529027</td>\n",
       "      <td>-0.113227</td>\n",
       "      <td>-0.431234</td>\n",
       "      <td>-0.153682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_student  is_overnight_trip  is_male  \\\n",
       "0           0                0.0        1   \n",
       "1           0                0.0        1   \n",
       "2           0                0.0        1   \n",
       "3           0                0.0        1   \n",
       "4           1                0.0        0   \n",
       "\n",
       "                                           mph  chosen  av_no_trip  av_s_car  \\\n",
       "0                        [-0.6226942218711282]       8           0         0   \n",
       "1                       [-0.32572174462565845]       8           0         1   \n",
       "2                         [1.2722796444295434]       1           0         1   \n",
       "3                          [1.785796573896156]       2           0         0   \n",
       "4  [-0.12709085963426053, -0.8967034453081592]       1           0         1   \n",
       "\n",
       "   av_p_micro  av_walk  av_s_micro  ...  wind_gusts_10m (mp/h)  cost_p_micro  \\\n",
       "0           0        1           0  ...              -1.118155           0.0   \n",
       "1           1        0           0  ...              -0.598757           0.0   \n",
       "2           1        1           0  ...               0.046555           0.0   \n",
       "3           1        0           0  ...               1.305702           0.0   \n",
       "4           1        1           0  ...              -0.457104           0.0   \n",
       "\n",
       "   cost_no_trip  cost_s_car cost_transit  cost_car cost_s_micro  \\\n",
       "0           0.0   -0.614415    -0.286713 -0.550431    -0.431234   \n",
       "1           0.0    0.102862    -0.529027  0.167956    -0.431234   \n",
       "2           0.0   -0.409306    -0.304397 -0.550431    -0.431234   \n",
       "3           0.0   -0.614415    -0.529027 -0.550431    -0.431234   \n",
       "4           0.0   -0.177887    -0.529027 -0.113227    -0.431234   \n",
       "\n",
       "   cost_ridehail  cost_walk  cost_unknown  \n",
       "0      -0.592465        0.0           0.0  \n",
       "1       0.128516        0.0           0.0  \n",
       "2      -0.386296        0.0           0.0  \n",
       "3      -0.592465        0.0           0.0  \n",
       "4      -0.153682        0.0           0.0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Final features for training:\\n{list(train_df.columns)}\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97a8d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.seq_df = df[['modes', 'durations', 'distances', 'mph']]\n",
    "        self.chosen = df['chosen']\n",
    "        self.features = df.drop(columns=[\n",
    "            'modes', 'durations', 'distances', 'mph', 'chosen', 'distance_miles'\n",
    "        ])\n",
    "        \n",
    "        # Start from 1 so that 0 becomes padding.\n",
    "        self.modes_dict = {\n",
    "            m: ix+1 for (ix, m) in enumerate(['walking', 'car', 'train', 'bus', 'bicycling', 'no_sensed'])\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        \n",
    "        t_modes = list()\n",
    "        t_metrics = list()\n",
    "        \n",
    "        chosen = self.chosen[ix]\n",
    "        seq = self.seq_df.loc[ix, :]\n",
    "        \n",
    "        # OHE the target.\n",
    "        y = F.one_hot(torch.tensor(chosen), num_classes=9)\n",
    "        \n",
    "        # Parse the sequence.\n",
    "        for seq_ix in range(len(seq['modes'])):\n",
    "            mode_ix = self.modes_dict[seq['modes'][seq_ix]]\n",
    "            \n",
    "            t_modes.append(torch.tensor(mode_ix))\n",
    "            \n",
    "            # distances, durations.\n",
    "            seq_vector = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([seq['durations'][seq_ix]]), \n",
    "                    torch.tensor([seq['distances'][seq_ix]]), \n",
    "                    torch.tensor([seq['mph'][seq_ix]])\n",
    "                ], dim=-1\n",
    "            )\n",
    "            \n",
    "            t_metrics.append(seq_vector)\n",
    "        \n",
    "        # Flat feature vector.\n",
    "        features = torch.tensor(self.features.loc[ix, :].values)\n",
    "        \n",
    "        return [torch.stack(t_modes).long(), torch.stack(t_metrics), features], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b78758",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = CustomDataset(train_df)\n",
    "test_dset = CustomDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "627b6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    X, y = zip(*batch)\n",
    "    \n",
    "    seq_modes = [x[0] for x in X]\n",
    "    seq_metrics = [x[1] for x in X]\n",
    "    features = [x[-1] for x in X]\n",
    "\n",
    "    padded_seq = pad_sequence([s for s in seq_modes], batch_first=True)\n",
    "    padded_metrics = pad_sequence([m for m in seq_metrics], batch_first=True)\n",
    "    lengths = [len(seq) for seq in seq_modes]\n",
    "    stacked_features = torch.stack(features)\n",
    "\n",
    "    return (padded_seq, padded_metrics, stacked_features), torch.stack(y), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ca34681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dset, batch_size=16, collate_fn=collate, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dset, batch_size=8, collate_fn=collate, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ca5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(modes, metrics, features), sY1, lX = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9eb5a93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 6, 3]), torch.Size([16, 6]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.size(), modes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0abf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 0 for no dropout.\n",
    "DROPOUT = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48871ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def new_gelu(x):\n",
    "    \"\"\"\n",
    "    Taken from OpenAI GPT-2 implementation.\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "\n",
    "class DilatedBlock(nn.Module):\n",
    "    def __init__(self, n_c):\n",
    "        super(DilatedBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(n_c, 4*n_c, bias=False),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(4*n_c, n_c, bias=False),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_features, head_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # in: (B, F, 64)\n",
    "        self.k = nn.Linear(n_features, head_size, bias=False)\n",
    "        self.q = nn.Linear(n_features, head_size, bias=False)\n",
    "        self.v = nn.Linear(n_features, head_size, bias=False)\n",
    "        self.dpt = nn.Dropout(DROPOUT)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(head_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "        v = self.v(x)\n",
    "        \n",
    "        # Q.K.t\n",
    "        dot = torch.bmm(q, k.permute(0, 2, 1))\n",
    "        \n",
    "        # normalize dot product.\n",
    "        dot /= self.sqrt_d\n",
    "        \n",
    "        # softmax over -1 dim.\n",
    "        softmax = self.dpt(torch.softmax(dot, dim=-1))\n",
    "        \n",
    "        # dot with values. (B, F, F) * (B, F, x) = (B, F, x)\n",
    "        return torch.bmm(softmax, v)\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # 64 dims, 4 heads => 16 dims per head.\n",
    "        head_size = n_dim//n_heads\n",
    "        self.heads = nn.ModuleList([SelfAttention(n_dim, head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_dim, n_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is (B, seq, n_dim)\n",
    "        cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.proj(cat)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_c):\n",
    "        super(Block, self).__init__()\n",
    "        \n",
    "        self.sa = MultiHeadAttention(n_heads=4, n_dim=n_c)\n",
    "        self.dilated = DilatedBlock(n_c)\n",
    "        self.ln1 = nn.LayerNorm(n_c)\n",
    "        self.ln2 = nn.LayerNorm(n_c)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.dilated(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, \n",
    "        output_size: int, n_lstm_layers: int = 1\n",
    "    ):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(7, 4, padding_idx=0)\n",
    "        self.dpt = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size + 4,\n",
    "            hidden_size=hidden_size,\n",
    "            bias=False,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            num_layers=n_lstm_layers\n",
    "        )\n",
    "    \n",
    "    def forward(self, modes, x, lengths):\n",
    "        mode_emb = self.embedding(modes)\n",
    "        x = torch.cat([x, mode_emb], dim=-1)\n",
    "        \n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.lstm(packed)\n",
    "        unpacked, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        return self.dpt(unpacked)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size: int, hidden_size: int, output_size: int, \n",
    "        n_features: int, n_lstm_layers: int = 1, **kwargs\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        block1_ip_dim = hidden_size*2\n",
    "        block2_ip_dim = (hidden_size*2) + n_features\n",
    "        \n",
    "        self.lstm = LSTMLayer(\n",
    "            input_size, hidden_size, \n",
    "            output_size, n_lstm_layers\n",
    "        )\n",
    "        \n",
    "        self.block_l1 = nn.ModuleList([Block(block1_ip_dim) for _ in range(kwargs['l1_blocks'])])\n",
    "        self.block_l2 = nn.ModuleList([Block(block2_ip_dim) for _ in range(kwargs['l2_blocks'])])\n",
    "        self.final_proj = nn.Linear(block2_ip_dim, output_size, bias=True)\n",
    "    \n",
    "    def forward(self, modes, x, features, lengths):\n",
    "        \n",
    "        b = x.size(0)\n",
    "        \n",
    "        # Out = (B, seq, hidden*2)\n",
    "        lstm_out = self.lstm(modes, x, lengths)\n",
    "        \n",
    "        # Pass the raw output through the blocks.\n",
    "        for module in self.block_l1:\n",
    "            lstm_out = module(lstm_out)\n",
    "        \n",
    "        features_rshp = features.unsqueeze(1).expand(b, lstm_out.size(1), -1)\n",
    "        \n",
    "        # Out = (B, seq, n+40)\n",
    "        cat = torch.cat([lstm_out, features_rshp], dim=-1)\n",
    "        \n",
    "        for module in self.block_l2:\n",
    "            cat = module(cat)\n",
    "        \n",
    "        # (8, 3, 104) -> (B, 104)\n",
    "        # flattened = cat.view(b, -1)\n",
    "        \n",
    "        # proj = self.runtime_ffw(flattened.size(-1), 64)(flattened)\n",
    "        proj = cat.mean(dim=1)\n",
    "        \n",
    "        return self.final_proj(proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70b4d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "        if module.padding_idx is not None:\n",
    "            module.weight.data[module.padding_idx].zero_()\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "    elif isinstance(module, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        init.xavier_normal_(module.weight.data)\n",
    "        if module.bias is not None:\n",
    "            init.normal_(module.bias.data)\n",
    "    elif isinstance(module, nn.LSTM):\n",
    "        for param in module.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "282ecd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm): LSTMLayer(\n",
      "    (embedding): Embedding(7, 4, padding_idx=0)\n",
      "    (dpt): Dropout(p=0.0, inplace=False)\n",
      "    (lstm): LSTM(7, 16, bias=False, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (block_l1): ModuleList(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (q): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (v): Linear(in_features=32, out_features=8, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=32, out_features=32, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=128, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=128, out_features=32, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (block_l2): ModuleList(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=72, out_features=72, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=72, out_features=288, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=288, out_features=72, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=72, out_features=72, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=72, out_features=288, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=288, out_features=72, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=72, out_features=72, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=72, out_features=288, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=288, out_features=72, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=72, out_features=72, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=72, out_features=288, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=288, out_features=72, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=72, out_features=72, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=72, out_features=288, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=288, out_features=72, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (k): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (q): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (v): Linear(in_features=72, out_features=18, bias=False)\n",
      "            (dpt): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=72, out_features=72, bias=False)\n",
      "      )\n",
      "      (dilated): DilatedBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Linear(in_features=72, out_features=288, bias=False)\n",
      "          (1): ELU(alpha=1.0)\n",
      "          (2): Linear(in_features=288, out_features=72, bias=False)\n",
      "          (3): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_proj): Linear(in_features=72, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    n_lstm_layers=1,\n",
    "    input_size=3,\n",
    "    hidden_size=16, \n",
    "    output_size=9,\n",
    "    n_features=40,\n",
    "    l1_blocks=6,\n",
    "    l2_blocks=6\n",
    ")\n",
    "\n",
    "model = model.apply(init_weights)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20fec22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453101\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ca4b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.31637209  0.45785206  0.97595891  2.59617829  0.35062587 72.2209596\n",
      "  4.71122642  1.08826104  1.53996715]\n"
     ]
    }
   ],
   "source": [
    "weights = train_df.shape[0]/(np.bincount(train_df.chosen.values) * len(np.unique(train_df.chosen)))\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7a2017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 7e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=INIT_LR)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6f1f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sX, sY, sL = next(iter(train_loader))\n",
    "\n",
    "# print(sX.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e53e4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loader, opt, criterion):\n",
    "    \n",
    "    print(\"\\tBeginning training.\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print_every = len(loader)//5\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for ix, (X, y, lengths) in enumerate(loader):\n",
    "        \n",
    "        modes, metrics, features = X\n",
    "        y = y.float()\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
    "\n",
    "        loss = criterion(y_pred.view(-1, 9), y.view(-1, 9))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if ix and ix % print_every == 0:\n",
    "            print(f\"\\t-> Average loss: {np.nanmean(losses)}\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(50*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a33fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, model, loader, criterion):\n",
    "    \n",
    "    print(\"\\tBeginning evaluation.\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print_every = len(loader)//5\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for ix, (X, y, lengths) in enumerate(loader):\n",
    "        \n",
    "        modes, metrics, features = X\n",
    "\n",
    "        y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
    "        y = y.float()\n",
    "        \n",
    "        loss = criterion(y_pred.view(-1, 9), y.view(-1, 9))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if ix and ix % print_every == 0:\n",
    "            print(f\"\\t -> Average loss: {np.nanmean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "650a5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def evaluate_f1(model, tr_loader, te_loader):\n",
    "    tr_preds, te_preds = np.array([]), np.array([])\n",
    "    tr_gt, te_gt = np.array([]), np.array([])\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\\tEvaluating F1...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ix, (X, y, lengths) in enumerate(tr_loader):\n",
    "        \n",
    "            modes, metrics, features = X\n",
    "\n",
    "            y_pred = model(modes, metrics.float(), features.float(), lengths).view(-1, 9)\n",
    "            y = y.float().view(-1, 9)\n",
    "\n",
    "            preds = torch.argmax(F.softmax(y_pred, dim=-1), dim=-1).numpy().ravel()\n",
    "            true = torch.argmax(y.long(), dim=-1).numpy().ravel()\n",
    "\n",
    "            tr_preds = np.append(tr_preds, preds)\n",
    "            tr_gt = np.append(tr_gt, true)\n",
    "            \n",
    "        tr_f1 = f1_score(y_true=tr_gt, y_pred=tr_preds, average='weighted')\n",
    "        print(f\"\\t -> Train F1: {tr_f1}\")\n",
    "        \n",
    "        for ix, (X, y, lengths) in enumerate(te_loader):\n",
    "        \n",
    "            modes, metrics, features = X\n",
    "\n",
    "            y_pred = model(modes, metrics.float(), features.float(), lengths).view(-1, 9)\n",
    "            y = y.float().view(-1, 9)\n",
    "            \n",
    "            preds = torch.argmax(F.softmax(y_pred, dim=-1), dim=-1).numpy().ravel()\n",
    "            true = torch.argmax(y.long(), dim=-1).numpy().ravel()\n",
    "\n",
    "            te_preds = np.append(te_preds, preds)\n",
    "            te_gt = np.append(te_gt, true)\n",
    "        \n",
    "        te_f1 = f1_score(y_true=te_gt, y_pred=te_preds, average='weighted')\n",
    "        print(f\"\\t -> Test F1: {te_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7191e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 5 :: 0.00070\n",
      "5 - 10 :: 0.00067\n",
      "10 - 15 :: 0.00063\n",
      "15 - 20 :: 0.00060\n",
      "20 - 25 :: 0.00057\n",
      "25 - 30 :: 0.00054\n"
     ]
    }
   ],
   "source": [
    "# Other training hyperparameters.\n",
    "num_epochs = 30\n",
    "num_decays = 6\n",
    "decay_at = num_epochs // num_decays\n",
    "decay = 0.95\n",
    "\n",
    "# Just checking what LRs should be after decaying.\n",
    "for power in range(num_decays):\n",
    "    print(f\"{decay_at * power} - {decay_at * (power + 1)} :: {INIT_LR * decay**power:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b72de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 3.4292205003553264\n",
      "\t-> Average loss: 3.0203611136815667\n",
      "\t-> Average loss: 2.9269992429190226\n",
      "\t-> Average loss: 2.7440315224998524\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.9845308899546468\n",
      "\t -> Average loss: 2.0912061837169675\n",
      "\t -> Average loss: 2.0278757847845554\n",
      "\t -> Average loss: 2.079901454872481\n",
      "\t -> Average loss: 2.119112247215273\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.17357571210719355\n",
      "\t -> Test F1: 0.16929205554584628\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 2:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 2.2447872489024805\n",
      "\t-> Average loss: 2.179984362536923\n",
      "\t-> Average loss: 2.184303961117914\n",
      "\t-> Average loss: 2.1579395363231675\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.8604771541483576\n",
      "\t -> Average loss: 1.96936972224629\n",
      "\t -> Average loss: 1.9072258116594\n",
      "\t -> Average loss: 1.9617513231750632\n",
      "\t -> Average loss: 1.9915817378616547\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.2375672788266048\n",
      "\t -> Test F1: 0.23161577435832603\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 3:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 2.009245126523785\n",
      "\t-> Average loss: 1.9820919889074202\n",
      "\t-> Average loss: 2.020936541432881\n",
      "\t-> Average loss: 1.9902592548037359\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.948894374363915\n",
      "\t -> Average loss: 2.0106319545865894\n",
      "\t -> Average loss: 1.9851899344418475\n",
      "\t -> Average loss: 2.007645890627648\n",
      "\t -> Average loss: 1.9973548486545132\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.178078853483658\n",
      "\t -> Test F1: 0.17889559699341365\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 4:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.9483501227707836\n",
      "\t-> Average loss: 1.954028883528493\n",
      "\t-> Average loss: 1.9755866212591542\n",
      "\t-> Average loss: 1.923766012835111\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.8338173299861353\n",
      "\t -> Average loss: 1.917397732834716\n",
      "\t -> Average loss: 1.8882776866875477\n",
      "\t -> Average loss: 1.9411674299967547\n",
      "\t -> Average loss: 1.9390272717393124\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.2937676034584698\n",
      "\t -> Test F1: 0.2797533840531726\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 5:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.912619205160514\n",
      "\t-> Average loss: 1.9341210979918848\n",
      "\t-> Average loss: 1.8943603618724927\n",
      "\t-> Average loss: 1.8774088714056938\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.690006344844509\n",
      "\t -> Average loss: 1.7453476386470395\n",
      "\t -> Average loss: 1.7205723014229268\n",
      "\t -> Average loss: 1.749604014185778\n",
      "\t -> Average loss: 1.742094624483038\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.19402127381450823\n",
      "\t -> Test F1: 0.1897796133960193\n",
      "Learning rate is now: 0.00067\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 6:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.6016220477040253\n",
      "\t-> Average loss: 1.7033424217912552\n",
      "\t-> Average loss: 1.6674059770536112\n",
      "\t-> Average loss: 1.6761879756166485\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.5424185771515915\n",
      "\t -> Average loss: 1.579921578193878\n",
      "\t -> Average loss: 1.5411476174842065\n",
      "\t -> Average loss: 1.5840133090508075\n",
      "\t -> Average loss: 1.5650895305837256\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 7:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.6066383687810526\n",
      "\t-> Average loss: 1.5486349471376795\n",
      "\t-> Average loss: 1.5466778286405511\n",
      "\t-> Average loss: 1.5999376366539295\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.6492426906551063\n",
      "\t -> Average loss: 1.664988368999708\n",
      "\t -> Average loss: 1.6178990316980366\n",
      "\t -> Average loss: 1.6296465851319595\n",
      "\t -> Average loss: 1.640547810166173\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 8:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.4707723498011434\n",
      "\t-> Average loss: 1.5709665634680428\n",
      "\t-> Average loss: 1.554789892461182\n",
      "\t-> Average loss: 1.5148731287814903\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.3899180381324705\n",
      "\t -> Average loss: 1.4627432638531799\n",
      "\t -> Average loss: 1.4123834414101804\n",
      "\t -> Average loss: 1.4201137627063722\n",
      "\t -> Average loss: 1.4303626401260212\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 9:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.5543432552471508\n",
      "\t-> Average loss: 1.4533692163074874\n",
      "\t-> Average loss: 1.3998552637881245\n",
      "\t-> Average loss: 1.4387366909199102\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.3969969056005584\n",
      "\t -> Average loss: 1.4605997060562348\n",
      "\t -> Average loss: 1.4105439760458114\n",
      "\t -> Average loss: 1.4442899707441483\n",
      "\t -> Average loss: 1.4501932569907734\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 10:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.4883403652789873\n",
      "\t-> Average loss: 1.4671129221103143\n",
      "\t-> Average loss: 1.4545180360540984\n",
      "\t-> Average loss: 1.4436850206850294\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.508042848642978\n",
      "\t -> Average loss: 1.502184015554148\n",
      "\t -> Average loss: 1.44931043248012\n",
      "\t -> Average loss: 1.4784588873428093\n",
      "\t -> Average loss: 1.4841084074967386\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.48333374111246125\n",
      "\t -> Test F1: 0.4722792652308931\n",
      "Learning rate is now: 0.00063\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 11:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.4074798150185766\n",
      "\t-> Average loss: 1.3590504143056463\n",
      "\t-> Average loss: 1.3730208680991913\n",
      "\t-> Average loss: 1.4093443684042937\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.65324043395133\n",
      "\t -> Average loss: 1.6848366409748585\n",
      "\t -> Average loss: 1.639395641301995\n",
      "\t -> Average loss: 1.626926458699458\n",
      "\t -> Average loss: 1.6171495253501786\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 12:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.31716444697127\n",
      "\t-> Average loss: 1.2871220497340277\n",
      "\t-> Average loss: 1.2746168654707137\n",
      "\t-> Average loss: 1.2706415657187125\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.2732290915270756\n",
      "\t -> Average loss: 1.3285720576564748\n",
      "\t -> Average loss: 1.2918836618982144\n",
      "\t -> Average loss: 1.2952009379613474\n",
      "\t -> Average loss: 1.2965014228495078\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 13:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.2921466177235768\n",
      "\t-> Average loss: 1.2243878973021864\n",
      "\t-> Average loss: 1.2450620088472837\n",
      "\t-> Average loss: 1.251400762683901\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.206730307313983\n",
      "\t -> Average loss: 1.2312866071721056\n",
      "\t -> Average loss: 1.208183620277959\n",
      "\t -> Average loss: 1.2216688226973618\n",
      "\t -> Average loss: 1.2299620342197903\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 14:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.1780336717736788\n",
      "\t-> Average loss: 1.2387724560844906\n",
      "\t-> Average loss: 1.2171782795114152\n",
      "\t-> Average loss: 1.2286130927401546\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.1736671675683399\n",
      "\t -> Average loss: 1.214920432480065\n",
      "\t -> Average loss: 1.1988129002083816\n",
      "\t -> Average loss: 1.1989111086031705\n",
      "\t -> Average loss: 1.2194756852068667\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 15:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.2168475217576133\n",
      "\t-> Average loss: 1.1433030896716647\n",
      "\t-> Average loss: 1.1906079676311259\n",
      "\t-> Average loss: 1.2125332475604398\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.4089278240776595\n",
      "\t -> Average loss: 1.445234133480312\n",
      "\t -> Average loss: 1.4113176648240926\n",
      "\t -> Average loss: 1.4362174157624148\n",
      "\t -> Average loss: 1.435766333294876\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.5093168030147552\n",
      "\t -> Test F1: 0.5026682286547539\n",
      "Learning rate is now: 0.00060\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 16:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.1724162185824782\n",
      "\t-> Average loss: 1.1224480470021565\n",
      "\t-> Average loss: 1.1285216524044377\n",
      "\t-> Average loss: 1.1660368838476076\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.210316137894572\n",
      "\t -> Average loss: 1.246378187157891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t -> Average loss: 1.2420701925267479\n",
      "\t -> Average loss: 1.2695545183091919\n",
      "\t -> Average loss: 1.273952293582052\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 17:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.0339172488734043\n",
      "\t-> Average loss: 1.0751683378473684\n",
      "\t-> Average loss: 1.1208586298338965\n",
      "\t-> Average loss: 1.1269230102115642\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.248624034475681\n",
      "\t -> Average loss: 1.2845101706423125\n",
      "\t -> Average loss: 1.2307171229161877\n",
      "\t -> Average loss: 1.2223140466163174\n",
      "\t -> Average loss: 1.2198367260838676\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 18:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.0498548433933843\n",
      "\t-> Average loss: 1.0665208559345316\n",
      "\t-> Average loss: 1.0762758616007386\n",
      "\t-> Average loss: 1.099983853102772\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.3901189185221101\n",
      "\t -> Average loss: 1.4441073030024976\n",
      "\t -> Average loss: 1.373450560356254\n",
      "\t -> Average loss: 1.3496260701087575\n",
      "\t -> Average loss: 1.3835973867276747\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 19:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.146373841807496\n",
      "\t-> Average loss: 1.085291807392025\n",
      "\t-> Average loss: 1.079380213429688\n",
      "\t-> Average loss: 1.1145779568127758\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.263674235169115\n",
      "\t -> Average loss: 1.2977842691269788\n",
      "\t -> Average loss: 1.243837918641407\n",
      "\t -> Average loss: 1.2629515259071111\n",
      "\t -> Average loss: 1.2418897094670163\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 20:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 0.9654322663665483\n",
      "\t-> Average loss: 1.0184031338499278\n",
      "\t-> Average loss: 1.0715286856706412\n",
      "\t-> Average loss: 1.0910986138052643\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.1675123429165206\n",
      "\t -> Average loss: 1.2705587090520591\n",
      "\t -> Average loss: 1.200293446571302\n",
      "\t -> Average loss: 1.2205115221362584\n",
      "\t -> Average loss: 1.2177007432530602\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.6190054946992625\n",
      "\t -> Test F1: 0.6097765604266691\n",
      "Learning rate is now: 0.00057\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 21:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.0445893098855152\n",
      "\t-> Average loss: 1.1112236296230893\n",
      "\t-> Average loss: 1.0962648112726123\n",
      "\t-> Average loss: 1.089011266353621\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.0674441020165741\n",
      "\t -> Average loss: 1.1406326175361245\n",
      "\t -> Average loss: 1.1207181137305364\n",
      "\t -> Average loss: 1.1347275835346557\n",
      "\t -> Average loss: 1.1342128750775322\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 22:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.0676555513877775\n",
      "\t-> Average loss: 1.0773798781600055\n",
      "\t-> Average loss: 1.0710756811421827\n",
      "\t-> Average loss: 1.0416493509739522\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.1352787101068975\n",
      "\t -> Average loss: 1.2286776423037469\n",
      "\t -> Average loss: 1.1571767105262225\n",
      "\t -> Average loss: 1.1650525711899125\n",
      "\t -> Average loss: 1.1702847076456864\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 23:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 0.9855309208053783\n",
      "\t-> Average loss: 1.0090662437060927\n",
      "\t-> Average loss: 1.0291784378419606\n",
      "\t-> Average loss: 1.0479850026693265\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.190445274031362\n",
      "\t -> Average loss: 1.2569891696834898\n",
      "\t -> Average loss: 1.1954538017761573\n",
      "\t -> Average loss: 1.2319692372545819\n",
      "\t -> Average loss: 1.2384895061925971\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 24:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 1.035038524541442\n",
      "\t-> Average loss: 1.007654058233497\n",
      "\t-> Average loss: 1.0265389064957022\n",
      "\t-> Average loss: 1.0250597899920955\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.27851615721287\n",
      "\t -> Average loss: 1.3390878153847647\n",
      "\t -> Average loss: 1.315815781799159\n",
      "\t -> Average loss: 1.2937761211286816\n",
      "\t -> Average loss: 1.2826113693858314\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 25:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 0.993596878173298\n",
      "\t-> Average loss: 1.0079565033431157\n",
      "\t-> Average loss: 1.0364318410870301\n",
      "\t-> Average loss: 1.032162597538999\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.2048218791378278\n",
      "\t -> Average loss: 1.2166278918723126\n",
      "\t -> Average loss: 1.1783969860607342\n",
      "\t -> Average loss: 1.1786176204535408\n",
      "\t -> Average loss: 1.1798632480913287\n",
      "\tEvaluating F1...\n",
      "\t -> Train F1: 0.6030226790226174\n",
      "\t -> Test F1: 0.5861836286317499\n",
      "Learning rate is now: 0.00054\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 26:\n",
      "\tBeginning training.\n",
      "\t-> Average loss: 0.9192066853189601\n",
      "\t-> Average loss: 1.0098440380341351\n",
      "\t-> Average loss: 1.006183010403619\n",
      "\t-> Average loss: 1.0023870221366835\n",
      "--------------------------------------------------\n",
      "\tBeginning evaluation.\n",
      "\t -> Average loss: 1.1226611448316601\n",
      "\t -> Average loss: 1.1899717054792218\n",
      "\t -> Average loss: 1.124479945067928\n",
      "\t -> Average loss: 1.1427228787120112\n",
      "\t -> Average loss: 1.1324994321057338\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 27:\n",
      "\tBeginning training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/654011310.py\", line 5, in <module>\n",
      "    train(epoch_ix, model, train_loader, optimizer, criterion)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/2806942638.py\", line 18, in train\n",
      "    y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 143, in forward\n",
      "    lstm_out = module(lstm_out)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 79, in forward\n",
      "    x = x + self.sa(self.ln1(x))\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 64, in forward\n",
      "    cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 64, in <listcomp>\n",
      "    cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 37, in forward\n",
      "    q = self.q(x)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 81, in join\n",
      "    sep = _get_sep(a)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 42, in _get_sep\n",
      "    if isinstance(path, bytes):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/654011310.py\", line 5, in <module>\n",
      "    train(epoch_ix, model, train_loader, optimizer, criterion)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/2806942638.py\", line 18, in train\n",
      "    y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 143, in forward\n",
      "    lstm_out = module(lstm_out)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 79, in forward\n",
      "    x = x + self.sa(self.ln1(x))\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 64, in forward\n",
      "    cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 64, in <listcomp>\n",
      "    cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 37, in forward\n",
      "    q = self.q(x)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/654011310.py\", line 5, in <module>\n",
      "    train(epoch_ix, model, train_loader, optimizer, criterion)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/2806942638.py\", line 18, in train\n",
      "    y_pred = model(modes, metrics.float(), features.float(), lengths)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 143, in forward\n",
      "    lstm_out = module(lstm_out)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 79, in forward\n",
      "    x = x + self.sa(self.ln1(x))\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 64, in forward\n",
      "    cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 64, in <listcomp>\n",
      "    cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/4x/l9lw50rn7qvf79m01f21x70mlpd6gh/T/ipykernel_13888/515180498.py\", line 37, in forward\n",
      "    q = self.q(x)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3258, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 81, in join\n",
      "    sep = _get_sep(a)\n",
      "  File \"/Users/rkulhall/miniconda3/envs/pytorch/lib/python3.7/posixpath.py\", line 42, in _get_sep\n",
      "    if isinstance(path, bytes):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# We'd like to start at a loss of at most -ln(1/9) ~ 2.19\n",
    "\n",
    "for epoch_ix in range(1, num_epochs+1):\n",
    "    print(f\"Epoch {epoch_ix}:\")\n",
    "    train(epoch_ix, model, train_loader, optimizer, criterion)\n",
    "    evaluate(epoch_ix, model, test_loader, criterion)\n",
    "    \n",
    "    if epoch_ix < 5 or epoch_ix % 5 == 0:\n",
    "        # Evaluate every 5 epochs.\n",
    "        evaluate_f1(model, train_loader, test_loader)\n",
    "    \n",
    "    if epoch_ix % decay_at == 0:\n",
    "        optimizer.param_groups[0]['lr'] *= decay\n",
    "        print(f\"Learning rate is now: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "    \n",
    "    print(40*'~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc396c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7d53498",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "```\n",
    "\n",
    "model = Model(\n",
    "    n_lstm_layers=1,\n",
    "    input_size=3,\n",
    "    hidden_size=16, \n",
    "    output_size=9,\n",
    "    n_features=40,\n",
    "    l1_blocks=3,\n",
    "    l2_blocks=3\n",
    ")\n",
    "\n",
    "emb_dim = 16\n",
    "\n",
    "Best stats:\n",
    "-> Train F1: 0.6327794050521978\n",
    "-> Test F1: 0.6208816430930885\n",
    "```\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8dc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
