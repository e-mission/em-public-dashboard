{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All experiments are logged in Notion [here](https://www.notion.so/Replacement-mode-modeling-257c2f460377498d921e6b167f465945)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "# Math and graphing.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn imports.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score, r2_score, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global experiment flags and variables.\n",
    "SEED = 19348\n",
    "TARGETS = ['p_micro', 'no_trip', 's_car', 'transit', 'car', 's_micro', 'ridehail', 'walk', 'unknown']\n",
    "\n",
    "# Set the Numpy seed too.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLIT_TYPE(Enum):\n",
    "    # 5 trips for user1, 4 trips in train, 1 in test\n",
    "    INTRA_USER = 0\n",
    "    # 5 users, 4 users in train, 1 user in test\n",
    "    INTER_USER = 1\n",
    "    # stratifying using target (chosen)\n",
    "    # [5000 samples (c=1), 8000 samples (c=2)]\n",
    "    # [4000 samples c=1, 3200 samples c=2 in train]\n",
    "    TARGET = 2\n",
    "    \n",
    "    MIXED = 3\n",
    "    \n",
    "\n",
    "class SPLIT(Enum):\n",
    "    TRAIN = 0\n",
    "    TEST = 1\n",
    "\n",
    "def get_splits(count_df: pd.DataFrame, n:int, test_size=0.2):\n",
    "    maxsize = int(n * test_size)\n",
    "\n",
    "    max_threshold = int(maxsize * 1.05)\n",
    "    min_threshold = int(maxsize * 0.95)\n",
    "\n",
    "    print(f\"{min_threshold}, {max_threshold}\")\n",
    "    \n",
    "    # Allow a 10% tolerance\n",
    "    def _dp(ix, curr_size, ids, cache):\n",
    "        \n",
    "        if ix >= count_df.shape[0]:\n",
    "            return []\n",
    "\n",
    "        key = ix\n",
    "\n",
    "        if key in cache:\n",
    "            return cache[key]\n",
    "\n",
    "        if curr_size > max_threshold:\n",
    "            return []\n",
    "\n",
    "        if min_threshold <= curr_size <= max_threshold:\n",
    "            return ids\n",
    "\n",
    "        # two options - either pick the current id or skip it.\n",
    "        branch_a = _dp(ix, curr_size+count_df.loc[ix, 'count'], ids+[count_df.loc[ix, 'index']], cache)\n",
    "        branch_b = _dp(ix+1, curr_size, ids, cache)\n",
    "        \n",
    "        curr_max = []\n",
    "        if branch_a and len(branch_a) > 0:\n",
    "            curr_max = branch_a\n",
    "        \n",
    "        if branch_b and len(branch_b) > len(branch_a):\n",
    "            curr_max = branch_b\n",
    "            \n",
    "        cache[key] = curr_max\n",
    "        return cache[key]\n",
    "    \n",
    "    return _dp(0, 0, ids=list(), cache=dict())\n",
    "\n",
    "\n",
    "def get_train_test_splits(data: pd.DataFrame, how=SPLIT_TYPE, test_ratio=0.2, shuffle=True):\n",
    "\n",
    "    n_users = list(data.user_id.unique())\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    if shuffle:\n",
    "        data = data.sample(data.shape[0], random_state=SEED).reset_index(drop=True, inplace=False)\n",
    "\n",
    "    if how == SPLIT_TYPE.INTER_USER:\n",
    "        # Make the split, ensuring that a user in one fold is not leaked into the other fold.\n",
    "        # Basic idea: we want to start with the users with the highest instances and place \n",
    "        # alternating users in each set.\n",
    "        counts = data.user_id.value_counts().reset_index(drop=False, inplace=False, name='count')\n",
    "\n",
    "        # Now, start with the user_id at the top, and keep adding to either split.\n",
    "        # This can be achieved using a simple DP program.\n",
    "        test_ids = get_splits(counts, data.shape[0])\n",
    "        test_data = data.loc[data.user_id.isin(test_ids), :]\n",
    "        train_index = data.index.difference(test_data.index)\n",
    "        train_data = data.loc[data.user_id.isin(train_index), :]\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    elif how == SPLIT_TYPE.INTRA_USER:\n",
    "        \n",
    "        # There are certain users with only one observation. What do we do with those?\n",
    "        # As per the mobilitynet modeling pipeline, we randomly assign them to either the\n",
    "        # training or test set.\n",
    "        \n",
    "        value_counts = data.user_id.value_counts()\n",
    "        single_count_ids = value_counts[value_counts == 1].index\n",
    "        \n",
    "        data_filtered = data.loc[~data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "        data_single_counts = data.loc[data.user_id.isin(single_count_ids), :].reset_index(drop=True)\n",
    "        \n",
    "        X_tr, X_te = train_test_split(\n",
    "            data_filtered, test_size=test_ratio, shuffle=shuffle, stratify=data_filtered.user_id,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        data_single_counts['assigned'] = np.random.choice(['train', 'test'], len(data_single_counts))\n",
    "        X_tr_merged = pd.concat(\n",
    "            [X_tr, data_single_counts.loc[data_single_counts.assigned == 'train', :].drop(\n",
    "                columns=['assigned'], inplace=False\n",
    "            )],\n",
    "            ignore_index=True, axis=0\n",
    "        )\n",
    "        \n",
    "        X_te_merged = pd.concat(\n",
    "            [X_te, data_single_counts.loc[data_single_counts.assigned == 'test', :].drop(\n",
    "                columns=['assigned'], inplace=False\n",
    "            )],\n",
    "            ignore_index=True, axis=0\n",
    "        )\n",
    "        \n",
    "        return X_tr_merged, X_te_merged\n",
    "    \n",
    "    elif how == SPLIT_TYPE.TARGET:\n",
    "        \n",
    "        X_tr, X_te = train_test_split(\n",
    "            data, test_size=test_ratio, shuffle=shuffle, stratify=data.chosen,\n",
    "            random_state=SEED\n",
    "        )\n",
    "        \n",
    "        return X_tr, X_te\n",
    "    \n",
    "    raise NotImplementedError(\"Unknown split type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data.\n",
    "# data = pd.read_csv('../data/FULL_preprocessed_data_RM_weather.csv')\n",
    "# data = pd.read_csv('../data/ReplacedMode_Fix.csv')\n",
    "data = pd.read_csv('../data/ReplacedMode_Fix_02072024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df, features=None):\n",
    "    if not features:\n",
    "        # All features.\n",
    "        features = df.columns.tolist()\n",
    "        \n",
    "    n_features = len(features)\n",
    "    \n",
    "    ncols = 6\n",
    "    nrows = n_features//ncols if n_features%ncols == 0 else (n_features//ncols) + 1\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
    "    for ix, ax in enumerate(axes.flatten()):\n",
    "        \n",
    "        if ix > n_features:\n",
    "            break\n",
    "        \n",
    "        df[features[ix]].hist(ax=ax)\n",
    "        ax.set(title=features[ix])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we map the user IDs to ints.\n",
    "\n",
    "USERS = list(data.user_id.unique())\n",
    "\n",
    "USER_MAP = {\n",
    "    u: i+1 for (i, u) in enumerate(USERS)\n",
    "}\n",
    "\n",
    "data['user_id'] = data['user_id'].apply(lambda x: USER_MAP[x])\n",
    "\n",
    "# data.rename(\n",
    "#     columns={'start_local_dt_weekday': 'start:DOW', 'end_local_dt_weekday': 'end:DOW'},\n",
    "#     inplace=True\n",
    "# )\n",
    "\n",
    "# Drop the samples with chosen == no trip or chosen == unknown\n",
    "# data.drop(index=data.loc[data.chosen.isin([2, 9])].index, inplace=True)\n",
    "\n",
    "# data.n_working_residents = data.n_working_residents.apply(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "# Fix some age preprocessing issues.\n",
    "# data.age = data.age.apply(lambda x: x if x < 100 else 2024-x)\n",
    "\n",
    "# Collapse 'train' and 'bus' into 'transit'\n",
    "# data.loc[\n",
    "#     data.section_mode_argmax.isin(['train', 'bus']), 'section_mode_argmax'\n",
    "# ] = 'transit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(data.section_mode_argmax.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transit = data.loc[data.section_mode_argmax == 'transit', :].copy()\n",
    "# transit['section_duration_argmax'] /= 60.\n",
    "\n",
    "# transit['mph'] = transit['section_distance_argmax']/transit['section_duration_argmax']\n",
    "\n",
    "# display(transit[['section_duration_argmax', 'section_distance_argmax', 'mph']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "\n",
    "# sp = data.loc[data.section_mode_argmax.isin(['car', 'transit', 'walking']), :]\n",
    "# fig = px.line(sp, y='section_distance_argmax', color='section_mode_argmax')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the figure above.\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_estimate(df: pd.DataFrame, dset: SPLIT, model_dict: dict):\n",
    "    \n",
    "    X_features = ['section_distance_argmax', 'age']\n",
    "    \n",
    "    if 'mph' in df.columns:\n",
    "        X_features += ['mph']\n",
    "    \n",
    "    if dset == SPLIT.TRAIN and model_dict is None:\n",
    "        model_dict = dict()\n",
    "    \n",
    "    if dset == SPLIT.TEST and model_dict is None:\n",
    "        raise AttributeError(\"Expected model dict for testing.\")\n",
    "    \n",
    "    if dset == SPLIT.TRAIN:\n",
    "        for section_mode in df.section_mode_argmax.unique():\n",
    "            section_data = df.loc[df.section_mode_argmax == section_mode, :]\n",
    "            if section_mode not in model_dict:\n",
    "                model_dict[section_mode] = dict()\n",
    "\n",
    "                model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "                X = section_data[\n",
    "                    X_features\n",
    "                ]\n",
    "                Y = section_data[['section_duration_argmax']]\n",
    "\n",
    "                model.fit(X, Y.values.ravel())\n",
    "\n",
    "                r2 = r2_score(y_pred=model.predict(X), y_true=Y.values.ravel())\n",
    "                print(f\"Train R2 for {section_mode}: {r2}\")\n",
    "\n",
    "                model_dict[section_mode]['model'] = model\n",
    "                \n",
    "    elif dset == SPLIT.TEST:\n",
    "        for section_mode in df.section_mode_argmax.unique():\n",
    "            section_data = df.loc[df.section_mode_argmax == section_mode, :]\n",
    "            X = section_data[\n",
    "                X_features\n",
    "            ]\n",
    "            Y = section_data[['section_duration_argmax']]\n",
    "            \n",
    "            y_pred = model_dict[section_mode]['model'].predict(X)\n",
    "            r2 = r2_score(y_pred=y_pred, y_true=Y.values.ravel())\n",
    "            print(f\"Test R2 for {section_mode}: {r2}\")\n",
    "    \n",
    "    # Create the new columns for the duration.\n",
    "    new_columns = ['p_micro','no_trip','s_car','transit','car','s_micro','ridehail','walk','unknown']\n",
    "    df[new_columns] = 0\n",
    "    df['temp'] = 0\n",
    "    \n",
    "    for section in df.section_mode_argmax.unique():\n",
    "        X_section = df.loc[df.section_mode_argmax == section, X_features]\n",
    "        \n",
    "        # broadcast to all columns.\n",
    "        df.loc[df.section_mode_argmax == section, 'temp'] = model_dict[section]['model'].predict(X_section)\n",
    "    \n",
    "    for c in new_columns:\n",
    "        df[c] = df['av_' + c] * df['temp']\n",
    "    \n",
    "    df.drop(columns=['temp'], inplace=True)\n",
    "    \n",
    "    df.rename(columns=dict([(x, 'tt_'+x) for x in new_columns]), inplace=True)\n",
    "    \n",
    "    # return model_dict, result_df\n",
    "    return model_dict, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we split the data (either inter-user or intra-user split)\n",
    "\n",
    "# train_data, test_data = get_train_test_splits(data=data, how=SPLIT_TYPE.INTER_USER, shuffle=True)\n",
    "\n",
    "train_data, test_data = get_train_test_splits(data=data, how=SPLIT_TYPE.INTRA_USER, shuffle=True)\n",
    "\n",
    "# train_data, test_data = get_train_test_splits(data=data, how=SPLIT_TYPE.TARGET, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', '_id', 'original_user_id', 'cleaned_trip', 'Mode_confirm',\n",
      "       'start_fmt_time', 'start:year', 'start:month', 'start:day',\n",
      "       'start:hour', 'start:DOW', 'end_fmt_time', 'end:year', 'end:month',\n",
      "       'end:day', 'end:hour', 'end:DOW', 'available_modes', 'birth_year',\n",
      "       'income_category', 'n_motor_vehicles', 'n_residence_members',\n",
      "       'n_residents_u18', 'gender', 'is_student', 'n_residents_with_license',\n",
      "       'duration', 'distance_miles', 'start_loc', 'end_loc', 'section_modes',\n",
      "       'section_distances', 'start:n_days_in_month', 'end:n_days_in_month',\n",
      "       'age', 'is_overnight_trip', 'n_working_residents', 'is_male',\n",
      "       'start_lat', 'start_lng', 'end_lat', 'end_lng', 'start:sin_HOD',\n",
      "       'start:sin_DOM', 'start:sin_MOY', 'start:cos_HOD', 'start:cos_DOM',\n",
      "       'start:cos_MOY', 'end:sin_HOD', 'end:sin_DOM', 'end:sin_MOY',\n",
      "       'end:cos_HOD', 'end:cos_DOM', 'end:cos_MOY', 'section_durations',\n",
      "       'section_locations_argmax', 'temperature_2m (°F)',\n",
      "       'relative_humidity_2m (%)', 'dew_point_2m (°F)', 'rain (inch)',\n",
      "       'snowfall (inch)', 'cloud_cover (%)', 'wind_speed_10m (mp/h)',\n",
      "       'wind_gusts_10m (mp/h)', 'section_distance_argmax',\n",
      "       'section_duration_argmax', 'section_mode_argmax',\n",
      "       'section_coordinates_argmax', 'mph', 'chosen', 'av_car', 'av_s_car',\n",
      "       'av_no_trip', 'av_walk', 'av_transit', 'av_s_micro', 'av_p_micro',\n",
      "       'av_ridehail', 'av_unknown', 'cost_p_micro', 'cost_no_trip',\n",
      "       'cost_s_car', 'cost_transit', 'cost_car', 'cost_s_micro',\n",
      "       'cost_ridehail', 'cost_walk', 'cost_unknown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2 for car: 0.9107819633844028\n",
      "Train R2 for bicycling: 0.9381499933467025\n",
      "Train R2 for walking: 0.7840120837242898\n",
      "Train R2 for no_sensed: 0.838164213315293\n",
      "Train R2 for transit: 0.9167190695089265\n",
      "----------\n",
      "Test R2 for car: 0.9115083509175145\n",
      "Test R2 for walking: 0.7836715824022498\n",
      "Test R2 for no_sensed: 0.8457638150514823\n",
      "Test R2 for bicycling: 0.94215202813422\n",
      "Test R2 for transit: 0.9130004787209818\n"
     ]
    }
   ],
   "source": [
    "params, train_data = get_duration_estimate(train_data, SPLIT.TRAIN, None)\n",
    "print(10 * \"-\")\n",
    "_, test_data = get_duration_estimate(test_data, SPLIT.TEST, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34064, 97), (8517, 97))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions that will help ease redundancy in the code.\n",
    "\n",
    "def drop_columns(df: pd.DataFrame):\n",
    "    u_time_features = [\n",
    "        'start_fmt_time', 'start:year', 'start:month', 'start:day',\n",
    "        'start:hour', 'end_fmt_time', 'end:year',\n",
    "        'end:month', 'end:day', 'end:hour', 'end:n_days_in_month', \n",
    "        'start:sin_DOM', 'start:sin_MOY', 'start:cos_MOY', 'start:cos_DOM',\n",
    "        'end:sin_DOM', 'end:sin_MOY', 'end:cos_DOM', 'end:cos_MOY', 'start:n_days_in_month'\n",
    "    ]\n",
    "\n",
    "    u_user_features = [\n",
    "        '_id', 'original_user_id', 'gender', 'birth_year',\n",
    "#         'user_id', \n",
    "    ]\n",
    "    \n",
    "    u_trip_features = [\n",
    "        'cleaned_trip', 'Mode_confirm', 'available_modes', 'duration', 'start_loc',\n",
    "        'end_loc', 'section_modes', 'section_distances', 'section_durations',\n",
    "        'section_locations_argmax', 'section_mode_argmax', 'section_coordinates_argmax',\n",
    "#         'start_lat', 'start_lng', 'end_lat', 'end_lng'\n",
    "    ]\n",
    "    \n",
    "    # Drop section_mode_argmax and available_modes.\n",
    "    return df.drop(\n",
    "        columns=u_time_features + u_user_features + u_trip_features, \n",
    "        inplace=False\n",
    "    )\n",
    "\n",
    "\n",
    "def scale_values(df: pd.DataFrame, split: SPLIT, scalers=None):\n",
    "    # Scale costs using StandardScaler.\n",
    "    costs = df[[c for c in df.columns if 'cost_' in c]].copy()\n",
    "    times = df[[c for c in df.columns if 'tt_' in c or 'duration' in c]].copy()\n",
    "    distances = df[[c for c in df.columns if 'distance' in c]]\n",
    "    \n",
    "    print(\n",
    "        \"Cost columns to be scaled: \", costs.columns,\"\\nTime columns to be scaled: \", times.columns, \\\n",
    "        \"\\nDistance columns to be scaled: \", distances.columns\n",
    "    )\n",
    "    \n",
    "    if split == SPLIT.TRAIN and scalers is None:\n",
    "        cost_scaler = StandardScaler()\n",
    "        tt_scaler = StandardScaler()\n",
    "        dist_scaler = StandardScaler()\n",
    "        \n",
    "        cost_scaled = pd.DataFrame(\n",
    "            cost_scaler.fit_transform(costs), \n",
    "            columns=costs.columns, \n",
    "            index=costs.index\n",
    "        )\n",
    "        \n",
    "        tt_scaled = pd.DataFrame(\n",
    "            tt_scaler.fit_transform(times),\n",
    "            columns=times.columns,\n",
    "            index=times.index\n",
    "        )\n",
    "        \n",
    "        dist_scaled = pd.DataFrame(\n",
    "            dist_scaler.fit_transform(distances),\n",
    "            columns=distances.columns,\n",
    "            index=distances.index\n",
    "        )\n",
    "    \n",
    "    elif split == SPLIT.TEST and scalers is not None:\n",
    "        \n",
    "        cost_scaler, tt_scaler, dist_scaler = scalers\n",
    "        \n",
    "        cost_scaled = pd.DataFrame(\n",
    "            cost_scaler.transform(costs), \n",
    "            columns=costs.columns, \n",
    "            index=costs.index\n",
    "        )\n",
    "        \n",
    "        tt_scaled = pd.DataFrame(\n",
    "            tt_scaler.transform(times), \n",
    "            columns=times.columns, \n",
    "            index=times.index\n",
    "        )\n",
    "        \n",
    "        dist_scaled = pd.DataFrame(\n",
    "            dist_scaler.transform(distances),\n",
    "            columns=distances.columns,\n",
    "            index=distances.index\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown split\")\n",
    "    \n",
    "    # Drop the original columns.\n",
    "    df.drop(\n",
    "        columns=costs.columns.tolist() + times.columns.tolist() + distances.columns.tolist(), \n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    df = df.merge(right=cost_scaled, left_index=True, right_index=True)\n",
    "    df = df.merge(right=tt_scaled, left_index=True, right_index=True)\n",
    "    df = df.merge(right=dist_scaled, left_index=True, right_index=True)\n",
    "    \n",
    "    return df, (cost_scaler, tt_scaler, dist_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, drop columns.\n",
    "\n",
    "train_data = drop_columns(train_data)\n",
    "\n",
    "# Scale cost.\n",
    "# train_data, scalers = scale_values(train_data, SPLIT.TRAIN, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = drop_columns(test_data)\n",
    "\n",
    "# Scale cost.\n",
    "# test_data, _ = scale_values(test_data, SPLIT.TEST, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'start:DOW', 'end:DOW', 'income_category',\n",
       "       'n_motor_vehicles', 'n_residence_members', 'n_residents_u18',\n",
       "       'is_student', 'n_residents_with_license', 'distance_miles', 'age',\n",
       "       'is_overnight_trip', 'n_working_residents', 'is_male', 'start_lat',\n",
       "       'start_lng', 'end_lat', 'end_lng', 'start:sin_HOD', 'start:cos_HOD',\n",
       "       'end:sin_HOD', 'end:cos_HOD', 'temperature_2m (°F)',\n",
       "       'relative_humidity_2m (%)', 'dew_point_2m (°F)', 'rain (inch)',\n",
       "       'snowfall (inch)', 'cloud_cover (%)', 'wind_speed_10m (mp/h)',\n",
       "       'wind_gusts_10m (mp/h)', 'section_distance_argmax',\n",
       "       'section_duration_argmax', 'mph', 'chosen', 'av_car', 'av_s_car',\n",
       "       'av_no_trip', 'av_walk', 'av_transit', 'av_s_micro', 'av_p_micro',\n",
       "       'av_ridehail', 'av_unknown', 'cost_p_micro', 'cost_no_trip',\n",
       "       'cost_s_car', 'cost_transit', 'cost_car', 'cost_s_micro',\n",
       "       'cost_ridehail', 'cost_walk', 'cost_unknown', 'tt_p_micro',\n",
       "       'tt_no_trip', 'tt_s_car', 'tt_transit', 'tt_car', 'tt_s_micro',\n",
       "       'tt_ridehail', 'tt_walk', 'tt_unknown'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.chosen.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from pprint import pprint\n",
    "from sklearn.inspection import permutation_importance\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# exp question - compute sample weights using user_id.\n",
    "\n",
    "rf_train = train_data.drop(columns=['chosen', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'user_id'])\n",
    "rf_test = test_data.drop(columns=['chosen', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'user_id'])\n",
    "\n",
    "if CV:\n",
    "\n",
    "    model = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "    # We want to build bootstrapped trees that would not always use all the features.\n",
    "\n",
    "    param_set2 = {\n",
    "        'n_estimators': [150, 200, 250],\n",
    "        'min_samples_split': [2, 3],\n",
    "        'class_weight': ['balanced_subsample'],\n",
    "        'max_features': [None, 'sqrt'],\n",
    "        'bootstrap': [True]\n",
    "    }\n",
    "\n",
    "    cv_set2 = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "    clf_set2 = GridSearchCV(model, param_set2, cv=cv_set2, n_jobs=-1, scoring='f1_weighted', verbose=1)\n",
    "\n",
    "    start = perf_counter()\n",
    "\n",
    "    clf_set2.fit(\n",
    "        rf_train,\n",
    "        train_data.chosen.values.ravel()\n",
    "    )\n",
    "\n",
    "    time_req = (perf_counter() - start)/60.\n",
    "\n",
    "    best_model = clf_set2.best_estimator_\n",
    "else:\n",
    "    best_model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        min_samples_split=3,\n",
    "        bootstrap=True,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    ).fit(rf_train, train_data.chosen.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_f1_set1 = f1_score(\n",
    "#     y_true=train_data.chosen.values,\n",
    "#     y_pred=model_set1.predict(rf_train),\n",
    "#     average='weighted'\n",
    "# )\n",
    "\n",
    "tr_f1_set2 = f1_score(\n",
    "    y_true=train_data.chosen.values,\n",
    "    y_pred=best_model.predict(rf_train),\n",
    "    average='weighted'\n",
    ")\n",
    "\n",
    "# print(tr_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te_f1_set1 = f1_score(\n",
    "#     y_true=test_data.chosen.values,s\n",
    "#     y_pred=model_set1.predict(rf_test),\n",
    "#     average='weighted'\n",
    "# )\n",
    "\n",
    "te_f1_set2 = f1_score(\n",
    "    y_true=test_data.chosen.values,\n",
    "    y_pred=best_model.predict(rf_test),\n",
    "    average='weighted'\n",
    ")\n",
    "\n",
    "# print(te_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOOTSTRAPPED] | Train F1: 1.0, Test F1: 0.7344136324607913\n"
     ]
    }
   ],
   "source": [
    "# print(f\"[NON BOOTSTRAPPED] | Train F1: {tr_f1_set1}, Test F1: {te_f1_set1}\")\n",
    "print(f\"[BOOTSTRAPPED] | Train F1: {tr_f1_set2}, Test F1: {te_f1_set2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('age', 0.13052971325233453),\n",
      " ('income_category', 0.05563529282438796),\n",
      " ('n_motor_vehicles', 0.05117069898186447),\n",
      " ('mph', 0.04901096852140811),\n",
      " ('dew_point_2m (°F)', 0.046445523590839706),\n",
      " ('temperature_2m (°F)', 0.04332851959366878),\n",
      " ('n_residents_u18', 0.04079428459138862),\n",
      " ('cost_transit', 0.03599739479849181),\n",
      " ('distance_miles', 0.03410054518532979),\n",
      " ('wind_gusts_10m (mp/h)', 0.02664916084517161),\n",
      " ('relative_humidity_2m (%)', 0.026557460924608728),\n",
      " ('wind_speed_10m (mp/h)', 0.02621237413168378),\n",
      " ('n_residence_members', 0.024503799663918274),\n",
      " ('section_duration_argmax', 0.024244750804804545),\n",
      " ('n_working_residents', 0.023773443113737733),\n",
      " ('n_residents_with_license', 0.023628029234229537),\n",
      " ('cloud_cover (%)', 0.02246590194480327),\n",
      " ('cost_s_micro', 0.022458268111427003),\n",
      " ('tt_p_micro', 0.02038772534963909),\n",
      " ('start:cos_HOD', 0.019229222189042564),\n",
      " ('is_male', 0.01922412856640586),\n",
      " ('end:cos_HOD', 0.019203897735717918),\n",
      " ('section_distance_argmax', 0.015253896310661041),\n",
      " ('end:sin_HOD', 0.015124047093651035),\n",
      " ('tt_walk', 0.015027814935913562),\n",
      " ('start:DOW', 0.014649650753683777),\n",
      " ('start:sin_HOD', 0.014574215098017478),\n",
      " ('cost_s_car', 0.013824194676480045),\n",
      " ('end:DOW', 0.013763191399800697),\n",
      " ('cost_car', 0.013558725784127607),\n",
      " ('cost_ridehail', 0.013029878623553506),\n",
      " ('tt_transit', 0.012674148135131848),\n",
      " ('is_student', 0.010245832739145545),\n",
      " ('tt_s_car', 0.009950842169405066),\n",
      " ('tt_car', 0.00979054895155533),\n",
      " ('tt_s_micro', 0.008712803885105263),\n",
      " ('tt_ridehail', 0.008579952310351583),\n",
      " ('av_p_micro', 0.005419336568222103),\n",
      " ('av_walk', 0.003300488646146801),\n",
      " ('av_transit', 0.0028638611907256797),\n",
      " ('rain (inch)', 0.0026761074332931224),\n",
      " ('av_car', 0.002642247634705974),\n",
      " ('av_s_car', 0.0018072513622939949),\n",
      " ('snowfall (inch)', 0.0016959335576669646),\n",
      " ('tt_unknown', 0.001566922069109617),\n",
      " ('av_ridehail', 0.0014388235245583704),\n",
      " ('av_s_micro', 0.0009841196788956074),\n",
      " ('tt_no_trip', 0.0007220255139425819),\n",
      " ('av_unknown', 0.0004000980804160885),\n",
      " ('av_no_trip', 0.00017193791853587658),\n",
      " ('is_overnight_trip', 0.0),\n",
      " ('cost_p_micro', 0.0),\n",
      " ('cost_no_trip', 0.0),\n",
      " ('cost_walk', 0.0),\n",
      " ('cost_unknown', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Feature importances - gini entropy\n",
    "\n",
    "pprint(\n",
    "    sorted(\n",
    "        zip(\n",
    "            best_model.feature_names_in_, \n",
    "            best_model.feature_importances_\n",
    "        ), \n",
    "        key=lambda x: x[-1], reverse=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'permutation_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m importance \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m(\n\u001b[1;32m      2\u001b[0m     best_model,\n\u001b[1;32m      3\u001b[0m     rf_test,\n\u001b[1;32m      4\u001b[0m     test_data\u001b[38;5;241m.\u001b[39mchosen\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      5\u001b[0m     n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      6\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mSEED,\n\u001b[1;32m      7\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'permutation_importance' is not defined"
     ]
    }
   ],
   "source": [
    "importance = permutation_importance(\n",
    "    best_model,\n",
    "    rf_test,\n",
    "    test_data.chosen.values,\n",
    "    n_repeats=5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1_weighted'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        'feature names': test_data.columns.delete(\n",
    "            test_data.columns.isin(['chosen'])\n",
    "        ),\n",
    "        'imp_mean': importance.importances_mean, \n",
    "        'imp_std': importance.importances_std\n",
    "    }\n",
    ").sort_values(by=['imp_mean'], axis='rows', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "y_pred = best_model.predict(rf_test)\n",
    "pred_df = pd.DataFrame(\n",
    "    {\n",
    "        'y_pred': y_pred.ravel(),\n",
    "        'y_true': test_data.chosen.values.ravel()\n",
    "    }\n",
    ")\n",
    "\n",
    "# pred_df.y_pred.hist(ax=ax[0])\n",
    "# pred_df.y_true.hist(ax=ax[1])\n",
    "\n",
    "# ax[0].set(\n",
    "#     xlabel=\"Label\",\n",
    "#     ylabel=\"Count\",\n",
    "#     title=\"Prediction\"\n",
    "# )\n",
    "\n",
    "# ax[1].set(\n",
    "#     xlabel=\"Label\",\n",
    "#     ylabel=\"Count\",\n",
    "#     title=\"GT\"\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "cm = ConfusionMatrixDisplay.from_estimator(\n",
    "    best_model,\n",
    "    X=rf_test,\n",
    "    y=test_data[['chosen']],\n",
    "    ax=ax\n",
    ")\n",
    "# ax.set_xticklabels(TARGETS, rotation=45)\n",
    "# ax.set_yticklabels(TARGETS)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=pred_df.y_true, y_pred=pred_df.y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# sample_weights = compute_sample_weight(class_weight='balanced', y=train_data.user_id.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "y_train = train_data.chosen.values.ravel() - 1\n",
    "y_test = test_data.chosen.values.ravel()\n",
    "\n",
    "# weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_pred), y_pred)\n",
    "\n",
    "xgm = XGBClassifier(\n",
    "    n_estimators=250,\n",
    "    max_depth=None,\n",
    "    tree_method='hist',\n",
    "    objective='multi:softmax',\n",
    "    num_class=9\n",
    ").fit(rf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgm.predict(rf_test) + 1\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # RF_RM.pkl = 0.8625 on test.\n",
    "# # RF_RM_1.pkl = 0.77 on test.\n",
    "# with open('../models/RF_RM_1.pkl', 'wb') as f:\n",
    "#     f.write(pickle.dumps(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "\n",
    "- Explain why location might not be a good feature to add (plot start and end on map and explain how model might just overfit to the raw coordinates)\n",
    "- Merge `unknown` and `no_trip` into one category and validate against models trained on (a) separate labels (b) dropped labels\n",
    "- Explore more of the abnormal `walking` trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab0c6e94c9422d07d42069ec9e3bb23090f5e156fc0e23cc25ca45a62375bf53"
  },
  "kernelspec": {
   "display_name": "emission",
   "language": "python",
   "name": "emission"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
