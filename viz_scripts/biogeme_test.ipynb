{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = None\n",
    "month = None\n",
    "program = \"prepilot\"\n",
    "include_replaced_modes_as_valid = True # Flip this when we want to get results versus generate the replaced_mode correction graphs\n",
    "input_dataset = \"ONLY_LABELED\" # \"ONLY_LABELED\", \"ONLY_SENSED\" or \"BEST_AVAILABLE\" for sensitivity analysis\n",
    "LABEL_ASSIST_THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.models as models\n",
    "import biogeme.distributions as dist\n",
    "from biogeme.expressions import Beta, DefineVariable, RandomVariable, bioDraws, log, MonteCarlo, Integrate\n",
    "import biogeme.results as res\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import emission.core.get_database as edb\n",
    "import emission.core.wrapper.entry as ecwe\n",
    "import emission.storage.decorations.analysis_timeseries_queries as esda\n",
    "import emission.storage.decorations.trip_queries as esdt\n",
    "import emission.storage.decorations.timeline as esdl\n",
    "import emission.storage.timeseries.abstract_timeseries as esta\n",
    "import emission.storage.timeseries.timequery as estt\n",
    "import scaffolding\n",
    "from uuid import UUID\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data From Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading mapping dictionaries from mapping_dictionaries notebook\n",
    "# Dictionary map is putting all other replaced modes into other\n",
    "%store -r df_EI\n",
    "%store -r dic_re\n",
    "%store -r dic_pur\n",
    "%store -r dic_fuel\n",
    "\n",
    "# convert a dictionary to a defaultdict\n",
    "dic_pur = defaultdict(lambda: 'Other',dic_pur)\n",
    "dic_re = defaultdict(lambda: 'Other',dic_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tq = scaffolding.get_time_query(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's get UUID lists for all the three categories\n",
    "# stage, all, non_stage\n",
    "stage_uuids = []\n",
    "all_uuids = []\n",
    "non_stage_uuids = []\n",
    "for ue in edb.get_uuid_db().find():\n",
    "    all_uuids.append(str(ue['uuid']))\n",
    "    if ue['user_email'].startswith(\"stage_\"):\n",
    "        stage_uuids.append(str(ue['uuid']))\n",
    "    else:\n",
    "        non_stage_uuids.append(str(ue['uuid']))\n",
    "stage_uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the proportions across them\n",
    "len(stage_uuids), len(non_stage_uuids), len(all_uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the participant confirmed trips\n",
    "participant_ct_df = scaffolding.load_all_participant_trips(program, tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait, we have 219 unique trips, which doesn't match any of the numbers\n",
    "len(participant_ct_df.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if this is because of strings; nope\n",
    "participant_ct_df[\"user_id_str\"] = participant_ct_df.user_id.apply(lambda u: str(u))\n",
    "len(participant_ct_df.user_id_str.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which category the missing trips fit into\n",
    "missing_uuids = set(all_uuids).difference(set(participant_ct_df.user_id_str))\n",
    "len(missing_uuids), 245 - 219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(missing_uuids).intersection(stage_uuids), set(missing_uuids).intersection(non_stage_uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They seem to be split pretty evenly between stage and non stage\n",
    "len(set(missing_uuids).intersection(stage_uuids)), len(set(missing_uuids).intersection(non_stage_uuids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stage users, comparing string, we find a difference\n",
    "non_stage_ct_df = participant_ct_df[~participant_ct_df['user_id_str'].isin(stage_uuids)]\n",
    "stage_ct_df = participant_ct_df[participant_ct_df['user_id_str'].isin(stage_uuids)]\n",
    "print(len(non_stage_ct_df))\n",
    "print(len(stage_ct_df))\n",
    "print(len(participant_ct_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have missing UUIDs, let's confirm that none of them have trips in the database\n",
    "# and that we are not missing trips just because of read limits\n",
    "missing_stage_uuids = set(missing_uuids).intersection(stage_uuids)\n",
    "missing_non_stage_uuids = set(missing_uuids).intersection(non_stage_uuids)\n",
    "\n",
    "from uuid import UUID\n",
    "import emission.core.get_database as edb\n",
    "\n",
    "for uuid_str in missing_stage_uuids.union(missing_non_stage_uuids):\n",
    "    print(f\"For {uuid_str}, found %d trips in the database\" % \n",
    "          (edb.get_analysis_timeseries_db().count_documents({\"user_id\": UUID(uuid_str), \"metadata.key\": \"analysis/confirmed_trip\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_ct = scaffolding.filter_labeled_trips(participant_ct_df)\n",
    "# expanded_ct = scaffolding.expand_userinputs(labeled_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sensed as well as labeled trips\n",
    "expanded_ct = scaffolding.expand_userinputs(participant_ct_df)\n",
    "expanded_stage_ct = scaffolding.expand_userinputs(stage_ct_df)\n",
    "expanded_non_stage_ct = scaffolding.expand_userinputs(non_stage_ct_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using non-stage users\n",
    "expanded_ct = scaffolding.data_quality_check(expanded_non_stage_ct)\n",
    "expanded_ct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SHANKARI: I am not sure it is OK to fill in the inferred values as `mode_confirm`\n",
    "# mode confirm should be reserved for user input\n",
    "# the inferred labels are generated by the label assist algorithm and have accuracy of ~ 50%\n",
    "\n",
    "def describe_labels(ct_df):\n",
    "    # How much data total\n",
    "    print(\"Total number of trips\", len(ct_df))\n",
    "\n",
    "    # data with user specified modes\n",
    "    print(\"Trips with user specified labels\", len(ct_df[~pd.isna(ct_df.mode_confirm)]))\n",
    "      \n",
    "    # how much data without labels and with and without label assist\n",
    "    no_user_label_ct_df = ct_df[pd.isna(ct_df.mode_confirm)]\n",
    "    print(\"Trips without user specified labels\", len(no_user_label_ct_df))\n",
    "    is_empty_check = lambda ll: len(ll) == 0\n",
    "    print(\"Trips without user label but with inferred label\", len(no_user_label_ct_df[~no_user_label_ct_df.inferred_labels.apply(is_empty_check)]))\n",
    "    print(\"Trips without user label or inferred label\", len(no_user_label_ct_df[no_user_label_ct_df.inferred_labels.apply(is_empty_check)]))\n",
    "\n",
    "describe_labels(expanded_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_user_label_ct_df[no_user_label_ct_df.inferred_labels.apply(is_empty_check)][[\"inferred_labels\", \"start_fmt_time\", \"end_fmt_time\", \"mode_confirm\", \"purpose_confirm\", \"user_id\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we really only have 57k labeled trips, not 115k\n",
    "# we can generate results with label assist and/or primary sensed mode for comparison,\n",
    "# but I think that the most principled version of the evaluation should use only labeled trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/allenmichael099/e-mission-eval-private-data/blob/hybrid_labeling_analysis_Michael/Error_bars/add_new_label_fields.py#L45\n",
    "# TODO: Need to clean up later instead of copy-paste\n",
    "def get_primary_sensed_mode(ct_row):\n",
    "    # These keys were found in emission/core/wrapper/modeprediction.py:\n",
    "    sensed_mode_types = {0: \"unknown\", 1: \"drove_alone\",2: \"bike\",\n",
    "                     3: \"bus\", 4: \"train\", 5: \"car\", 6: \"air_or_hsr\",\n",
    "                     7: \"train\", 8: \"train\", 9: \"train\"}\n",
    "\n",
    "    # Get the segments for the trip.\n",
    "    #cleaned_section will only have walk/bike/automotive, inferred_section is the one that has bus/train/car etc \n",
    "    segments = esdt.get_sections_for_trip(key = \"analysis/inferred_section\", user_id = ct_row[\"user_id\"], trip_id = ct_row['cleaned_trip'])\n",
    "\n",
    "    # get pairs of mode type and duration\n",
    "    trip_mode_durations = {}\n",
    "    total_dur = 0\n",
    "    for s in segments:\n",
    "\n",
    "        # the sensed mode is a number in the database, so I'm relabeling it as a string.\n",
    "        mode = sensed_mode_types[s['data']['sensed_mode']]\n",
    "        duration = s['data']['duration']\n",
    "\n",
    "        if mode not in trip_mode_durations.keys(): trip_mode_durations[mode] = 0\n",
    "        trip_mode_durations[mode] += duration\n",
    "\n",
    "        total_dur += duration\n",
    "    # convert the durations to fractions of the total segment moving time (not the trip time, since trips include stop times)\n",
    "    # output is something like {'bicycling': 0.3244066758052265, 'walking': 0.6755933241947736}\n",
    "    mode_split_map = pd.Series({mode: duration/total_dur  for mode,duration in trip_mode_durations.items()})\n",
    "    primary_mode = mode_split_map.index[mode_split_map.argmax()]\n",
    "    print(f\"After processing {len(segments)} segments for trip {ct_row['_id']} for user {ct_row['user_id']}, returning {primary_mode}\")\n",
    "    return primary_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small unit test\n",
    "# mode_split_map = pd.Series({'bicycling': 0.3244066758052265, 'walking': 0.6755933241947736})\n",
    "# mode_split_map.index[mode_split_map.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_prob_label(inferred_label_list, p_threshold):\n",
    "    # copied from emission/storage/decorations/trip_queries.py\n",
    "    # Two columns: \"labels\" and \"p\"\n",
    "    label_prob_df = pd.DataFrame(inferred_label_list)\n",
    "    # logging.debug(label_prob_df)\n",
    "    # idxmax returns the index corresponding to the max data value in each column\n",
    "    max_p = label_prob_df.p.max()\n",
    "    if max_p > p_threshold:\n",
    "        max_p_idx = label_prob_df.p.idxmax()\n",
    "        # logging.debug(max_p_idx)\n",
    "        # now we look up the labels for that index\n",
    "        return label_prob_df.loc[max_p_idx].labels\n",
    "    else:\n",
    "        print(f\"max_p {max_p} < threshold {p_threshold}, returning None\")\n",
    "        return None\n",
    "\n",
    "def get_best_label_assist_mode(ct_row, p_threshold):\n",
    "    # copied and modified from emission/storage/decorations/trip_queries.py line 290-ish\n",
    "    all_inferred_labels = ct_row.inferred_labels\n",
    "    if len(all_inferred_labels) > 0:\n",
    "        max_p_labels = get_max_prob_label(all_inferred_labels, p_threshold)\n",
    "        return max_p_labels[\"mode_confirm\"] if max_p_labels is not None else None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_best_hybrid_mode(ct_row):\n",
    "    if ct_row.mode_confirm is not None:\n",
    "        return ct_row\n",
    "    else:\n",
    "        best_label_assist_mode = get_best_label_assist_mode(ct_row, LABEL_ASSIST_THRESHOLD)\n",
    "        if best_label_assist_mode is not None:\n",
    "            return best_label_assist_mode\n",
    "        else:\n",
    "            return get_primary_sensed_mode(ct_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic sanity checks\n",
    "sanity_test_df = expanded_ct.sample(n=50, random_state=1234)\n",
    "print(\"Sanity testing characteristics \")\n",
    "describe_labels(sanity_test_df)\n",
    "\n",
    "only_sensed_df = sanity_test_df.copy()\n",
    "only_sensed_df.mode_confirm = sanity_test_df.apply(lambda row: get_primary_sensed_mode(row), axis=1)\n",
    "# This should be zero\n",
    "print(only_sensed_df[pd.isna(only_sensed_df.mode_confirm)])\n",
    "\n",
    "only_label_assist_df = sanity_test_df.copy()\n",
    "only_label_assist_df.mode_confirm = sanity_test_df.apply(lambda row: get_best_label_assist_mode(row, LABEL_ASSIST_THRESHOLD), axis=1)\n",
    "print(only_label_assist_df[pd.isna(only_label_assist_df.mode_confirm)][[\"start_fmt_time\", \"end_fmt_time\"]])\n",
    "\n",
    "hybrid_df = sanity_test_df.copy()\n",
    "hybrid_df.mode_confirm = sanity_test_df.apply(lambda row: get_best_hybrid_mode(row), axis=1)\n",
    "print(hybrid_df[pd.isna(hybrid_df.mode_confirm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_dataset == \"ONLY_LABELED\":\n",
    "    expanded_ct = scaffolding.filter_labeled_trips(expanded_ct)\n",
    "elif input_dataset == \"BEST_AVAILABLE\":\n",
    "    expanded_ct = expanded_ct.apply(lambda row: get_best_hybrid_mode(row), axis=1)\n",
    "elif input_dataset == \"ONLY_SENSED\":\n",
    "    expanded_ct.mode_confirm = expanded_ct.apply(lambda row: get_primary_sensed_mode(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mapping new labels with dictionaries\n",
    "expanded_ct['Trip_purpose'] = expanded_ct['purpose_confirm'].map(dic_pur)\n",
    "expanded_ct['Mode_confirm'] = expanded_ct['mode_confirm'].map(dic_re)\n",
    "expanded_ct['Replaced_mode'] = expanded_ct['replaced_mode'].map(dic_re)\n",
    "\n",
    "# Mapping fuel\n",
    "expanded_ct['Mode_confirm_fuel'] = expanded_ct['Mode_confirm'].map(dic_fuel)\n",
    "expanded_ct['Replaced_mode_fuel'] = expanded_ct['Replaced_mode'].map(dic_fuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change meters to miles\n",
    "scaffolding.unit_conversions(expanded_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_suffix = scaffolding.get_file_suffix(year, month, program)\n",
    "quality_text = scaffolding.get_quality_text(participant_ct_df, expanded_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate energy impact\n",
    "expanded_ct = scaffolding.energy_intensity(expanded_ct, df_EI, 'distance_miles', 'Replaced_mode', 'Mode_confirm')\n",
    "expanded_ct = scaffolding.energy_impact_kWH(expanded_ct, 'distance_miles', 'Replaced_mode', 'Mode_confirm')\n",
    "expanded_ct = scaffolding.CO2_impact_lb(expanded_ct, 'distance_miles', 'Replaced_mode', 'Mode_confirm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the expanded database data to socioeconomic data\n",
    "socio_data = pd.read_csv('./replacement_modeling/Can Do Colorado eBike Program - en.csv')\n",
    "socio_data.rename(columns={'Unique User ID (auto-filled, do not edit)':'user_id',\n",
    "                          'Please identify which category represents your total household income, before taxes, for last year.':'HHINC',\n",
    "                          'How many motor vehicles are owned, leased, or available for regular use by the people who currently live in your household?':'VEH',\n",
    "                           'In which year were you born?':'AGE',\n",
    "                          'Including yourself, how many people live in your home?':'HHSIZE',\n",
    "                          'How many children under age 18 live in your home?':'CHILDREN',\n",
    "                          'What is your gender?':'GENDER',\n",
    "                          'If you were unable to use your household vehicle(s), which of the following options would be available to you to get you from place to place?':'available_modes',\n",
    "                          'Are you a student?':'STUDENT'}, inplace=True)\n",
    "socio_data = socio_data[~socio_data.user_id.isnull()]\n",
    "\n",
    "# Deal with people who have multiple responses by using most recent\n",
    "socio_data = socio_data.sort_values(by=['user_id', 'Timestamp'])\n",
    "socio_data.drop_duplicates(subset=['user_id'], keep='last', inplace=True)\n",
    "\n",
    "# Lose some trips due to people with no survey responses\n",
    "expanded_ct.user_id = expanded_ct.user_id.astype(str)\n",
    "expanded_ct.user_id = [i.replace('-','') for i in expanded_ct.user_id] # remove all dashes from strings\n",
    "expanded_ct = expanded_ct.merge(socio_data, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_ct.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add non-label category\n",
    "expanded_ct['replaced_mode'] = expanded_ct['replaced_mode'].fillna('Unlabeled')\n",
    "expanded_ct.loc[expanded_ct['replaced_mode'] == 'Unlabeled', 'Replaced_mode'] = \"Unlabeled\"\n",
    "\n",
    "# Select variables of interest from complete OpenPATH data\n",
    "data = expanded_ct[['Mode_confirm','Replaced_mode','replaced_mode','Trip_purpose','duration','distance_miles','start_local_dt_weekday','available_modes','AGE','HHINC','VEH','HHSIZE','CHILDREN','GENDER','STUDENT','user_id','start_local_dt_year','start_local_dt_month','start_local_dt_day']].copy()\n",
    "\n",
    "# Get timestamp from known year/month/day aggregated to days\n",
    "data.rename(columns={'start_local_dt_year':'year','start_local_dt_month':'month','start_local_dt_day':'day'}, inplace=True)\n",
    "data['date_time'] = pd.to_datetime(data[['year','month','day']])\n",
    "data = data.drop(columns=['year','month','day'])\n",
    "\n",
    "# Fix age\n",
    "data['AGE'] = 2022 - data['AGE']\n",
    "\n",
    "# Get number of workers\n",
    "data['WORKERS'] = data['HHSIZE'] - data['CHILDREN']\n",
    "\n",
    "# Duration in minutes\n",
    "data['duration'] = data['duration'] / 60\n",
    "\n",
    "# Filter out some responses to data\n",
    "data = data[~data['Mode_confirm'].isin(['Not a Trip','Other'])]\n",
    "# data = data[~data['Trip_purpose'].isin(['not_a_trip','Other'])]\n",
    "# data = data[~data['Replaced_mode'].isin(['Not a Trip','Other'])]\n",
    "# data = data[~data['HHINC'].isin(['Prefer not to say'])]\n",
    "# data = data[~data['VEH'].isin(['Prefer not to say / Prefiero no decir.'])]\n",
    "# Had to add the \"prefer not to say\" here otherwise I get an KeyError \"KeyError: 'Prefer not to say'\"\n",
    "# -- SHANKARI\n",
    "data = data[~data['available_modes'].isin(['None', 'Prefer not to say'])]\n",
    "\n",
    "# Combine variable categories\n",
    "data = data.replace('Car, drove alone', 'car')\n",
    "data = data.replace('Car, with others', 's_car')\n",
    "data = data.replace('Bikeshare', 's_micro')\n",
    "data = data.replace('Scooter share', 's_micro')\n",
    "data = data.replace('Regular Bike', 'p_micro')\n",
    "data = data.replace('Skate board', 'p_micro')\n",
    "data = data.replace('Train', 'transit')\n",
    "data = data.replace('Free Shuttle', 'transit')\n",
    "data = data.replace('Bus', 'transit')\n",
    "data = data.replace('Walk', 'walk')\n",
    "data = data.replace('Taxi/Uber/Lyft', 'ridehail')\n",
    "data = data.replace('Pilot ebike', 'ebike')\n",
    "\n",
    "# data = data.replace(['Home','School','Work'], 'hbw')\n",
    "# data = data.replace(['Entertainment/Social','Meal','Personal/Medical','Pick-up/Drop off','Recreation/Exercise','Religious','Shopping','Transit transfer'], 'non_hbw')\n",
    "\n",
    "# data['start_local_dt_weekday'] = data['start_local_dt_weekday'].replace(['1','2','3','4','5'],'1')\n",
    "# data['start_local_dt_weekday'] = data['start_local_dt_weekday'].replace(['0','6'],'0')\n",
    "\n",
    "# data = data.replace(['By hours ','Custodian','Fire Fighter 2 Training',\n",
    "#  'Graduate','Prefer not to say','Taking prerequisites missing for grad program ',\n",
    "#  'Yes - Full Time College/University',\n",
    "#  'Yes - Part-Time College/University',\n",
    "#  'Yes - Vocation/Technical/Trade School',\n",
    "#  'taking classes toward early childhood licensure'], 'student')\n",
    "# data = data.replace('Not a student', 'non_student')\n",
    "\n",
    "# Calculate travel times for each trip, across every mode\n",
    "def add_all_mode_tt(data, mode_col, duration_col, dist_col):\n",
    "    mode_travel_times = {}\n",
    "    for mode in pd.unique(data[mode_col]):\n",
    "\n",
    "        # Linear model for duration based on distance for trips belonging to each mode\n",
    "        mode_data = data[data[mode_col]==mode]\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(mode_data[dist_col].values.reshape(-1,1), mode_data[duration_col].values.reshape(-1,1))\n",
    "\n",
    "        # Make prediction for ALL trips\n",
    "        mode_duration_pred = regr.predict(data[dist_col].values.reshape(-1,1))\n",
    "        mode_travel_times['tt_'+mode] = mode_duration_pred\n",
    "\n",
    "    # Apply for each mode existing in the dataframe\n",
    "    for mode in mode_travel_times:\n",
    "        data[mode] = mode_travel_times[mode]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Calculate all mode travel times and add to dataframe\n",
    "data = add_all_mode_tt(data,'Mode_confirm','duration','distance_miles')\n",
    "\n",
    "# Calculate vehicle costs based roughly on $/mi from: https://www.vtpi.org/tca/tca0501.pdf\n",
    "cost_factors = {'car':0.80,\n",
    "                's_car':0.40,\n",
    "                'ridehail':3.00,\n",
    "                's_micro':1.50,\n",
    "                'transit':0.40}\n",
    "\n",
    "def add_all_mode_cost(data, cost_factors, dist_col):\n",
    "    for factor in cost_factors:\n",
    "        data['cost_'+factor] = cost_factors[factor] * data[dist_col]\n",
    "    return data\n",
    "\n",
    "# Calculate all mode travel costs and add to dataframe\n",
    "add_all_mode_cost(data, cost_factors, 'distance_miles')\n",
    "\n",
    "# Labels for modes in the availability survey\n",
    "availability_codes = {'Public transportation (bus, subway, light rail, etc.)':'transit',\n",
    "                    'Get a ride from a friend or family member':'s_car',\n",
    "                    'Rental car (including Zipcar/ Car2Go)':'car',\n",
    "                    'Taxi (regular taxi, Uber, Lyft, etc)':'ridehail',\n",
    "                    'Bicycle':'p_micro',\n",
    "                    'Shared bicycle or scooter':'s_micro',\n",
    "                    'Walk/roll':'walk',\n",
    "                    'Skateboard':'p_micro',\n",
    "                    'ebike':'ebike'}\n",
    "\n",
    "# Create columns for available modes under each trip\n",
    "def add_mode_availability(data, availability_codes, availability_col, choice_col, replaced_col):\n",
    "    mode_list = np.unique(list(availability_codes.values()))\n",
    "    available_list = data[availability_col].values\n",
    "    choice_list = data[choice_col].values\n",
    "    replaced_list = data[replaced_col].values\n",
    "    for mode in mode_list:\n",
    "        mode_avail = []\n",
    "        i=0\n",
    "        for available in available_list:\n",
    "            if 'None' in available:\n",
    "                mode_avail.append(1)\n",
    "                i+=1\n",
    "                continue\n",
    "            options = [availability_codes[x] for x in available.split(';')]\n",
    "            # Chosen mode must be in the available modes list, if mode was chosen it is assumed available\n",
    "            if include_replaced_modes_as_valid:\n",
    "                mode_check = lambda mode: mode in options or mode==choice_list[i] or mode==replaced_list[i]\n",
    "            else:\n",
    "                mode_check = lambda mode: mode in options or mode==choice_list[i]\n",
    "            if mode_check(mode):\n",
    "                mode_avail.append(1)\n",
    "            else:\n",
    "                mode_avail.append(0)\n",
    "            i+=1\n",
    "        data['av_'+mode] = mode_avail\n",
    "\n",
    "    return data\n",
    "\n",
    "# Add availability variables to data\n",
    "data = add_mode_availability(data, availability_codes, 'available_modes', 'Mode_confirm', 'Replaced_mode')\n",
    "\n",
    "# Handle all variables that are ordinal; otherwise they may not end up in correct order\n",
    "# data.HHINC = pd.Categorical(data.HHINC,\n",
    "#                             ordered=True,\n",
    "#                             categories=['Less than $24,999',\n",
    "#                                        '$25,000-$49,999',\n",
    "#                                        '$50,000-$99,999',\n",
    "#                                        '$100,000 -$149,999',\n",
    "#                                        '$150,000-$199,999',\n",
    "#                                        '$200,000 or more'])\n",
    "# data.VEH = pd.Categorical(data.VEH,\n",
    "#                             ordered=True,\n",
    "#                             categories=['0',\n",
    "#                                        '1',\n",
    "#                                        '2',\n",
    "#                                        '3',\n",
    "#                                        '4+'])\n",
    "\n",
    "# Make sure that the confirmed and replaced modes align after being converted to numeric variables\n",
    "data.Mode_confirm = pd.Categorical(data.Mode_confirm,\n",
    "                            ordered=True,\n",
    "                            categories=['car',\n",
    "                                        's_car',\n",
    "                                        'ridehail',\n",
    "                                        'transit',\n",
    "                                        'p_micro',\n",
    "                                        's_micro',\n",
    "                                        'walk',\n",
    "                                        'ebike'])\n",
    "data.Replaced_mode = pd.Categorical(data.Replaced_mode,\n",
    "                            ordered=True,\n",
    "                            categories=['car',\n",
    "                                        's_car',\n",
    "                                        'ridehail',\n",
    "                                        'transit',\n",
    "                                        'p_micro',\n",
    "                                        's_micro',\n",
    "                                        'walk',\n",
    "                                        'ebike',\n",
    "                                        'Other',\n",
    "                                        'No Travel',\n",
    "                                        'Unlabeled'])\n",
    "\n",
    "# Convert categorical variables to numeric\n",
    "cat_columns = data.select_dtypes(['object','category']).columns\n",
    "all_categories = []\n",
    "for i in range(0,len(cat_columns)):\n",
    "    # Keep a record of what order the categories are in when converted\n",
    "    var_categories = data[cat_columns].astype('category').iloc[:,i].cat.categories\n",
    "    all_categories.append(var_categories)\n",
    "data[cat_columns] = data[cat_columns].apply(lambda x: x.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show listed categories in their order\n",
    "print(cat_columns)\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All replaced trips\n",
    "df_replaced_trips = data[~data['Replaced_mode'].isin([8,9,10])].copy()\n",
    "\n",
    "# Only non-ebike\n",
    "df_non_ebike = data[~data['Mode_confirm'].isin([7])].copy()\n",
    "\n",
    "# Only ebike, labeled trips\n",
    "df_ebike = data[data['Mode_confirm'].isin([7])].copy()\n",
    "df_ebike = df_ebike[~df_ebike['Replaced_mode'].isin([7,8,9,10])]\n",
    "\n",
    "# Only ebike, unlabeled trips\n",
    "df_ebike_unlabeled = data[data['Mode_confirm'].isin([7])].copy()\n",
    "df_ebike_unlabeled = df_ebike_unlabeled[df_ebike_unlabeled['Replaced_mode'].isin([10])]\n",
    "\n",
    "# Only ebike, labeled and unlabeled trips\n",
    "df_ebike_to_label = data[data['Mode_confirm'].isin([7])].copy()\n",
    "df_ebike_to_label = df_ebike_to_label[~df_ebike_to_label['Replaced_mode'].isin([7,8,9])]\n",
    "\n",
    "# Only ebike, only new trips\n",
    "df_ebike_new_travel = data[data['Mode_confirm'].isin([7])].copy()\n",
    "df_ebike_new_travel = df_ebike_new_travel[df_ebike_new_travel['Replaced_mode'].isin([9])]\n",
    "\n",
    "# Set up K-fold cross validation\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data stats\n",
    "print(f\"Trips: {len(data)}\")\n",
    "print(f\"Users: {len(np.unique(data.user_id))}\")\n",
    "print(f\"Trips per user: {len(data) / len(pd.unique(data.user_id))}\")\n",
    "print(f\"New activity: {len(df_ebike_new_travel) / len(df_ebike)}\")\n",
    "print(f\"Unlabeled all trips: {len(data[data.Replaced_mode==10]) / len(data)}\")\n",
    "print(f\"Unlabeled ebike trips (we model these): {len(df_ebike_unlabeled) / len(df_ebike)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['tt_car','tt_s_car','tt_walk','tt_p_micro','tt_transit','tt_s_micro','tt_ridehail',\n",
    "             'cost_car','cost_s_car','cost_ridehail','cost_s_micro','cost_transit',\n",
    "             'av_car','av_s_car','av_walk','av_p_micro','av_transit','av_s_micro','av_ridehail']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test on Primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test random forest on the primary mode\n",
    "accuracy = []\n",
    "f1 = []\n",
    "confusion = []\n",
    "for train_index, test_index in kf.split(df_non_ebike.values):\n",
    "    X_train, X_test = df_non_ebike[feature_list].values[train_index], df_non_ebike[feature_list].values[test_index]\n",
    "    y_train, y_test = df_non_ebike['Mode_confirm'].values[train_index], df_non_ebike['Mode_confirm'].values[test_index]\n",
    "\n",
    "    # Train random forest on non-ebike trip training set\n",
    "    rf = RandomForestClassifier(n_estimators=50)\n",
    "    rf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy.append(sum(y_pred==y_test) / len(y_test))\n",
    "    f1.append(sklearn.metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "    confusion.append(sklearn.metrics.confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4,5,6], normalize='pred'))\n",
    "\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "\n",
    "# Use the parameters from this model for final labeling\n",
    "rf_keep = rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, xticklabels=all_categories[0].values[:-1], yticklabels=all_categories[0].values[:-1], cbar=False).set(title='Random Forest Confusion Matrix (Primary)', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Primary Test on Replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test random forest on the replaced mode\n",
    "accuracy = []\n",
    "f1 = []\n",
    "confusion = []\n",
    "for train_index, test_index in kf.split(df_non_ebike.values):\n",
    "    X_train, X_test = df_non_ebike[feature_list].values[train_index], df_non_ebike[feature_list].values[test_index]\n",
    "    y_train, y_test = df_non_ebike['Mode_confirm'].values[train_index], df_non_ebike['Mode_confirm'].values[test_index]\n",
    "\n",
    "    X_test = df_ebike[feature_list].values\n",
    "    y_test = df_ebike['Replaced_mode'].values\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy.append(sum(y_pred==y_test) / len(y_test))\n",
    "    f1.append(sklearn.metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "    confusion.append(sklearn.metrics.confusion_matrix(y_test, y_pred, labels=[0,1,2,3,4,5,6], normalize='pred'))\n",
    "\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, xticklabels=all_categories[0].values[:-1], yticklabels=all_categories[0].values[:-1], cbar=False).set(title='Random Forest Confusion Matrix (Replaced)', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Replaced Test on Replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AND Test random forest on the replaced mode\n",
    "accuracy = []\n",
    "f1 = []\n",
    "for train_index, test_index in kf.split(df_ebike.values):\n",
    "    X_train, X_test = df_ebike[feature_list].values[train_index], df_ebike[feature_list].values[test_index]\n",
    "    y_train, y_test = df_ebike['Replaced_mode'].values[train_index], df_ebike['Replaced_mode'].values[test_index]\n",
    "\n",
    "    # Train random forest on non-ebike trip training set\n",
    "    rf = RandomForestClassifier(n_estimators=50)\n",
    "    rf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy.append(sum(y_pred==y_test) / len(y_test))\n",
    "    f1.append(sklearn.metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNL Choice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The estimation results can be read from the pickle file instead f desired\n",
    "# results = res.bioResults(pickleFile='openpath_mxl~05.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mixed logit model in Biogeme\n",
    "# # Best so far: openpath_mxl~05\n",
    "\n",
    "# # Alternative specific constants\n",
    "# ASC_CAR = Beta('ASC_CAR',0,None,None,1)\n",
    "# ASC_EBIKE = Beta('ASC_EBIKE',0,None,None,0)\n",
    "# ASC_P_MICRO = Beta('ASC_P_MICRO',0,None,None,0)\n",
    "# ASC_RIDEHAIL = Beta('ASC_RIDEHAIL',0,None,None,0)\n",
    "# ASC_S_MICRO = Beta('ASC_S_MICRO',0,None,None,0)\n",
    "# ASC_TRANSIT = Beta('ASC_TRANSIT',0,None,None,0)\n",
    "# ASC_WALK = Beta('ASC_WALK',0,None,None,0)\n",
    "\n",
    "# # Define a random parameter, normally distributed, designed to be used\n",
    "# # for Monte-Carlo simulation\n",
    "# B_TIME = Beta('B_TIME', 0, None, None, 0)\n",
    "# B_COST = Beta('B_COST', 0, None, None, 0)\n",
    "\n",
    "# # Alternative specific variables\n",
    "# B_HHINC = Beta('B_HHINC', 0, None, None, 0)\n",
    "\n",
    "# # It is advised not to use 0 as starting value for the following parameter.\n",
    "# B_TIME_S = Beta('B_TIME_S', 1, None, None, 0)\n",
    "# B_TIME_RND = B_TIME + B_TIME_S * bioDraws('B_TIME_RND', 'NORMAL')\n",
    "\n",
    "# # Utility functions\n",
    "# V0 = ASC_CAR + \\\n",
    "# B_TIME_RND * tt_car + \\\n",
    "# B_COST * cost_car\n",
    "\n",
    "# V1 = ASC_P_MICRO + \\\n",
    "# B_TIME_RND * tt_p_micro\n",
    "\n",
    "# V2 = ASC_RIDEHAIL + \\\n",
    "# B_TIME_RND * tt_ridehail + \\\n",
    "# B_COST * cost_ridehail\n",
    "\n",
    "# V3 = ASC_S_MICRO + \\\n",
    "# B_TIME_RND * tt_s_micro + \\\n",
    "# B_COST * cost_s_micro\n",
    "\n",
    "# V4 = ASC_TRANSIT + \\\n",
    "# B_TIME_RND * tt_transit + \\\n",
    "# B_COST * cost_transit\n",
    "\n",
    "# V5 = ASC_WALK + \\\n",
    "# B_TIME_RND * tt_walk\n",
    "\n",
    "# # Map modes to utility functions\n",
    "# V = {0: V0,\n",
    "#     1: V1,\n",
    "#     2: V2,\n",
    "#     3: V3,\n",
    "#     4: V4,\n",
    "#     5: V5}\n",
    "\n",
    "# # Mode availability\n",
    "# av = {0: av_car,\n",
    "#     1: av_p_micro,\n",
    "#     2: av_ridehail,\n",
    "#     3: av_s_micro,\n",
    "#     4: av_transit,\n",
    "#     5: av_walk}\n",
    "\n",
    "# # Conditional to B_TIME_RND, we have a logit model (called the kernel)\n",
    "# prob = models.logit(V, av, Mode_confirm)\n",
    "\n",
    "# # We integrate over B_TIME_RND using Monte-Carlo\n",
    "# logprob = log(MonteCarlo(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test on Primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MNL on primary mode\n",
    "accuracy = []\n",
    "f1 = []\n",
    "confusion = []\n",
    "for train_index, test_index in kf.split(df_non_ebike.values):\n",
    "    X_train, X_test = df_non_ebike.iloc[train_index], df_non_ebike.iloc[test_index]\n",
    "    y_train, y_test = df_non_ebike.iloc[train_index]['Mode_confirm'].values, df_non_ebike.iloc[test_index]['Mode_confirm'].values\n",
    "\n",
    "    # Put the variables in global namespace to make Biogeme happy\n",
    "    df_train = X_train.drop(columns=['date_time'])\n",
    "    database_train = db.Database('openpath_train', df_train)\n",
    "    globals().update(database_train.variables)\n",
    "    \n",
    "    df_test = X_test.drop(columns=['date_time'])\n",
    "    database_test = db.Database('openpath_test', df_test)\n",
    "    globals().update(database_test.variables)\n",
    "\n",
    "    # Multinomial logit model in Biogeme\n",
    "    # Alternative specific constants\n",
    "    ASC_CAR = Beta('ASC_CAR',0,None,None,1)\n",
    "    ASC_S_CAR = Beta('ASC_S_CAR',0,None,None,0)\n",
    "    ASC_RIDEHAIL = Beta('ASC_RIDEHAIL',0,None,None,0)\n",
    "    ASC_TRANSIT = Beta('ASC_TRANSIT',0,None,None,0)\n",
    "    ASC_P_MICRO = Beta('ASC_P_MICRO',0,None,None,0)\n",
    "    ASC_S_MICRO = Beta('ASC_S_MICRO',0,None,None,0)\n",
    "    ASC_WALK = Beta('ASC_WALK',0,None,None,0)\n",
    "    ASC_EBIKE = Beta('ASC_EBIKE',0,None,None,0)\n",
    "\n",
    "    # Trip parameters\n",
    "    B_COST = Beta('B_COST',0,None,None,0)\n",
    "    B_TT = Beta('B_TT',0,None,None,0)\n",
    "\n",
    "    # Mode parameters\n",
    "    B_ASV_TT_MOTOR = Beta('B_ASV_TT_MOTOR',0,None,None,0)\n",
    "    B_ASV_TT_PHYS = Beta('B_ASV_TT_PHYS',0,None,None,0)\n",
    "\n",
    "    # Utility functions\n",
    "    V0 = ASC_CAR + \\\n",
    "    B_COST * cost_car + \\\n",
    "    B_ASV_TT_MOTOR * tt_car\n",
    "\n",
    "    V1 = ASC_S_CAR + \\\n",
    "    B_COST * cost_s_car + \\\n",
    "    B_ASV_TT_MOTOR * tt_s_car\n",
    "\n",
    "    V2 = ASC_RIDEHAIL + \\\n",
    "    B_COST * cost_ridehail + \\\n",
    "    B_ASV_TT_MOTOR * tt_ridehail\n",
    "\n",
    "    V3 = ASC_TRANSIT + \\\n",
    "    B_COST * cost_transit + \\\n",
    "    B_ASV_TT_MOTOR * tt_transit\n",
    "\n",
    "    V4 = ASC_P_MICRO + \\\n",
    "    B_ASV_TT_PHYS * tt_p_micro\n",
    "\n",
    "    V5 = ASC_S_MICRO + \\\n",
    "    B_COST * cost_s_micro + \\\n",
    "    B_ASV_TT_PHYS * tt_s_micro\n",
    "\n",
    "    V6 = ASC_WALK + \\\n",
    "    B_ASV_TT_PHYS * tt_walk\n",
    "\n",
    "    # Map modes to utility functions\n",
    "    V = {0: V0,\n",
    "         1: V1,\n",
    "         2: V2,\n",
    "         3: V3,\n",
    "         4: V4,\n",
    "         5: V5,\n",
    "         6: V6}\n",
    "\n",
    "    # Mode availability\n",
    "    av = {0: av_car,\n",
    "          1: av_s_car,\n",
    "          2: av_ridehail,\n",
    "          3: av_transit,\n",
    "          4: av_p_micro,\n",
    "          5: av_s_micro,\n",
    "          6: av_walk}\n",
    "    \n",
    "    # Train the model parameters\n",
    "    logprob = models.loglogit(V, av, Mode_confirm)\n",
    "    biogeme = bio.BIOGEME(database_train, logprob)\n",
    "    biogeme.modelName = 'openpath_mnl_train'\n",
    "    biogeme.generateHtml = False\n",
    "    biogeme.generatePickle = False\n",
    "    results = biogeme.estimate()\n",
    "    \n",
    "    # Assemble utility functions for testing modes\n",
    "    prob_car = models.logit(V, av, 0)\n",
    "    prob_s_car = models.logit(V, av, 1)\n",
    "    prob_ridehail = models.logit(V, av, 2)\n",
    "    prob_transit = models.logit(V, av, 3)\n",
    "    prob_p_micro = models.logit(V, av, 4)\n",
    "    prob_s_micro = models.logit(V, av, 5)\n",
    "    prob_walk = models.logit(V, av, 6)\n",
    "\n",
    "    simulate ={'Prob. car': prob_car,\n",
    "               'Prob. s_car': prob_s_car,\n",
    "               'Prob. ridehail': prob_ridehail,\n",
    "               'Prob. transit': prob_transit,\n",
    "               'Prob. p_micro': prob_p_micro,\n",
    "               'Prob. s_micro': prob_s_micro,\n",
    "               'Prob. walk': prob_walk}\n",
    "\n",
    "    betas = results.getBetaValues()\n",
    "\n",
    "    # Calculate utility values for each row in the test database\n",
    "    biogeme = bio.BIOGEME(database_test, simulate)\n",
    "    biogeme.modelName = 'openpath_mnl_test'\n",
    "    simulatedValues = biogeme.simulate(betas)\n",
    "\n",
    "    # Test predicting maximum mode utility as choice\n",
    "    # Identify the column of highest probability, replace with number corresponding to the mode\n",
    "    prob_max = simulatedValues.idxmax(axis=1)\n",
    "    prob_max = prob_max.replace({'Prob. car': 0,\n",
    "                                 'Prob. s_car': 1,\n",
    "                                 'Prob. ridehail': 2,\n",
    "                                 'Prob. transit': 3,\n",
    "                                 'Prob. p_micro': 4,\n",
    "                                 'Prob. s_micro': 5,\n",
    "                                 'Prob. walk': 6})\n",
    "    data_res = {'y_Actual':df_test['Mode_confirm'], 'y_Predicted': prob_max}\n",
    "    \n",
    "#     # Test predicting car every time\n",
    "#     data_res['y_Predicted_Car'] = np.repeat(0,len(data_res['y_Actual']))\n",
    "\n",
    "#     # Test predicting probabilistically\n",
    "#     def probabilistic_mode_choice(probs):\n",
    "#         return np.random.choice(np.arange(0,len(probs)), p=probs)\n",
    "#     data_res['y_Predicted_Prob'] = np.apply_along_axis(probabilistic_mode_choice, axis=1, arr=simulatedValues.values)\n",
    "    \n",
    "    # Cross tabulate to see accuracy for each mode\n",
    "    df = pd.DataFrame(data_res, columns=['y_Actual','y_Predicted'])\n",
    "#     confusion_matrix = pd.crosstab(df['y_Actual'],df['y_Predicted'],rownames=['Actual'],colnames=['Predicted'],normalize=True)\n",
    "#     print(round(confusion_matrix,2))\n",
    "    accuracy.append(len(df[df['y_Actual']==df['y_Predicted']])/len(df))\n",
    "    f1.append(sklearn.metrics.f1_score(df['y_Actual'], df['y_Predicted'], average='weighted'))\n",
    "    confusion.append(sklearn.metrics.confusion_matrix(df['y_Actual'], df['y_Predicted'], labels=[0,1,2,3,4,5,6], normalize='pred'))\n",
    "\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "print(sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, xticklabels=all_categories[0].values[:-1], yticklabels=all_categories[0].values[:-1], cbar=False))\n",
    "\n",
    "# Use the parameters from this model for final labeling\n",
    "keep_betas = betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, xticklabels=all_categories[0].values[:-1], yticklabels=all_categories[0].values[:-1], cbar=False).set(title='MNL Confusion Matrix (Primary)', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Primary Test on Replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test MNL on primary mode\n",
    "accuracy = []\n",
    "f1 = []\n",
    "confusion = []\n",
    "for train_index, test_index in kf.split(df_non_ebike.values):\n",
    "    X_train, X_test = df_non_ebike.iloc[train_index], df_non_ebike.iloc[test_index]\n",
    "    y_train, y_test = df_non_ebike.iloc[train_index]['Mode_confirm'].values, df_non_ebike.iloc[test_index]['Mode_confirm'].values\n",
    "\n",
    "    # Put the variables in global namespace to make Biogeme happy\n",
    "    df_train = X_train.drop(columns=['date_time'])\n",
    "    database_train = db.Database('openpath_train', df_train)\n",
    "    globals().update(database_train.variables)\n",
    "    \n",
    "    # Point to the ebike trips dataframe\n",
    "    df_test = df_ebike.drop(columns=['date_time'])\n",
    "    database_test = db.Database('openpath_test', df_test)\n",
    "    globals().update(database_test.variables)\n",
    "\n",
    "    # Multinomial logit model in Biogeme\n",
    "    # Alternative specific constants\n",
    "    ASC_CAR = Beta('ASC_CAR',0,None,None,1)\n",
    "    ASC_S_CAR = Beta('ASC_S_CAR',0,None,None,0)\n",
    "    ASC_RIDEHAIL = Beta('ASC_RIDEHAIL',0,None,None,0)\n",
    "    ASC_TRANSIT = Beta('ASC_TRANSIT',0,None,None,0)\n",
    "    ASC_P_MICRO = Beta('ASC_P_MICRO',0,None,None,0)\n",
    "    ASC_S_MICRO = Beta('ASC_S_MICRO',0,None,None,0)\n",
    "    ASC_WALK = Beta('ASC_WALK',0,None,None,0)\n",
    "    ASC_EBIKE = Beta('ASC_EBIKE',0,None,None,0)\n",
    "\n",
    "    # Trip parameters\n",
    "    B_COST = Beta('B_COST',0,None,None,0)\n",
    "    B_TT = Beta('B_TT',0,None,None,0)\n",
    "\n",
    "    # Mode parameters\n",
    "    B_ASV_TT_MOTOR = Beta('B_ASV_TT_MOTOR',0,None,None,0)\n",
    "    B_ASV_TT_PHYS = Beta('B_ASV_TT_PHYS',0,None,None,0)\n",
    "\n",
    "    # Utility functions\n",
    "    V0 = ASC_CAR + \\\n",
    "    B_COST * cost_car + \\\n",
    "    B_ASV_TT_MOTOR * tt_car\n",
    "\n",
    "    V1 = ASC_S_CAR + \\\n",
    "    B_COST * cost_s_car + \\\n",
    "    B_ASV_TT_MOTOR * tt_s_car\n",
    "\n",
    "    V2 = ASC_RIDEHAIL + \\\n",
    "    B_COST * cost_ridehail + \\\n",
    "    B_ASV_TT_MOTOR * tt_ridehail\n",
    "\n",
    "    V3 = ASC_TRANSIT + \\\n",
    "    B_COST * cost_transit + \\\n",
    "    B_ASV_TT_MOTOR * tt_transit\n",
    "\n",
    "    V4 = ASC_P_MICRO + \\\n",
    "    B_ASV_TT_PHYS * tt_p_micro\n",
    "\n",
    "    V5 = ASC_S_MICRO + \\\n",
    "    B_COST * cost_s_micro + \\\n",
    "    B_ASV_TT_PHYS * tt_s_micro\n",
    "\n",
    "    V6 = ASC_WALK + \\\n",
    "    B_ASV_TT_PHYS * tt_walk\n",
    "\n",
    "    # Map modes to utility functions\n",
    "    V = {0: V0,\n",
    "         1: V1,\n",
    "         2: V2,\n",
    "         3: V3,\n",
    "         4: V4,\n",
    "         5: V5,\n",
    "         6: V6}\n",
    "\n",
    "    # Mode availability\n",
    "    av = {0: av_car,\n",
    "          1: av_s_car,\n",
    "          2: av_ridehail,\n",
    "          3: av_transit,\n",
    "          4: av_p_micro,\n",
    "          5: av_s_micro,\n",
    "          6: av_walk}\n",
    "    \n",
    "    # Train the model parameters\n",
    "    logprob = models.loglogit(V, av, Mode_confirm)\n",
    "    biogeme = bio.BIOGEME(database_train, logprob)\n",
    "    biogeme.modelName = 'openpath_mnl_train'\n",
    "    biogeme.generateHtml = False\n",
    "    biogeme.generatePickle = False\n",
    "    results = biogeme.estimate()\n",
    "    \n",
    "    # Assemble utility functions for testing modes\n",
    "    prob_car = models.logit(V, av, 0)\n",
    "    prob_s_car = models.logit(V, av, 1)\n",
    "    prob_ridehail = models.logit(V, av, 2)\n",
    "    prob_transit = models.logit(V, av, 3)\n",
    "    prob_p_micro = models.logit(V, av, 4)\n",
    "    prob_s_micro = models.logit(V, av, 5)\n",
    "    prob_walk = models.logit(V, av, 6)\n",
    "\n",
    "    simulate ={'Prob. car': prob_car,\n",
    "               'Prob. s_car': prob_s_car,\n",
    "               'Prob. ridehail': prob_ridehail,\n",
    "               'Prob. transit': prob_transit,\n",
    "               'Prob. p_micro': prob_p_micro,\n",
    "               'Prob. s_micro': prob_s_micro,\n",
    "               'Prob. walk': prob_walk}\n",
    "\n",
    "    betas = results.getBetaValues()\n",
    "\n",
    "    # Calculate utility values for each row in the test database\n",
    "    biogeme = bio.BIOGEME(database_test, simulate)\n",
    "    biogeme.modelName = 'openpath_mnl_test'\n",
    "    simulatedValues = biogeme.simulate(betas)\n",
    "\n",
    "    # Test predicting maximum mode utility as choice\n",
    "    # Identify the column of highest probability, replace with number corresponding to the mode\n",
    "    prob_max = simulatedValues.idxmax(axis=1)\n",
    "    prob_max = prob_max.replace({'Prob. car': 0,\n",
    "                                 'Prob. s_car': 1,\n",
    "                                 'Prob. ridehail': 2,\n",
    "                                 'Prob. transit': 3,\n",
    "                                 'Prob. p_micro': 4,\n",
    "                                 'Prob. s_micro': 5,\n",
    "                                 'Prob. walk': 6})\n",
    "    data_res = {'y_Actual':df_test['Replaced_mode'], 'y_Predicted': prob_max}\n",
    "    \n",
    "#     # Test predicting car every time\n",
    "#     data_res['y_Predicted_Car'] = np.repeat(0,len(data_res['y_Actual']))\n",
    "\n",
    "#     # Test predicting probabilistically\n",
    "#     def probabilistic_mode_choice(probs):\n",
    "#         return np.random.choice(np.arange(0,len(probs)), p=probs)\n",
    "#     data_res['y_Predicted_Prob'] = np.apply_along_axis(probabilistic_mode_choice, axis=1, arr=simulatedValues.values)\n",
    "    \n",
    "    # Cross tabulate to see accuracy for each mode\n",
    "    df = pd.DataFrame(data_res, columns=['y_Actual','y_Predicted'])\n",
    "#     confusion_matrix = pd.crosstab(df['y_Actual'],df['y_Predicted'],rownames=['Actual'],colnames=['Predicted'],normalize=True)\n",
    "#     print(round(confusion_matrix,2))\n",
    "    accuracy.append(len(df[df['y_Actual']==df['y_Predicted']])/len(df))\n",
    "    f1.append(sklearn.metrics.f1_score(df['y_Actual'], df['y_Predicted'], average='weighted'))\n",
    "    confusion.append(sklearn.metrics.confusion_matrix(df['y_Actual'], df['y_Predicted'], labels=[0,1,2,3,4,5,6], normalize='pred'))\n",
    "\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, xticklabels=all_categories[0].values[:-1], yticklabels=all_categories[0].values[:-1], cbar=False).set(title='MNL Confusion Matrix (Replaced)', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Replaced Test on Replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MNL on primary mode\n",
    "accuracy = []\n",
    "f1 = []\n",
    "for train_index, test_index in kf.split(df_ebike.values):\n",
    "    X_train, X_test = df_ebike.iloc[train_index], df_ebike.iloc[test_index]\n",
    "    y_train, y_test = df_ebike.iloc[train_index]['Replaced_mode'].values, df_ebike.iloc[test_index]['Replaced_mode'].values\n",
    "\n",
    "    # Put the variables in global namespace to make Biogeme happy\n",
    "    df_train = X_train.drop(columns=['date_time'])\n",
    "    database_train = db.Database('openpath_train', df_train)\n",
    "    globals().update(database_train.variables)\n",
    "    \n",
    "    # Point to the ebike trips dataframe\n",
    "    df_test = df_ebike.drop(columns=['date_time'])\n",
    "    database_test = db.Database('openpath_test', df_test)\n",
    "    globals().update(database_test.variables)\n",
    "\n",
    "    # Multinomial logit model in Biogeme\n",
    "    # Alternative specific constants\n",
    "    ASC_CAR = Beta('ASC_CAR',0,None,None,1)\n",
    "    ASC_S_CAR = Beta('ASC_S_CAR',0,None,None,0)\n",
    "    ASC_RIDEHAIL = Beta('ASC_RIDEHAIL',0,None,None,0)\n",
    "    ASC_TRANSIT = Beta('ASC_TRANSIT',0,None,None,0)\n",
    "    ASC_P_MICRO = Beta('ASC_P_MICRO',0,None,None,0)\n",
    "    ASC_S_MICRO = Beta('ASC_S_MICRO',0,None,None,0)\n",
    "    ASC_WALK = Beta('ASC_WALK',0,None,None,0)\n",
    "    ASC_EBIKE = Beta('ASC_EBIKE',0,None,None,0)\n",
    "\n",
    "    # Trip parameters\n",
    "    B_COST = Beta('B_COST',0,None,None,0)\n",
    "    B_TT = Beta('B_TT',0,None,None,0)\n",
    "\n",
    "    # Mode parameters\n",
    "    B_ASV_TT_MOTOR = Beta('B_ASV_TT_MOTOR',0,None,None,0)\n",
    "    B_ASV_TT_PHYS = Beta('B_ASV_TT_PHYS',0,None,None,0)\n",
    "\n",
    "    # Utility functions\n",
    "    V0 = ASC_CAR + \\\n",
    "    B_COST * cost_car + \\\n",
    "    B_ASV_TT_MOTOR * tt_car\n",
    "\n",
    "    V1 = ASC_S_CAR + \\\n",
    "    B_COST * cost_s_car + \\\n",
    "    B_ASV_TT_MOTOR * tt_s_car\n",
    "\n",
    "    V2 = ASC_RIDEHAIL + \\\n",
    "    B_COST * cost_ridehail + \\\n",
    "    B_ASV_TT_MOTOR * tt_ridehail\n",
    "\n",
    "    V3 = ASC_TRANSIT + \\\n",
    "    B_COST * cost_transit + \\\n",
    "    B_ASV_TT_MOTOR * tt_transit\n",
    "\n",
    "    V4 = ASC_P_MICRO + \\\n",
    "    B_ASV_TT_PHYS * tt_p_micro\n",
    "\n",
    "    V5 = ASC_S_MICRO + \\\n",
    "    B_COST * cost_s_micro + \\\n",
    "    B_ASV_TT_PHYS * tt_s_micro\n",
    "\n",
    "    V6 = ASC_WALK + \\\n",
    "    B_ASV_TT_PHYS * tt_walk\n",
    "\n",
    "    # Map modes to utility functions\n",
    "    V = {0: V0,\n",
    "         1: V1,\n",
    "         2: V2,\n",
    "         3: V3,\n",
    "         4: V4,\n",
    "         5: V5,\n",
    "         6: V6}\n",
    "\n",
    "    # Mode availability\n",
    "    av = {0: av_car,\n",
    "          1: av_s_car,\n",
    "          2: av_ridehail,\n",
    "          3: av_transit,\n",
    "          4: av_p_micro,\n",
    "          5: av_s_micro,\n",
    "          6: av_walk}\n",
    "    \n",
    "    # Train the model parameters\n",
    "    logprob = models.loglogit(V, av, Replaced_mode)\n",
    "    biogeme = bio.BIOGEME(database_train, logprob)\n",
    "    biogeme.modelName = 'openpath_mnl_train'\n",
    "    biogeme.generateHtml = False\n",
    "    biogeme.generatePickle = False\n",
    "    results = biogeme.estimate()\n",
    "    \n",
    "    # Assemble utility functions for testing modes\n",
    "    prob_car = models.logit(V, av, 0)\n",
    "    prob_s_car = models.logit(V, av, 1)\n",
    "    prob_ridehail = models.logit(V, av, 2)\n",
    "    prob_transit = models.logit(V, av, 3)\n",
    "    prob_p_micro = models.logit(V, av, 4)\n",
    "    prob_s_micro = models.logit(V, av, 5)\n",
    "    prob_walk = models.logit(V, av, 6)\n",
    "\n",
    "    simulate ={'Prob. car': prob_car,\n",
    "               'Prob. s_car': prob_s_car,\n",
    "               'Prob. ridehail': prob_ridehail,\n",
    "               'Prob. transit': prob_transit,\n",
    "               'Prob. p_micro': prob_p_micro,\n",
    "               'Prob. s_micro': prob_s_micro,\n",
    "               'Prob. walk': prob_walk}\n",
    "\n",
    "    betas = results.getBetaValues()\n",
    "\n",
    "    # Calculate utility values for each row in the test database\n",
    "    biogeme = bio.BIOGEME(database_test, simulate)\n",
    "    biogeme.modelName = 'openpath_mnl_test'\n",
    "    simulatedValues = biogeme.simulate(betas)\n",
    "\n",
    "    # Test predicting maximum mode utility as choice\n",
    "    # Identify the column of highest probability, replace with number corresponding to the mode\n",
    "    prob_max = simulatedValues.idxmax(axis=1)\n",
    "    prob_max = prob_max.replace({'Prob. car': 0,\n",
    "                                 'Prob. s_car': 1,\n",
    "                                 'Prob. ridehail': 2,\n",
    "                                 'Prob. transit': 3,\n",
    "                                 'Prob. p_micro': 4,\n",
    "                                 'Prob. s_micro': 5,\n",
    "                                 'Prob. walk': 6})\n",
    "    data_res = {'y_Actual':df_test['Replaced_mode'], 'y_Predicted': prob_max}\n",
    "    \n",
    "#     # Test predicting car every time\n",
    "#     data_res['y_Predicted_Car'] = np.repeat(0,len(data_res['y_Actual']))\n",
    "\n",
    "#     # Test predicting probabilistically\n",
    "#     def probabilistic_mode_choice(probs):\n",
    "#         return np.random.choice(np.arange(0,len(probs)), p=probs)\n",
    "#     data_res['y_Predicted_Prob'] = np.apply_along_axis(probabilistic_mode_choice, axis=1, arr=simulatedValues.values)\n",
    "    \n",
    "    # Cross tabulate to see accuracy for each mode\n",
    "    df = pd.DataFrame(data_res, columns=['y_Actual','y_Predicted'])\n",
    "#     confusion_matrix = pd.crosstab(df['y_Actual'],df['y_Predicted'],rownames=['Actual'],colnames=['Predicted'],normalize=True)\n",
    "#     print(round(confusion_matrix,2))\n",
    "    accuracy.append(len(df[df['y_Actual']==df['y_Predicted']])/len(df))\n",
    "    f1.append(sklearn.metrics.f1_score(df['y_Actual'], df['y_Predicted'], average='weighted'))\n",
    "\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ebike Substitution Rates and Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_ebike.copy()\n",
    "\n",
    "# Relabel with original mode names for plotting\n",
    "for mode in av:\n",
    "    mode_text = '_'.join(str(av[mode]).split('_')[1:])\n",
    "    df_analysis['Replaced_mode'] = df_analysis['Replaced_mode'].replace(mode,mode_text)\n",
    "    \n",
    "# Substitution rate of ebike trips not including new trips\n",
    "plot_data = df_analysis.groupby(['Replaced_mode']).count()[['Mode_confirm']].reset_index()\n",
    "plot_data['subst_rate'] = plot_data['Mode_confirm'] / sum(plot_data['Mode_confirm'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,5))\n",
    "sns.barplot(ax=ax, data=plot_data, x='Replaced_mode', y='subst_rate').set(title='Ebike Mode Replacement', xlabel='Replaced Mode', ylabel='Substitution Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "print(plot_data['subst_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now label the replaced mode\n",
    "y_pred = rf_keep.predict(df_ebike_to_label[feature_list].values)\n",
    "\n",
    "for i in range(0, len(y_pred)):\n",
    "    if df_ebike_to_label.iloc[i]['Replaced_mode']!=10:\n",
    "        y_pred[i] = df_ebike_to_label.iloc[i].Replaced_mode\n",
    "        \n",
    "df_ebike_to_label = df_ebike_to_label.copy()\n",
    "df_ebike_to_label['Replaced_mode_new'] = y_pred\n",
    "\n",
    "# Generate figure again\n",
    "# Relabel with original mode names for plotting\n",
    "for mode in av:\n",
    "    mode_text = '_'.join(str(av[mode]).split('_')[1:])\n",
    "    df_ebike_to_label['Replaced_mode_new'] = df_ebike_to_label['Replaced_mode_new'].replace(mode,mode_text)\n",
    "\n",
    "# Substitution rate of ebike trips not including new trips\n",
    "plot_data = df_ebike_to_label.groupby(['Replaced_mode_new']).count()[['Mode_confirm']].reset_index()\n",
    "plot_data['subst_rate'] = plot_data['Mode_confirm'] / sum(plot_data['Mode_confirm'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,5))\n",
    "sns.barplot(ax=ax, data=plot_data, x='Replaced_mode_new', y='subst_rate').set(title='Ebike Mode Replacement (w/Labeling)', xlabel='Replaced Mode', ylabel='Substitution Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "print(plot_data['subst_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From df_EI\n",
    "# Combine variable categories\n",
    "df_EI = df_EI.replace('Car, drove alone', 'car')\n",
    "df_EI = df_EI.replace('Car, with others', 's_car')\n",
    "df_EI = df_EI.replace('Bikeshare', 's_micro')\n",
    "df_EI = df_EI.replace('Scooter share', 's_micro')\n",
    "df_EI = df_EI.replace('Regular Bike', 'p_micro')\n",
    "df_EI = df_EI.replace('Skate board', 'p_micro')\n",
    "df_EI = df_EI.replace('Train', 'transit')\n",
    "df_EI = df_EI.replace('Free Shuttle', 'transit')\n",
    "df_EI = df_EI.replace('Bus', 'transit')\n",
    "df_EI = df_EI.replace('Walk', 'walk')\n",
    "df_EI = df_EI.replace('Taxi/Uber/Lyft', 'ridehail')\n",
    "df_EI = df_EI.replace('Pilot ebike', 'ebike')\n",
    "emission_rates = df_EI.groupby(['mode']).mean().reset_index()[['mode','energy_intensity_factor','CO2_factor']]\n",
    "emission_rates['g_CO2_per_passmi'] = emission_rates.energy_intensity_factor*emission_rates.CO2_factor*0.000001*453.592\n",
    "emission_data = plot_data.merge(emission_rates, left_on='Replaced_mode_new', right_on='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From df_EI\n",
    "emission_rates = emission_data.g_CO2_per_passmi.values\n",
    "subst_rates = emission_data.subst_rate.values\n",
    "\n",
    "# g-CO2/mi reduction through ebike availability\n",
    "sum(emission_rates * subst_rates) / sum(subst_rates) - 0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From table in paper\n",
    "emission_rates = [343.3, 18.5, 343.3, (343.3/2), 39.8, 123.8, 0.0]\n",
    "subst_rates = plot_data['subst_rate'].values\n",
    "\n",
    "# g-CO2/mi reduction through ebike availability\n",
    "sum(emission_rates * subst_rates) / sum(subst_rates) - 39.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Replaced Mode Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    av = {0: 'av_car',\n",
    "          1: 'av_s_car',\n",
    "          2: 'av_ridehail',\n",
    "          3: 'av_transit',\n",
    "          4: 'av_p_micro',\n",
    "          5: 'av_s_micro',\n",
    "          6: 'av_walk',\n",
    "          7: 'av_ebike'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_list = [df_replaced_trips[av[x]].iloc[i] for i, x in enumerate(df_replaced_trips.Replaced_mode)]\n",
    "df_replaced_trips['replaced_in_stated'] = replaced_list\n",
    "\n",
    "# Relabel with original mode names for plotting\n",
    "for mode in av:\n",
    "    mode_text = '_'.join(str(av[mode]).split('_')[1:])\n",
    "    df_replaced_trips['Mode_confirm'] = df_replaced_trips['Mode_confirm'].replace(mode,mode_text)\n",
    "    df_replaced_trips['Replaced_mode'] = df_replaced_trips['Replaced_mode'].replace(mode,mode_text)\n",
    "\n",
    "df_replaced_trips['Mode_confirm'] = df_replaced_trips['Mode_confirm'].replace(7,'ebike')\n",
    "df_replaced_trips['Replaced_mode'] = df_replaced_trips['Replaced_mode'].replace(7,'ebike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurately stated replacement mode for all users\n",
    "plot_data = df_replaced_trips[df_replaced_trips['Mode_confirm']=='ebike']\n",
    "plot_data = plot_data.groupby(['date_time'], as_index=False)['replaced_in_stated'].agg(['sum','count']).apply(lambda x: x.rolling(14,1).mean())\n",
    "plot_data['proportion'] = plot_data['sum'] / plot_data['count']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,5))\n",
    "sns.lineplot(ax=ax, data=plot_data, x='date_time', y='proportion').set(title='Proportion of Daily E-Bike Trips With Correctly Stated Replacement Mode', xlabel='Date', ylabel='Proportion Correct')\n",
    "plt.xticks(rotation=45)\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurately stated replacement mode for all users across modes\n",
    "plot_data = df_replaced_trips.groupby(['Mode_confirm'], as_index=False)['replaced_in_stated'].agg(['sum','count']).reset_index()\n",
    "plot_data['proportion'] = 1 - (plot_data['sum'] / plot_data['count'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,5))\n",
    "sns.barplot(ax=ax, data=plot_data, x='Mode_confirm', y='proportion').set(title='Proportion of Infeasible Replacements by Primary Mode', xlabel='Primary Mode', ylabel='Proportion Incorrect')\n",
    "plt.xticks(rotation=45)\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurately stated replacement mode for all users across modes\n",
    "plot_data = df_replaced_trips.groupby(['Replaced_mode'], as_index=False)['replaced_in_stated'].agg(['sum','count']).reset_index()\n",
    "plot_data['proportion'] = 1 - (plot_data['sum'] / plot_data['count'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,5))\n",
    "sns.barplot(ax=ax, data=plot_data, x='Replaced_mode', y='proportion').set(title='Proportion of Infeasible Replacements by Replaced Mode', xlabel='Stated Mode Replaced', ylabel='Proportion Incorrect')\n",
    "plt.xticks(rotation=45)\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurately stated replacement mode for all users\n",
    "df_replaced_trips.user_id = df_replaced_trips.user_id.astype(str)\n",
    "plot_data = df_replaced_trips.groupby(['user_id'], as_index=False)['replaced_in_stated'].agg(['sum','count']).reset_index()\n",
    "plot_data['proportion'] = plot_data['sum'] / plot_data['count']\n",
    "plot_data = plot_data.sort_values('proportion', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "sns.barplot(ax=ax, data=plot_data, x='user_id', y='proportion', color='darkblue').set(title='Proportion of Trips With Correctly Stated Replacement Mode', xlabel='User', ylabel='Proportion Correct')\n",
    "plt.xticks(rotation=90)\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurately stated replacement mode for all users\n",
    "df_replaced_trips.user_id = df_replaced_trips.user_id.astype(str)\n",
    "plot_data = df_replaced_trips.groupby(['user_id'], as_index=False)['replaced_in_stated'].agg(['sum','count']).reset_index()\n",
    "plot_data['incorrect'] = plot_data['count'] - plot_data['sum']\n",
    "plot_data['user_id'] = plot_data['user_id'].astype(str).str[-4:]\n",
    "plot_data = plot_data.sort_values('incorrect', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "sns.barplot(ax=ax, data=plot_data, x='user_id', y='incorrect', color='darkblue').set(title='Trips With Unavailable Stated Replacement Mode', xlabel='User', ylabel='Count Incorrect')\n",
    "plt.xticks(rotation=90)\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
