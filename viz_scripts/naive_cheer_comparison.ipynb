{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e578da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/JGreenlee/e-mission-common.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scaffolding\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import asyncio\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from plots import *\n",
    "\n",
    "\n",
    "import emcommon.diary.base_modes as emcb\n",
    "import emcommon.metrics.footprint.util as emcfu\n",
    "import emcommon.metrics.footprint.footprint_calculations as emcfc\n",
    "import emcommon.util as emcommonutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to show all columns\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pur = pd.read_csv(r'auxiliary_files/purpose_labels.csv')\n",
    "df_re = pd.read_csv(r'auxiliary_files/mode_labels.csv')\n",
    "df_ei = pd.read_csv(r'auxiliary_files/energy_intensity.csv')\n",
    "\n",
    "#dictionaries:\n",
    "dic_pur = dict(zip(df_pur['purpose_confirm'],df_pur['bin_purpose'])) # bin purpose\n",
    "dic_re  = dict(zip(df_re['replaced_mode'],df_re['mode_clean'])) # bin modes\n",
    "dic_fuel = dict(zip(df_ei['mode'],df_ei['fuel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert a dictionary to a defaultdict\n",
    "dic_re = defaultdict(lambda: 'Other',dic_re)\n",
    "dic_pur = defaultdict(lambda: 'Other',dic_pur)\n",
    "dic_fuel = defaultdict(lambda: 'Other',dic_fuel)\n",
    "\n",
    "mode_of_interest = \"E-bike\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cbaaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_traceback_handler(exception_type, exception, traceback):\n",
    "    print(\"%s: %s\" % (exception_type.__name__, exception), file=sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "# CASE 2 of https://github.com/e-mission/em-public-dashboard/issues/69#issuecomment-1256835867\n",
    "unique_users = lambda df: len(df.user_id.unique()) if \"user_id\" in df.columns else 0\n",
    "trip_label_count = lambda s, df: len(df[s].dropna()) if s in df.columns else 0\n",
    "\n",
    "\n",
    "def add_energy_labels(expanded_ct, df_ei, dic_fuel):\n",
    "    \"\"\" Inputs:\n",
    "    expanded_ct = dataframe of trips that has had Mode_confirm and Replaced_mode added\n",
    "    dic/df_* = label mappings for energy impact and fuel\n",
    "    \"\"\"\n",
    "    expanded_ct['Mode_confirm_fuel']= expanded_ct['Mode_confirm'].map(dic_fuel)\n",
    "    expanded_ct = energy_intensity(expanded_ct, df_ei, 'Mode_confirm')\n",
    "    expanded_ct = energy_footprint_kWH(expanded_ct, 'distance_miles', 'Mode_confirm')\n",
    "    expanded_ct = CO2_footprint_lb(expanded_ct, 'distance_miles', 'Mode_confirm')\n",
    "    return expanded_ct\n",
    "\n",
    "def add_energy_impact(expanded_ct, df_ei, dic_fuel):\n",
    "    # Let's first calculate everything for the mode confirm\n",
    "    # And then calculate everything for the replaced mode\n",
    "    expanded_ct = add_energy_labels(expanded_ct, df_ei, dic_fuel)\n",
    "    expanded_ct['Replaced_mode_fuel']= expanded_ct['Replaced_mode'].map(dic_fuel)\n",
    "    expanded_ct = energy_intensity(expanded_ct, df_ei, 'Replaced_mode')\n",
    "    # and then compute the impacts\n",
    "    expanded_ct = energy_impact_kWH(expanded_ct, 'distance_miles')\n",
    "    expanded_ct = CO2_impact_lb(expanded_ct, 'distance_miles')\n",
    "    return expanded_ct\n",
    "\n",
    "def get_quality_text(before_df, after_df, mode_of_interest=None, include_test_users=False):\n",
    "    \"\"\" Inputs:\n",
    "    before_df = dataframe prior to filtering (usually participant_ct_df)\n",
    "    after_df = dataframe after filtering (usually expanded_ct)\n",
    "    mode_of_interest = optional detail to include in the text string\n",
    "    \"\"\"\n",
    "    # CASE 1 of https://github.com/e-mission/em-public-dashboard/issues/69#issuecomment-1256835867\n",
    "    after_pct = (len(after_df) * 100) / len(before_df) if len(before_df) != 0 else np.nan\n",
    "    cq = (len(after_df), unique_users(after_df), len(before_df), unique_users(before_df),\n",
    "        after_pct, )\n",
    "    interest_str = mode_of_interest + ' ' if mode_of_interest is not None else ''\n",
    "    total_str = 'confirmed' if mode_of_interest is not None else ''\n",
    "    user_str = 'testers and participants' if include_test_users else 'users'\n",
    "    quality_text = f\"Based on %s confirmed {interest_str}trips from %d {user_str}\\nof %s total {total_str} trips from %d users (%.2f%%)\" % cq\n",
    "    print(quality_text)\n",
    "    return quality_text\n",
    "\n",
    "\n",
    "\n",
    "def data_quality_check(expanded_ct):\n",
    "    '''1. Delete rows where the mode_confirm was pilot_ebike and repalced_mode was pilot_ebike.\n",
    "       2. Delete rows where the mode_confirm was pilot_ebike and repalced_mode was same_mode.\n",
    "       3. Replace same_mode for the mode_confirm for Energy Impact Calcualtion.'''\n",
    "\n",
    "    # TODO: This is only really required for the initial data collection around the minipilot\n",
    "    # in subsequent deployes, we removed \"same mode\" and \"pilot_ebike\" from the options, so the\n",
    "    # dataset did not contain of these data quality issues\n",
    "\n",
    "    if 'replaced_mode' in expanded_ct.columns:\n",
    "        expanded_ct.drop(expanded_ct[(expanded_ct['mode_confirm'] == 'pilot_ebike') & (expanded_ct['replaced_mode'] == 'pilot_ebike')].index, inplace=True)\n",
    "        expanded_ct.drop(expanded_ct[(expanded_ct['mode_confirm'] == 'pilot_ebike') & (expanded_ct['replaced_mode'] == 'same_mode')].index, inplace=True)\n",
    "        expanded_ct['replaced_mode'] = np.where(expanded_ct['replaced_mode'] == 'same_mode',expanded_ct['mode_confirm'], expanded_ct['replaced_mode'])\n",
    "    \n",
    "    return expanded_ct\n",
    "\n",
    "def unit_conversions(df):\n",
    "    df['distance_miles']= df[\"distance\"]*0.00062 #meters to miles\n",
    "\n",
    "def energy_intensity(trip_df,mode_intensity_df,col):\n",
    "    \"\"\" Inputs:\n",
    "    trip_df = dataframe with data\n",
    "    mode_intensity_df = dataframe with energy/cost/time factors\n",
    "    col = the column for which we want to map the intensity\n",
    "    \"\"\"\n",
    "\n",
    "    mode_intensity_df = mode_intensity_df.copy()\n",
    "    mode_intensity_df[col] = mode_intensity_df['mode']\n",
    "    dic_ei_factor = dict(zip(mode_intensity_df[col],mode_intensity_df['energy_intensity_factor']))\n",
    "    dic_CO2_factor = dict(zip(mode_intensity_df[col],mode_intensity_df['CO2_factor']))\n",
    "    dic_ei_trip = dict(zip(mode_intensity_df[col],mode_intensity_df['(kWH)/trip']))\n",
    "\n",
    "    trip_df['ei_'+col] = trip_df[col].map(dic_ei_factor)\n",
    "    trip_df['CO2_'+col] = trip_df[col].map(dic_CO2_factor)\n",
    "    trip_df['ei_trip_'+col] = trip_df[col].map(dic_ei_trip)\n",
    "    return trip_df\n",
    "\n",
    "def energy_footprint_kWH(df,distance_miles,col):\n",
    "    \"\"\" Inputs:\n",
    "    df = dataframe with data\n",
    "    distance = distance in miles\n",
    "    col = Replaced_mode or Mode_confirm\n",
    "    \"\"\"\n",
    "    conditions_col = [(df[col+'_fuel'] =='gasoline'),\n",
    "                       (df[col+'_fuel'] == 'diesel'),\n",
    "                       (df[col+'_fuel'] == 'electric')]\n",
    "    gasoline_col = (df[distance_miles]*df['ei_'+col]*0.000293071) # 1 BTU = 0.000293071 kWH\n",
    "    diesel_col   = (df[distance_miles]*df['ei_'+col]*0.000293071)\n",
    "    electric_col = (df[distance_miles]*df['ei_'+col])+ df['ei_trip_'+col]\n",
    "    values_col = [gasoline_col,diesel_col,electric_col]\n",
    "    df[col+'_EI(kWH)'] = np.select(conditions_col, values_col)\n",
    "    return df\n",
    "\n",
    "def energy_impact_kWH(df,distance_miles):\n",
    "    if 'Mode_confirm_EI(kWH)' not in df.columns:\n",
    "        print(\"Mode confirm footprint not found, computing before impact\")\n",
    "        df = energy_footprint_kWH(df, distance_miles, \"Mode_confirm\")\n",
    "    df = energy_footprint_kWH(df, distance_miles, \"Replaced_mode\")\n",
    "    df['Energy_Impact(kWH)']  = round((df['Replaced_mode_EI(kWH)'] - df['Mode_confirm_EI(kWH)']),3)\n",
    "    return df\n",
    "\n",
    "def CO2_footprint_lb(df, distance_miles, col):\n",
    "    \"\"\" Inputs:\n",
    "    df = dataframe with data\n",
    "    distance = distance in miles\n",
    "    col = Replaced_mode or Mode_confirm\n",
    "    \"\"\"\n",
    "    conditions_col = [(df[col+'_fuel'] =='gasoline'),\n",
    "                       (df[col+'_fuel'] == 'diesel'),\n",
    "                       (df[col+'_fuel'] == 'electric')]\n",
    "   \n",
    "    gasoline_col = (df[distance_miles]*df['ei_'+col]*0.000001)* df['CO2_'+col]\n",
    "    diesel_col   = (df[distance_miles]*df['ei_'+col]*0.000001)* df['CO2_'+col]\n",
    "    electric_col = (((df[distance_miles]*df['ei_'+col])+df['ei_trip_'+col])*0.001)*df['CO2_'+col]\n",
    "\n",
    "    values_col = [gasoline_col,diesel_col,electric_col]\n",
    "    df[col+'_lb_CO2'] = np.select(conditions_col, values_col)\n",
    "    return df\n",
    "    \n",
    "def CO2_impact_lb(df,distance_miles):\n",
    "    if 'Mode_confirm_lb_CO2' not in df.columns:\n",
    "        print(\"Mode confirm footprint not found, computing before impact\")\n",
    "        df = CO2_footprint_lb(df, distance_miles, \"Mode_confirm\")\n",
    "    df = CO2_footprint_lb(df, distance_miles, \"Replaced_mode\")\n",
    "    df['CO2_Impact(lb)']  = round((df['Replaced_mode_lb_CO2'] - df['Mode_confirm_lb_CO2']),3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9079775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path configuration\n",
    "to_data_folder = \"PaperVizualizations/Data/abby_ceo/sc\" #data folder, where composite data was written from the TSDC_data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73190bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('viz_scripts/abby_ceo/sc/analysis_confirmed_trip.csv')\n",
    "# we are using smart commute data. zip with this csv taken from onedrive.\n",
    "df = pd.read_csv(to_data_folder + \"/analysis_confirmed_trip.csv\")\n",
    "# expanded_ct_2=pd.read_csv(to_data_folder + \"/tsdc_filtered_merged_trips.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78adcf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(dic_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def calculate_naive_and_cheer(passed_df: pd.DataFrame):\n",
    "    # we dont have demographic questions.\n",
    "    passed_df.rename(columns={\n",
    "        'user_id_socio': 'user_id',\n",
    "        'please_identify_which_category_represents_your_total_household_': 'HHINC',\n",
    "        'how_many_motor_vehicles_are_owned_leased_or_available_for_regul': 'VEH',\n",
    "        ' how_many_motor_vehicles_are_owned_leased_or_available_for_regul': 'VEH',\n",
    "        'how_many_motor_vehicles_are_owned_leased_or_available_for_regul ': 'VEH',\n",
    "        'in_which_year_were_you_born?': 'AGE',\n",
    "        'including_yourself_how_many_people_live_in_your_home?': 'HHSIZE',\n",
    "        'how_many_children_under_age_18_live_in_your_home?': 'CHILDREN',\n",
    "        'what_is_your_gender?': 'GENDER',\n",
    "        'if_you_were_unable_to_use_your_household_vehicles_which_of_the_': 'available_modes',\n",
    "        'are_you_a_student?': 'STUDENT',\n",
    "        'data_duration': 'duration',\n",
    "        'data_distance': 'distance'\n",
    "    }, inplace=True, errors='ignore')\n",
    "    \n",
    "    df_mapped = passed_df.copy()\n",
    "\n",
    "    #first, add the cleaned mode\n",
    "    df_mapped['Mode_confirm']= df_mapped['data_user_input_mode_confirm'].map(dic_re)\n",
    "\n",
    "    #second, add the cleaned replaced mode ASSUMES PROGRAM\n",
    "    df_mapped['Replaced_mode']= df_mapped['data_user_input_replaced_mode'].map(dic_re)\n",
    "\n",
    "    #third, add the cleaned purpose\n",
    "    df_mapped['Trip_purpose']= df_mapped['data_user_input_purpose_confirm'].map(dic_pur)\n",
    "    \n",
    "    # Get timestamp from known year/month/day aggregated to days\n",
    "    df_mapped.rename(columns={'data_start_local_dt_year':'year','data_start_local_dt_month':'month','data_start_local_dt_day':'day'}, inplace=True)\n",
    "    df_mapped['date_time'] = pd.to_datetime(df_mapped[['year','month','day']])\n",
    "\n",
    "    # Fix age (birth year to age)\n",
    "    # df_mapped['AGE'] = 2022 - df_mapped['AGE']\n",
    "\n",
    "    # Number of workers (size of HH - kids)\n",
    "    # df_mapped['WORKERS'] = df_mapped['HHSIZE'] - df_mapped['CHILDREN']\n",
    "\n",
    "    # Duration in minutes (hours to minutes)\n",
    "    df_mapped['duration'] = df_mapped['duration'] / 60\n",
    "\n",
    "    # duration in miles (meters to miles)\n",
    "    df_mapped['distance_miles'] = df_mapped['distance'] * 0.0006213712\n",
    "\n",
    "    # E-bike/not E-Bike variable\n",
    "    # df_mapped['is_ebike'] = \"E-Bike Trips\"\n",
    "    # df_mapped.loc[df_mapped['Mode_confirm']!=\"E-bike\", 'is_ebike'] = \"Non E-Bike Trips\"\n",
    "    \n",
    "    expanded_ct = df_mapped\n",
    "    expanded_ct = add_energy_impact(expanded_ct, df_ei, dic_fuel) if len(expanded_ct) > 0 else expanded_ct\n",
    "    expanded_ct[expanded_ct['Mode_confirm'] == 'E-bike']\n",
    "    # expanded_ct.head(30)\n",
    "    \n",
    "    if 'mode_confirm' in expanded_ct.columns:\n",
    "        mode_of_interest_df = expanded_ct.query(f\"mode_confirm == '{mode_of_interest}'\")\n",
    "        debug_df.loc[f\"{mode_of_interest}_trips\"] = len(mode_of_interest_df)\n",
    "        debug_df.loc[f\"{mode_of_interest}_trips_with_replaced_mode\"] = scaffolding.trip_label_count(\"Replaced_mode\", mode_of_interest_df)\n",
    "        \n",
    "    # CASE 2 of https://github.com/e-mission/em-public-dashboard/issues/69#issuecomment-1256835867\n",
    "    data_eb = expanded_ct.query(f\"Mode_confirm == '{mode_of_interest}'\") if \"Mode_confirm\" in expanded_ct.columns else expanded_ct\n",
    "    \n",
    "    quality_text = get_quality_text(expanded_ct, data_eb, mode_of_interest)\n",
    "    \n",
    "    # ebei : ebike energy impact\n",
    "    plot_title_no_quality=f\"Sketch of Energy Impact of {mode_of_interest} trips\"\n",
    "    file_name =f'sketch_energy_impact_{mode_of_interest}%s'\n",
    "\n",
    "    ebei=data_eb.groupby('Replaced_mode').agg({'Energy_Impact(kWH)': ['sum', 'mean']},)\n",
    "    ebei.columns = ['Sketch of Total Energy_Impact(kWH)', 'Sketch of Average Energy_Impact(kWH)']\n",
    "    ebei= ebei.reset_index()\n",
    "    ebei = ebei.sort_values(by=['Sketch of Total Energy_Impact(kWH)'], ascending=False)\n",
    "    ebei['boolean'] = ebei['Sketch of Total Energy_Impact(kWH)'] > 0\n",
    "    net_energy_saved = round(sum(ebei['Sketch of Total Energy_Impact(kWH)']), 2)\n",
    "\n",
    "    x = ebei['Sketch of Total Energy_Impact(kWH)']\n",
    "    y = ebei['Replaced_mode']\n",
    "    color =ebei['boolean']\n",
    "\n",
    "    plot_title= plot_title_no_quality+f\"\\n Contribution by replaced mode towards a total of {net_energy_saved}(kWH)\\n\"+quality_text\n",
    "#     energy_impact(x,y,color,plot_title,file_name)\n",
    "    alt_text = store_alt_text_bar(pd.DataFrame(x.values,y), file_name, plot_title)\n",
    "    \n",
    "\n",
    "    \n",
    "    # emcb.BASE_MODES['pilot_ebike'] = emcb.BASE_MODES['E_BIKE']\n",
    "    # print(emcb.BASE_MODES['pilot_ebike'])\n",
    "    #\n",
    "    # the default json does not use pilot_ebike, so manually go in and add it to that json.\n",
    "    #\n",
    "    default_json = await emcfu.read_json_resource('label-options.default.json')\n",
    "    default_json['MODE'].append({\"value\":\"pilot_ebike\", \"base_mode\":\"E_BIKE\"})\n",
    "    # print(default_json)\n",
    "    \n",
    "    expanded_ct = expanded_ct.dropna(subset=['data_user_input_mode_confirm'])\n",
    "    expanded_ct = expanded_ct.dropna(subset=['data_user_input_replaced_mode'])\n",
    "    expanded_ct = expanded_ct[expanded_ct['Mode_confirm'] != 'Not a Trip']\n",
    "\n",
    "    # expanded_ct['data_user_input_mode_confirm'] = expanded_ct['data_user_input_mode_confirm'].replace('pilot_ebike', 'e-bike')\n",
    "\n",
    "\n",
    "    print(expanded_ct['data_user_input_mode_confirm'].unique())\n",
    "    \n",
    "    async def get_commute_data(row, labels, ):\n",
    "    #     mode_footprint = emcb.get_rich_mode(mode)[\"footprint\"]\n",
    "    #     mode_footprint = passed_mode_footprint['footprint']\n",
    "    #     distance = row[distance_col] * 1609.34 # converts miles to meters #might not need to convert - one of the distance cols (raw one) should be in meters\n",
    "    #     year = row['year']\n",
    "    #     coords = row[\"geometry\"].centroid\n",
    "        # long, lat\n",
    "        # can we make this assumption to use start?\n",
    "    #     coords = [row['data_start_loc_longitude'], row['data_start_loc_latitude']] #double check if start or end are used in production\n",
    "        #coords = [row['data_start_loc_latitude'], row['data_start_loc_longitude']] #double check if start or end are used in production\n",
    "        #\n",
    "    #     uace = None #find uace code similar to egrid region\n",
    "    #     egrid_region = await get_egrid_region(coords, year)\n",
    "    #     passengers = 1\n",
    "\n",
    "        trip_object = {\n",
    "            \"_id\": row['_id'],\n",
    "            \"distance\": row['distance'],\n",
    "            \"start_fmt_time\": row['data_start_fmt_time'],\n",
    "            \"start_loc\": {\"coordinates\": [row['data_start_loc_longitude'], row['data_start_loc_latitude']]},\n",
    "            \"user_input\": {\"mode_confirm\": row['data_user_input_mode_confirm'],\n",
    "                           \"replaced_mode_confirm\": row['data_user_input_replaced_mode']\n",
    "                          }\n",
    "        }\n",
    "\n",
    "\n",
    "    #     footprint = await emcfc.calc_footprint(mode_footprint, distance, year, coords, uace, egrid_region, passengers)\n",
    "    #     try:\n",
    "    #     print(row['data_user_input_replaced_mode'])\n",
    "    #     print(\":)\")\n",
    "        footprint = await emcfc.calc_footprint_for_trip(trip_object, labels, )\n",
    "        replaced_footprint = await emcfc.calc_footprint_for_trip(trip_object, labels, 'replaced_mode')\n",
    "    #     except ValueError:\n",
    "    #         print('!'*90, end='')\n",
    "    #         print(row['data_user_input_mode_confirm'])\n",
    "    #         return\n",
    "        return {\n",
    "            'footprint': footprint,\n",
    "            'replaced_footprint': replaced_footprint\n",
    "        }\n",
    "\n",
    "    # asyncio concurrency\n",
    "    async def get_commute_data_task(row, index, expanded_ct, labels):\n",
    "        commute_data = await get_commute_data(row, labels)\n",
    "\n",
    "        # update fields with values from `commute_data` in expanded_ct directly\n",
    "        if 'kg_co2' in commute_data['footprint'][0] and 'kwh' in commute_data['footprint'][0]:\n",
    "            expanded_ct.at[index, 'cheer_kg_co2'] = commute_data['footprint'][0]['kg_co2']\n",
    "            expanded_ct.at[index, 'cheer_kwh'] = commute_data['footprint'][0]['kwh']\n",
    "            expanded_ct.at[index, 'cheer_replaced_kg_co2'] = commute_data['replaced_footprint'][0]['kg_co2']\n",
    "            expanded_ct.at[index, 'cheer_replaced_kwh'] = commute_data['replaced_footprint'][0]['kwh']\n",
    "\n",
    "            for uncertain_column in ['kg_co2_uncertain', 'kwh_uncertain']:\n",
    "                if uncertain_column in commute_data['replaced_footprint'][0]:\n",
    "                    expanded_ct.at[index, f'cheer_replaced_{uncertain_column}'] = commute_data['replaced_footprint'][0][uncertain_column]\n",
    "\n",
    "        expanded_ct.at[index, 'Mode_confirm_kg_CO2'] = row['Mode_confirm_lb_CO2'] * 0.453592\n",
    "\n",
    "        # sanity check\n",
    "        if row['Mode_confirm'] == 'Walk' and ('cheer_kg_co2' in row) and (row['cheer_kg_co2'] != 0):\n",
    "            print('!!!!!!!!!!!!!!' * 2)\n",
    "\n",
    "\n",
    "    # Check if indexes are unique\n",
    "    print('unique or not?')\n",
    "    expanded_ct = expanded_ct.reset_index()\n",
    "    print(expanded_ct.index.is_unique)\n",
    "\n",
    "    # Create tasks for each row in expanded_ct\n",
    "    tasks = [\n",
    "        get_commute_data_task(row, index, expanded_ct, default_json)\n",
    "        for index, row in expanded_ct.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Run all tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "        #calculate energy and emissions saved\n",
    "    #     row[\"cheer_e_saved\"] = row[\"work_car\"][0][\"kwh\"] - x[\"work_ecar\"][0][\"kwh\"], axis = 1)\n",
    "    #     row[\"cheer_co2_saved\"] = test_df.apply(lambda x : x[\"work_car\"][0][\"kg_co2\"] - x[\"work_ecar\"][0][\"kg_co2\"], axis = 1)\n",
    "\n",
    "    #     break\n",
    "    \n",
    "    return expanded_ct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotter(passed_expanded_ct: pd.DataFrame, name_of_dataset: str):\n",
    "\n",
    "    # remove 'Air' and 'Other' from Mode_confirm,\n",
    "    # because they really offset the others and made the others unreadable\n",
    "    expanded_ct_filtered = passed_expanded_ct[~passed_expanded_ct['Mode_confirm'].isin(['Air', 'Other'])]\n",
    "\n",
    "    # group by 'Mode_confirm' and calculate the mean for each mode\n",
    "    grouped_data_filtered = expanded_ct_filtered.groupby('Mode_confirm')[['Mode_confirm_kg_CO2', 'cheer_kg_co2']].mean()\n",
    "\n",
    "    # Define a function to create and save plots with specified y-axis scale\n",
    "    def create_and_save_plot(scale: str):\n",
    "        # plotting two bars for each mode: one for Mode_confirm_kg_CO2 (naive) and one for cheer_kg_co2\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = grouped_data_filtered.plot(kind='bar', figsize=(10, 6), color=['lightblue', 'orange'])\n",
    "\n",
    "        # Title update based on scale\n",
    "        plt.title(f'Comparison of CO2 Emissions for Each Mode of Transportation\\n{name_of_dataset} Dataset ({scale.capitalize()} Scale)')\n",
    "        plt.ylabel('Average CO2 Emissions (kg)')\n",
    "        plt.xlabel('Mode of Transportation')\n",
    "\n",
    "        # Apply scale settings\n",
    "        if scale == 'log':\n",
    "            ax.set_yscale('log')\n",
    "            ax.set_ylim(0.01, 13)  # Set limit for log scale\n",
    "            ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:g}'))\n",
    "        else:\n",
    "            ax.set_ylim(0, 13)  # Limit for normal scale\n",
    "\n",
    "        # Set legend location to top left\n",
    "        plt.legend(['Naive Calculation (Mode_confirm_kg_CO2)', 'CHEER Calculation (cheer_kg_co2)'], loc='upper left')\n",
    "\n",
    "        # show plot\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot as PDF and PNG in the figures directory\n",
    "        plt.savefig(f'figures/{name_of_dataset}_{scale}.pdf')\n",
    "        plt.savefig(f'figures/{name_of_dataset}_{scale}.png')\n",
    "        plt.close()\n",
    "\n",
    "    # Create and save normal scale plot\n",
    "    create_and_save_plot(scale='normal')\n",
    "    \n",
    "    # Create and save logarithmic scale plot\n",
    "    create_and_save_plot(scale='log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def load_or_fetch_data(dataset_name: str,\n",
    "                             original_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a pickle file if it exists, otherwise fetches it asynchronously and saves it.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_name (str): The name of the dataset to load or fetch (used as the filename).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded or fetched dataset as a DataFrame.\n",
    "    \"\"\"\n",
    "    filename = f\"{dataset_name}.pkl\"\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        # Fetch data asynchronously\n",
    "        data = await calculate_naive_and_cheer(original_df)\n",
    "        \n",
    "        # Save to pickle file\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "    else:\n",
    "        # Load data from pickle file\n",
    "        with open(filename, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "smartcommute = await load_or_fetch_data(\"smart_commute\",\n",
    "                             df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d532bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(smartcommute, \"Smart Commute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c90a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path configuration\n",
    "to_data_parent = \"PaperVizualizations/Data/abby_ceo\" #path to the parent folder, should contain program subfolders\n",
    "to_mini_data = \"PaperVizualizations/Data/mini_pilot/data\" #path to the mini data folder, contains an analysis trips file\n",
    "# to_data_folder = \"PaperVizualizations\" #data folder, where composite data files will be written/read\n",
    "\n",
    "#loop over\n",
    "folders = ['4c', 'cc', 'fc', 'pc', 'sc', 'vail_22']\n",
    "datasets = []\n",
    "\n",
    "for program in folders:\n",
    "    print('\\nstarting with ', program)\n",
    "    \n",
    "    #create dataset with surveys and trips\n",
    "    trips = pd.read_csv(to_data_parent + '/' + program + '/analysis_confirmed_trip.csv')\n",
    "    print(len(trips), 'trips')\n",
    "    print(trips.perno.nunique(), 'people')\n",
    "\n",
    "    surveys = pd.read_csv(to_data_parent + '/' + program + '/' + program + '_survey_household.csv')\n",
    "    print(len(surveys), 'surveys')\n",
    "\n",
    "    #drop any null ids\n",
    "    socio_data = surveys[~surveys['unique_user_id_autofilled_do_not_edit'].isnull()]\n",
    "    print(len(socio_data), 'surveys after dropping null ids')\n",
    "\n",
    "    #drop duplicates\n",
    "    socio_data = socio_data.sort_values(by=['unique_user_id_autofilled_do_not_edit', 'timestamp'])\n",
    "    socio_data.drop_duplicates(subset=['unique_user_id_autofilled_do_not_edit'], keep='last', inplace=True)\n",
    "    print(len(socio_data),'surveys', socio_data['unique_user_id_autofilled_do_not_edit'].nunique(), 'users after dropping duplicates')\n",
    "\n",
    "    #prepare survey ids for merging\n",
    "    socio_data['user_id_socio'] = socio_data['unique_user_id_autofilled_do_not_edit'].astype(str)\n",
    "    socio_data['user_id_socio'] = socio_data['user_id_socio'].str.strip() #remove leading or trailing whitespace!!\n",
    "    socio_data['user_id_socio'] = socio_data['user_id_socio']\n",
    "    socio_data = socio_data.drop(labels='unique_user_id_autofilled_do_not_edit', axis=1)\n",
    "    \n",
    "    \n",
    "    #prepare trip ids for merging\n",
    "    trips['user_id_socio'] = trips.perno.astype(str)\n",
    "    trips['user_id_socio'] = trips['user_id_socio'].str.strip() #remove leading or trailing whitespace!!\n",
    "    trips.user_id_socio = [i.replace('-','') for i in trips.user_id_socio] # remove all dashes from strings\n",
    "    \n",
    "    #merge the data\n",
    "    data = trips.merge(socio_data, on='user_id_socio')\n",
    "    print(len(data), 'trips after merging')\n",
    "    print(data.user_id_socio.nunique(), 'people after merging')\n",
    "    \n",
    "    data['program'] = program.split('_')[0]\n",
    "    \n",
    "    #add to list of datasets\n",
    "    datasets.append(data)\n",
    "    \n",
    "#merge them all together\n",
    "full_data = pd.concat(datasets)\n",
    "print(len(full_data), 'trips')\n",
    "print(full_data.perno.nunique(), 'users')\n",
    "# print(full_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# TEMPORARY. REMOVE\n",
    "#\n",
    "# full_data = full_data[full_data['data_user_input_mode_confirm'] == 'walk'].head(55)\n",
    "# full_data = full_data.head(20000)\n",
    "# full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_data['data_user_input_mode_confirm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "canbikeco = await load_or_fetch_data(\"canbikeco\",\n",
    "                                     full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8825f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug cheer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d72131",
   "metadata": {},
   "outputs": [],
   "source": [
    "whatsthis = canbikeco[(canbikeco['Mode_confirm'] == 'Walk') & (canbikeco['cheer_kg_co2'] > 0)]\n",
    "# Drop specified columns from the DataFrame before selecting the first row\n",
    "whatsthis.head(1)\n",
    "# whatsthis_filtered = whatsthis.drop(columns=['cheer_kg_co2', 'cheer_kwh', 'cheer_replaced_kg_co2', 'cheer_replaced_kwh'])\n",
    "\n",
    "# # Select the first row and convert it to a dictionary\n",
    "# first_row_dict = whatsthis_filtered.iloc[0].to_dict()\n",
    "\n",
    "# # Print the dictionary\n",
    "# # print(first_row_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac534884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goodone = canbikeco[(canbikeco['Mode_confirm'] == 'Walk') & (canbikeco['cheer_kg_co2'] == 0)]\n",
    "# print(goodone.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Concatenate the two DataFrames along rows (default)\n",
    "# combined_df = pd.concat([whatsthis_filtered.iloc[0], goodone.head(1)], ignore_index=True)\n",
    "# combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a3b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(canbikeco, \"CanBikeCO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_test = canbikeco[canbikeco['Mode_confirm'] == 'Walk'].head(2)\n",
    "a_test\n",
    "# a_test = pd.DataFrame(a_test\n",
    "#                      )\n",
    "# row = a_test.iloc[0]\n",
    "# print(a_test.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0fb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474938f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(replaced_footprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d31773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bull Durham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710450ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('bull'):\n",
    "    if not os.path.isfile('tsdc-2022-bull-e-bike-pilot-program-study-full-survey-data 2.zip'):\n",
    "        raise ValueError(\"i need the zip, ask TSDC\")\n",
    "    else:\n",
    "        # Unzip the file to the 'bull' directory\n",
    "        with zipfile.ZipFile('tsdc-2022-bull-e-bike-pilot-program-study-full-survey-data 2.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('bull')\n",
    "        print(\"Unzipped to 'bull' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bull = pd.read_csv('bull/tsdc-2022-bull-e-bike-pilot-program-study-full-survey-data 2/data/analysis_confirmed_trip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea068144",
   "metadata": {},
   "outputs": [],
   "source": [
    "bull_loaded = await load_or_fetch_data(\"bull-durham\",\n",
    "                                     bull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda14393",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(bull_loaded, \"Bull (Durham, NC) eBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MassCEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7209ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'MassCEC' directory exists\n",
    "if not os.path.isdir('MassCEC'):\n",
    "    # Check if the tar.gz file exists\n",
    "    if not os.path.isfile('mass_jacques.tar.gz'):\n",
    "        raise ValueError(\"I need the tar.gz file, ask TSDC\")\n",
    "    else:\n",
    "        # Extract the tar.gz file to the 'MassCEC' directory\n",
    "        with tarfile.open('mass_jacques.tar.gz', 'r:gz') as tar_ref:\n",
    "            tar_ref.extractall('MassCEC')\n",
    "        print(\"Extracted to 'MassCEC' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = pd.read_csv('MassCEC/mass_jacques/analysis_confirmed_trip.csv')\n",
    "# print(mass.columns)\n",
    "print(mass['data_user_input_mode_confirm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass[mass['data_user_input_mode_confirm'] == 'bus'].head(55)\n",
    "print(len(mass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_loaded = await load_or_fetch_data(\"masscec\",\n",
    "                                       mass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass_loaded[mass_loaded['Mode_confirm'] == 'bus'].head(55)\n",
    "print(len(mass_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mass_loaded['Mode_confirm'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(mass_loaded, \"MassCEC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ad45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded_ct.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(expanded_ct.head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac05fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
