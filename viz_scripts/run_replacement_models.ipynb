{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from plots import *\n",
    "import replacement_models as rm\n",
    "import scaffolding\n",
    "\n",
    "# For reloading modules from Jupyter\n",
    "import importlib\n",
    "importlib.reload(rm)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "SAVE_DIR = '/plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./processed_replacement_modeling_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_col = 'Replaced_mode_num'\n",
    "feature_list = ['Mode_confirm_num',\n",
    "                'hhinc_$25,000-$49,999',\n",
    "                'hhinc_$50,000-$99,999',\n",
    "                'hhinc_Less than $24,999',\n",
    "                'purp_Home',\n",
    "                'purp_commute',\n",
    "                'purp_discretionary',\n",
    "                'purp_pudo',\n",
    "                'purp_recreation',\n",
    "                'purp_transit_transfer',\n",
    "                'AGE',\n",
    "                'VEH',\n",
    "                'HHSIZE',\n",
    "                'is_male',\n",
    "                'sin_time',\n",
    "                'cos_time',\n",
    "                'sin_month',\n",
    "                'cos_month',\n",
    "                'is_weekend',\n",
    "                'duration',\n",
    "                'distance_miles',\n",
    "                'av_car',\n",
    "                'av_s_car',\n",
    "                'av_ebike',\n",
    "                'av_p_micro',\n",
    "                'av_s_micro',\n",
    "                'av_ridehail',\n",
    "                'av_transit',\n",
    "                'av_walk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up K-fold cross validation\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# Collect all scores to show at end of modeling\n",
    "score_results = {}\n",
    "score_results_holdout = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up holdout users\n",
    "all_users = pd.unique(data['user_id'])\n",
    "holdout_users = np.random.choice(all_users, 10)\n",
    "holdout_data = data[data['user_id'].isin(holdout_users)]\n",
    "non_holdout_data = data[~data['user_id'].isin(holdout_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on All Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "rf, accuracy, f1, confusion = rm.rf(data, choice_col, feature_list, kf)\n",
    "\n",
    "# Save scores for model comparison\n",
    "score_results['rf'] = (np.mean(accuracy), np.mean(f1))\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "\n",
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, cbar=False).set(title='Random Forest Confusion Matrix', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Holdout Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "rf, accuracy, f1, confusion = rm.rf(non_holdout_data, choice_col, feature_list, kf)\n",
    "\n",
    "holdout_true = holdout_data[choice_col].values\n",
    "holdout_pred = rf.predict(holdout_data[feature_list].values)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(holdout_true, holdout_pred)\n",
    "f1 = sklearn.metrics.f1_score(holdout_true, holdout_pred, average='weighted')\n",
    "score_results_holdout['rf'] = (np.mean(accuracy), np.mean(f1))\n",
    "\n",
    "print(f\"Holdout Accuracy: {accuracy}\")\n",
    "print(f\"Holdout F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on All Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "rf, accuracy, f1, confusion = rm.gbdt(data, choice_col, feature_list, kf)\n",
    "\n",
    "# Save scores for model comparison\n",
    "score_results['gbdt'] = (np.mean(accuracy), np.mean(f1))\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "\n",
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, cbar=False).set(title='GBDT Confusion Matrix', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Holdout Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "gbdt, accuracy, f1, confusion = rm.gbdt(non_holdout_data, choice_col, feature_list, kf)\n",
    "\n",
    "holdout_true = holdout_data[choice_col].values\n",
    "holdout_pred = gbdt.predict(holdout_data[feature_list].values)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(holdout_true, holdout_pred)\n",
    "f1 = sklearn.metrics.f1_score(holdout_true, holdout_pred, average='weighted')\n",
    "score_results_holdout['gbdt'] = (np.mean(accuracy), np.mean(f1))\n",
    "\n",
    "print(f\"Holdout Accuracy: {accuracy}\")\n",
    "print(f\"Holdout F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on All Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "rf, accuracy, f1, confusion = rm.svm(data, choice_col, feature_list, kf)\n",
    "\n",
    "# Save scores for model comparison\n",
    "score_results['svm'] = (np.mean(accuracy), np.mean(f1))\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "\n",
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, cbar=False).set(title='SVM Confusion Matrix', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Holdout Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "svm, accuracy, f1, confusion = rm.svm(non_holdout_data, choice_col, feature_list, kf)\n",
    "\n",
    "holdout_true = holdout_data[choice_col].values\n",
    "holdout_pred = svm.predict(holdout_data[feature_list].values)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(holdout_true, holdout_pred)\n",
    "f1 = sklearn.metrics.f1_score(holdout_true, holdout_pred, average='weighted')\n",
    "score_results_holdout['svm'] = (np.mean(accuracy), np.mean(f1))\n",
    "\n",
    "print(f\"Holdout Accuracy: {accuracy}\")\n",
    "print(f\"Holdout F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on All Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "knn, accuracy, f1, confusion = rm.knn(data, choice_col, feature_list, kf)\n",
    "\n",
    "# Save scores for model comparison\n",
    "score_results['knn'] = (np.mean(accuracy), np.mean(f1))\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"F1: {np.mean(f1)}\")\n",
    "\n",
    "# Average and plot the confusion matrices\n",
    "confusion_mean = np.mean(np.array(confusion), axis=0)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.heatmap(confusion_mean, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=.5, cbar=False).set(title='KNN Confusion Matrix', xlabel='Predicted', ylabel='Actual')\n",
    "plt.subplots_adjust(bottom=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Holdout Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test model\n",
    "knn, accuracy, f1, confusion = rm.knn(non_holdout_data, choice_col, feature_list, kf)\n",
    "\n",
    "holdout_true = holdout_data[choice_col].values\n",
    "holdout_pred = knn.predict(holdout_data[feature_list].values)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(holdout_true, holdout_pred)\n",
    "f1 = sklearn.metrics.f1_score(holdout_true, holdout_pred, average='weighted')\n",
    "score_results_holdout['knn'] = (np.mean(accuracy), np.mean(f1))\n",
    "\n",
    "print(f\"Holdout Accuracy: {accuracy}\")\n",
    "print(f\"Holdout F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall results\n",
    "accuracy_all = pd.DataFrame(pd.DataFrame(score_results).iloc[0,:]).reset_index()\n",
    "accuracy_all.columns = ['Model','Score']\n",
    "accuracy_all['Type'] = 'All'\n",
    "f1_all = pd.DataFrame(pd.DataFrame(score_results).iloc[1,:]).reset_index()\n",
    "f1_all.columns = ['Model','Score']\n",
    "f1_all['Type'] = 'All'\n",
    "score_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall results\n",
    "accuracy_holdout = pd.DataFrame(pd.DataFrame(score_results_holdout).iloc[0,:]).reset_index()\n",
    "accuracy_holdout.columns = ['Model','Score']\n",
    "accuracy_holdout['Type'] = 'Holdout'\n",
    "f1_holdout = pd.DataFrame(pd.DataFrame(score_results_holdout).iloc[1,:]).reset_index()\n",
    "f1_holdout.columns = ['Model','Score']\n",
    "f1_holdout['Type'] = 'Holdout'\n",
    "score_results_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = pd.concat([accuracy_all, accuracy_holdout])\n",
    "f1 = pd.concat([f1_all, f1_holdout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=accuracy, x='Model', y='Score', hue='Type').set(title='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=f1, x='Model', y='Score', hue='Type').set(title='F1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
